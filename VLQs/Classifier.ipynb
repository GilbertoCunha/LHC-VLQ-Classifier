{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import and rearrange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "bkgd = pd.read_hdf(\"data/preprocessed/bkgd.h5\", key=\"bkgd\")\n",
    "vlq = pd.read_hdf(\"data/preprocessed/vlq.h5\", key=\"vlq\")\n",
    "X_train = pd.concat([bkgd, vlq])\n",
    "del bkgd, vlq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train, test and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train.drop([\"Label\"], axis=1), X_train[\"Label\"], \n",
    "                                                    test_size=1/3, random_state=56)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train data\n",
    "X_train.to_hdf(\"data/classifier/train.h5\", key=\"X\")\n",
    "y_train.to_hdf(\"data/classifier/train.h5\", key=\"y\")\n",
    "\n",
    "# Save validation data\n",
    "X_val.to_hdf(\"data/classifier/validation.h5\", key=\"X\")\n",
    "y_val.to_hdf(\"data/classifier/validation.h5\", key=\"y\")\n",
    "\n",
    "# Save test data\n",
    "X_test.to_hdf(\"data/classifier/test.h5\", key=\"X\")\n",
    "y_test.to_hdf(\"data/classifier/test.h5\", key=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data samples\n",
    "train_samples, val_samples, test_samples = X_train[\"Sample\"], X_val[\"Sample\"], X_test[\"Sample\"]\n",
    "\n",
    "# Get data weights\n",
    "train_weights, val_weights, test_weights = X_train[\"gen_weights\"], X_val[\"gen_weights\"], X_test[\"gen_weights\"]\n",
    "\n",
    "# Remove sample and weight columns\n",
    "X_train.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "X_val.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "X_test.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {\n",
    "    0: 1,\n",
    "    1: len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using this class doesn't allow for model loading later\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_samples):\n",
    "        self.means = np.mean(data_samples, axis=0, keepdims=True)\n",
    "        self.stds = np.std(data_samples, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means) / (self.stds + keras.backend.epsilon())\n",
    "    \n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hidden_layers=[100, 100, 100], dropout=0.1, batch_norm=True, optimizer=\"Nadam\"):\n",
    "    \"\"\"\n",
    "    This function creates a keras model, given the desired hidden_layers, dropout rate\n",
    "    and optimizer of choice\n",
    "    \n",
    "    hidden_layers -> [int]: size of each desired hidden layer\n",
    "    dropout -> float: desired dropout rate\n",
    "    optimizer -> string: optimizer you choose to utilize\n",
    "    \n",
    "    returns a keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Generate model structure\n",
    "    inputs = keras.Input(shape=(69,))\n",
    "    bn = keras.layers.BatchNormalization()(inputs)\n",
    "    drop = bn\n",
    "    for i in range(len(hidden_layers)-1):\n",
    "        fc = keras.layers.Dense(hidden_layers[i], activation='relu')(drop)\n",
    "        if batch_norm:\n",
    "            bn = keras.layers.BatchNormalization()(fc)\n",
    "        else:\n",
    "            bn = fc\n",
    "        drop = keras.layers.Dropout(dropout)(bn, training=True)\n",
    "    fc = keras.layers.Dense(hidden_layers[-1], activation='relu')(drop)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(fc)\n",
    "    \n",
    "    # Instanciate and compile model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras.metrics.AUC()])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "hidden_layers = [100, 100, 100]\n",
    "batch_size = 1024\n",
    "dropout = 0.15\n",
    "batch_norm = True\n",
    "optimizer = \"Nadam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               7000      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 28,377\n",
      "Trainable params: 27,839\n",
      "Non-trainable params: 538\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "model = get_model(hidden_layers, dropout, batch_norm, optimizer)\n",
    "\n",
    "# Model name\n",
    "name = \"Hidden:\" + str(hidden_layers).replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\") + \"|\"\n",
    "name += f\"BatchS:{batch_size}|Dropout:{dropout}|\"\n",
    "if batch_norm: name += \"BatchNorm|\"\n",
    "name += optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hidden:100,100,100|BatchS:1024|Dropout:0.15|BatchNorm|Nadam'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "TB = keras.callbacks.TensorBoard(\"logs/\" + name, write_images=True)\n",
    "\n",
    "# Early Stopping\n",
    "ES = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")\n",
    "\n",
    "# Model Checkpoint\n",
    "MC = keras.callbacks.ModelCheckpoint(\"models/\" + name + \".h5\", save_best_only=True, monitor=\"val_loss\",\n",
    "                                     mode=\"min\")\n",
    "\n",
    "# Reduce LR on Plateau\n",
    "LR = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, mode=\"min\", \n",
    "                                       min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  1/270 [..............................] - ETA: 0s - loss: 4.1683e-04 - accuracy: 0.4990 - auc: 0.3435WARNING:tensorflow:From /home/gilbertocunha/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.0145s). Check your callbacks.\n",
      "270/270 [==============================] - 14s 51ms/step - loss: 1.5492e-05 - accuracy: 0.8573 - auc: 0.9247 - val_loss: 2.0607e-06 - val_accuracy: 0.8526 - val_auc: 0.9572\n",
      "Epoch 2/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.6634e-06 - accuracy: 0.8300 - auc: 0.9437 - val_loss: 1.1953e-06 - val_accuracy: 0.8498 - val_auc: 0.9502\n",
      "Epoch 3/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.3808e-06 - accuracy: 0.8452 - auc: 0.9447 - val_loss: 8.2291e-07 - val_accuracy: 0.8055 - val_auc: 0.9233\n",
      "Epoch 4/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.4425e-06 - accuracy: 0.8211 - auc: 0.9328 - val_loss: 9.4542e-07 - val_accuracy: 0.8443 - val_auc: 0.9450\n",
      "Epoch 5/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.2075e-06 - accuracy: 0.8451 - auc: 0.9438 - val_loss: 1.0556e-06 - val_accuracy: 0.8683 - val_auc: 0.9518\n",
      "Epoch 6/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.1071e-06 - accuracy: 0.8622 - auc: 0.9498 - val_loss: 8.7171e-07 - val_accuracy: 0.8589 - val_auc: 0.9487\n",
      "Epoch 7/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.1011e-06 - accuracy: 0.8621 - auc: 0.9501 - val_loss: 8.9495e-07 - val_accuracy: 0.8599 - val_auc: 0.9491\n",
      "Epoch 8/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0563e-06 - accuracy: 0.8638 - auc: 0.9505 - val_loss: 8.7925e-07 - val_accuracy: 0.8667 - val_auc: 0.9521\n",
      "Epoch 9/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0617e-06 - accuracy: 0.8654 - auc: 0.9511 - val_loss: 1.1043e-06 - val_accuracy: 0.8671 - val_auc: 0.9514\n",
      "Epoch 10/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0813e-06 - accuracy: 0.8675 - auc: 0.9514 - val_loss: 8.5301e-07 - val_accuracy: 0.8667 - val_auc: 0.9512\n",
      "Epoch 11/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0453e-06 - accuracy: 0.8686 - auc: 0.9515 - val_loss: 7.8317e-07 - val_accuracy: 0.8687 - val_auc: 0.9522\n",
      "Epoch 12/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0489e-06 - accuracy: 0.8692 - auc: 0.9520 - val_loss: 9.5523e-07 - val_accuracy: 0.8680 - val_auc: 0.9516\n",
      "Epoch 13/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0378e-06 - accuracy: 0.8688 - auc: 0.9521 - val_loss: 9.6282e-07 - val_accuracy: 0.8675 - val_auc: 0.9511\n",
      "Epoch 14/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0504e-06 - accuracy: 0.8688 - auc: 0.9521 - val_loss: 9.7741e-07 - val_accuracy: 0.8686 - val_auc: 0.9524\n",
      "Epoch 15/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0269e-06 - accuracy: 0.8693 - auc: 0.9520 - val_loss: 8.3272e-07 - val_accuracy: 0.8683 - val_auc: 0.9510\n",
      "Epoch 16/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0618e-06 - accuracy: 0.8696 - auc: 0.9521 - val_loss: 1.0389e-06 - val_accuracy: 0.8679 - val_auc: 0.9512\n",
      "Epoch 17/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0580e-06 - accuracy: 0.8691 - auc: 0.9515 - val_loss: 9.7173e-07 - val_accuracy: 0.8679 - val_auc: 0.9518\n",
      "Epoch 18/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0463e-06 - accuracy: 0.8685 - auc: 0.9516 - val_loss: 9.2298e-07 - val_accuracy: 0.8682 - val_auc: 0.9516\n",
      "Epoch 19/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0508e-06 - accuracy: 0.8685 - auc: 0.9515 - val_loss: 7.5779e-07 - val_accuracy: 0.8673 - val_auc: 0.9515\n",
      "Epoch 20/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0375e-06 - accuracy: 0.8686 - auc: 0.9517 - val_loss: 9.9338e-07 - val_accuracy: 0.8680 - val_auc: 0.9521\n",
      "Epoch 21/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0061e-06 - accuracy: 0.8693 - auc: 0.9512 - val_loss: 9.5905e-07 - val_accuracy: 0.8677 - val_auc: 0.9516\n",
      "Epoch 22/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0411e-06 - accuracy: 0.8688 - auc: 0.9518 - val_loss: 9.7434e-07 - val_accuracy: 0.8683 - val_auc: 0.9516\n",
      "Epoch 23/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0637e-06 - accuracy: 0.8687 - auc: 0.9522 - val_loss: 1.0111e-06 - val_accuracy: 0.8677 - val_auc: 0.9515\n",
      "Epoch 24/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0287e-06 - accuracy: 0.8688 - auc: 0.9525 - val_loss: 9.0659e-07 - val_accuracy: 0.8680 - val_auc: 0.9512\n",
      "Epoch 25/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0094e-06 - accuracy: 0.8690 - auc: 0.9525 - val_loss: 1.0446e-06 - val_accuracy: 0.8678 - val_auc: 0.9516\n",
      "Epoch 26/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0740e-06 - accuracy: 0.8681 - auc: 0.9515 - val_loss: 9.5276e-07 - val_accuracy: 0.8676 - val_auc: 0.9512\n",
      "Epoch 27/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0764e-06 - accuracy: 0.8689 - auc: 0.9515 - val_loss: 8.6173e-07 - val_accuracy: 0.8674 - val_auc: 0.9512\n",
      "Epoch 28/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0604e-06 - accuracy: 0.8686 - auc: 0.9522 - val_loss: 1.0051e-06 - val_accuracy: 0.8679 - val_auc: 0.9517\n",
      "Epoch 29/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0344e-06 - accuracy: 0.8686 - auc: 0.9516 - val_loss: 9.2100e-07 - val_accuracy: 0.8669 - val_auc: 0.9513\n",
      "Epoch 30/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0579e-06 - accuracy: 0.8685 - auc: 0.9523 - val_loss: 1.0084e-06 - val_accuracy: 0.8679 - val_auc: 0.9516\n",
      "Epoch 31/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0442e-06 - accuracy: 0.8686 - auc: 0.9518 - val_loss: 9.4392e-07 - val_accuracy: 0.8677 - val_auc: 0.9515\n",
      "Epoch 32/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0214e-06 - accuracy: 0.8686 - auc: 0.9512 - val_loss: 8.6031e-07 - val_accuracy: 0.8670 - val_auc: 0.9513\n",
      "Epoch 33/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0567e-06 - accuracy: 0.8684 - auc: 0.9518 - val_loss: 9.4536e-07 - val_accuracy: 0.8671 - val_auc: 0.9515\n",
      "Epoch 34/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0362e-06 - accuracy: 0.8687 - auc: 0.9521 - val_loss: 8.9470e-07 - val_accuracy: 0.8664 - val_auc: 0.9508\n",
      "Epoch 35/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0243e-06 - accuracy: 0.8688 - auc: 0.9518 - val_loss: 8.7219e-07 - val_accuracy: 0.8679 - val_auc: 0.9516\n",
      "Epoch 36/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0197e-06 - accuracy: 0.8688 - auc: 0.9519 - val_loss: 1.0023e-06 - val_accuracy: 0.8679 - val_auc: 0.9514\n",
      "Epoch 37/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0482e-06 - accuracy: 0.8690 - auc: 0.9515 - val_loss: 9.9494e-07 - val_accuracy: 0.8681 - val_auc: 0.9515\n",
      "Epoch 38/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0225e-06 - accuracy: 0.8687 - auc: 0.9524 - val_loss: 8.5214e-07 - val_accuracy: 0.8676 - val_auc: 0.9516\n",
      "Epoch 39/500\n",
      "270/270 [==============================] - 2s 8ms/step - loss: 1.0784e-06 - accuracy: 0.8685 - auc: 0.9516 - val_loss: 1.0024e-06 - val_accuracy: 0.8676 - val_auc: 0.9514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f16663dd4f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.values, y_train.values, batch_size=batch_size, epochs=500, callbacks=[TB, ES, MC, LR],\n",
    "          validation_data=(X_val.values, y_val.values, val_weights.values), shuffle=True,\n",
    "          sample_weight=train_weights.values, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
