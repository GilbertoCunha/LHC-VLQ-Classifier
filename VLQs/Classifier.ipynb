{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import and rearrange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "bkgd = pd.read_hdf(\"data/preprocessed/bkgd.h5\", key=\"bkgd\")\n",
    "vlq = pd.read_hdf(\"data/preprocessed/vlq.h5\", key=\"vlq\")\n",
    "X_train = pd.concat([bkgd, vlq])\n",
    "del bkgd, vlq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train, test and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train.drop([\"Label\"], axis=1), X_train[\"Label\"], \n",
    "                                                    test_size=1/3, random_state=56)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train data\n",
    "X_train.to_hdf(\"data/classifier/train.h5\", key=\"X\")\n",
    "y_train.to_hdf(\"data/classifier/train.h5\", key=\"y\")\n",
    "\n",
    "# Save validation data\n",
    "X_val.to_hdf(\"data/classifier/validation.h5\", key=\"X\")\n",
    "y_val.to_hdf(\"data/classifier/validation.h5\", key=\"y\")\n",
    "\n",
    "# Save test data\n",
    "X_test.to_hdf(\"data/classifier/test.h5\", key=\"X\")\n",
    "y_test.to_hdf(\"data/classifier/test.h5\", key=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data samples\n",
    "train_samples, val_samples, test_samples = X_train[\"Sample\"], X_val[\"Sample\"], X_test[\"Sample\"]\n",
    "\n",
    "# Get data weights\n",
    "train_weights, val_weights, test_weights = X_train[\"gen_weights\"], X_val[\"gen_weights\"], X_test[\"gen_weights\"]\n",
    "\n",
    "# Remove sample and weight columns\n",
    "X_train.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "X_val.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "X_test.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {\n",
    "    0: 1,\n",
    "    1: len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: This doesn't use a standardization layer since I couldn't load saved models with this costum layer. BatchNorm is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hidden_layers=[100, 100, 100], dropout=0.1, batch_norm=True, optimizer=\"Nadam\"):\n",
    "    \"\"\"\n",
    "    This function creates a keras model, given the desired hidden_layers, dropout rate\n",
    "    and optimizer of choice\n",
    "    \n",
    "    hidden_layers -> [int]: size of each desired hidden layer\n",
    "    dropout -> float: desired dropout rate\n",
    "    optimizer -> string: optimizer you choose to utilize\n",
    "    \n",
    "    returns a keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate model structure\n",
    "    inputs = keras.Input(shape=(69,))\n",
    "    bn = keras.layers.BatchNormalization()(inputs)\n",
    "    drop = bn\n",
    "    for i in range(len(hidden_layers)-1):\n",
    "        fc = keras.layers.Dense(hidden_layers[i], activation='relu')(drop)\n",
    "        if batch_norm:\n",
    "            bn = keras.layers.BatchNormalization()(fc)\n",
    "        else:\n",
    "            bn = fc\n",
    "        drop = keras.layers.Dropout(dropout)(bn, training=True)\n",
    "    fc = keras.layers.Dense(hidden_layers[-1], activation='relu')(drop)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(fc)\n",
    "    \n",
    "    # Instanciate and compile model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras.metrics.AUC()])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Bayesian Hyperparameter Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for bayesian inference hyperparameter search\n",
    "    \n",
    "    trial -> optuna trial object\n",
    "    \n",
    "    return -> float: validation accuracy of the best model with early stopping and model checkpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining parameters\n",
    "    num_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
    "    hidden_layers = []\n",
    "    for i in range(num_layers):\n",
    "        num_features = trial.suggest_int(f\"num_features_layer_{i}\", 20, 150)\n",
    "        hidden_layers.append(num_features)\n",
    "    dropout = trial.suggest_discrete_uniform(\"dropout\", 0.05, 0.5, 0.01)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024])\n",
    "    batch_norm = trial.suggest_categorical(\"batch_norm\", [True, False])\n",
    "    optimizer = \"Adam\"\n",
    "    \n",
    "    # Create model\n",
    "    model = get_model(hidden_layers, dropout, batch_norm, optimizer)\n",
    "    name = f\"trial_{trial.number}\"\n",
    "    \n",
    "    # Callbacks\n",
    "    TB = keras.callbacks.TensorBoard(\"logs/\" + name, write_images=True)\n",
    "    ES = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")\n",
    "    MC = keras.callbacks.ModelCheckpoint(\"models/\" + name + \".h5\", save_best_only=True, monitor=\"val_loss\",\n",
    "                                     mode=\"min\")\n",
    "    LR = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, mode=\"min\", \n",
    "                                       min_lr=1e-6)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train.values, y_train.values, batch_size=batch_size, epochs=500, callbacks=[TB, ES, MC, LR],\n",
    "              validation_data=(X_val.values, y_val.values, val_weights.values), shuffle=True,\n",
    "              sample_weight=train_weights.values, class_weight=class_weights, verbose=2)\n",
    "    \n",
    "    # Get accuracy\n",
    "    y_preds = model.predict(X_val.values)\n",
    "    val_accuracy = accuracy_score(y_preds.round(), y_val.values)\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 76)                5320      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 76)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 51)                3927      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 51)                2652      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 52        \n",
      "=================================================================\n",
      "Total params: 12,227\n",
      "Trainable params: 12,089\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /home/gilbertocunha/.local/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0086s). Check your callbacks.\n",
      "270/270 - 13s - loss: 2.2048e-05 - accuracy: 0.7363 - auc: 0.7680 - val_loss: 1.3239e-06 - val_accuracy: 0.7407 - val_auc: 0.9146\n",
      "Epoch 2/500\n",
      "270/270 - 1s - loss: 1.8135e-06 - accuracy: 0.7455 - auc: 0.9236 - val_loss: 9.4213e-07 - val_accuracy: 0.7580 - val_auc: 0.9383\n",
      "Epoch 3/500\n",
      "270/270 - 1s - loss: 1.4897e-06 - accuracy: 0.7727 - auc: 0.9419 - val_loss: 9.0130e-07 - val_accuracy: 0.7904 - val_auc: 0.9439\n",
      "Epoch 4/500\n",
      "270/270 - 1s - loss: 1.3497e-06 - accuracy: 0.7961 - auc: 0.9464 - val_loss: 8.3798e-07 - val_accuracy: 0.8027 - val_auc: 0.9475\n",
      "Epoch 5/500\n",
      "270/270 - 1s - loss: 1.2277e-06 - accuracy: 0.8149 - auc: 0.9512 - val_loss: 7.7207e-07 - val_accuracy: 0.8210 - val_auc: 0.9514\n",
      "Epoch 6/500\n",
      "270/270 - 1s - loss: 1.1434e-06 - accuracy: 0.8286 - auc: 0.9520 - val_loss: 9.1327e-07 - val_accuracy: 0.8530 - val_auc: 0.9587\n",
      "Epoch 7/500\n",
      "270/270 - 1s - loss: 1.0980e-06 - accuracy: 0.8516 - auc: 0.9580 - val_loss: 8.9273e-07 - val_accuracy: 0.8484 - val_auc: 0.9565\n",
      "Epoch 8/500\n",
      "270/270 - 1s - loss: 1.1166e-06 - accuracy: 0.8457 - auc: 0.9564 - val_loss: 8.7801e-07 - val_accuracy: 0.8430 - val_auc: 0.9553\n",
      "Epoch 9/500\n",
      "270/270 - 1s - loss: 1.1192e-06 - accuracy: 0.8422 - auc: 0.9554 - val_loss: 8.0105e-07 - val_accuracy: 0.8418 - val_auc: 0.9540\n",
      "Epoch 10/500\n",
      "270/270 - 1s - loss: 1.0703e-06 - accuracy: 0.8433 - auc: 0.9553 - val_loss: 8.3999e-07 - val_accuracy: 0.8433 - val_auc: 0.9546\n",
      "Epoch 11/500\n",
      "270/270 - 1s - loss: 1.0752e-06 - accuracy: 0.8456 - auc: 0.9553 - val_loss: 7.6220e-07 - val_accuracy: 0.8445 - val_auc: 0.9546\n",
      "Epoch 12/500\n",
      "270/270 - 1s - loss: 1.0847e-06 - accuracy: 0.8448 - auc: 0.9548 - val_loss: 7.9529e-07 - val_accuracy: 0.8445 - val_auc: 0.9546\n",
      "Epoch 13/500\n",
      "270/270 - 1s - loss: 1.0603e-06 - accuracy: 0.8450 - auc: 0.9553 - val_loss: 8.0313e-07 - val_accuracy: 0.8442 - val_auc: 0.9547\n",
      "Epoch 14/500\n",
      "270/270 - 1s - loss: 1.0703e-06 - accuracy: 0.8447 - auc: 0.9553 - val_loss: 7.4914e-07 - val_accuracy: 0.8451 - val_auc: 0.9548\n",
      "Epoch 15/500\n",
      "270/270 - 1s - loss: 1.0875e-06 - accuracy: 0.8451 - auc: 0.9550 - val_loss: 8.3472e-07 - val_accuracy: 0.8455 - val_auc: 0.9549\n",
      "Epoch 16/500\n",
      "270/270 - 1s - loss: 1.0732e-06 - accuracy: 0.8453 - auc: 0.9553 - val_loss: 7.9086e-07 - val_accuracy: 0.8455 - val_auc: 0.9550\n",
      "Epoch 17/500\n",
      "270/270 - 1s - loss: 1.0668e-06 - accuracy: 0.8459 - auc: 0.9556 - val_loss: 7.8814e-07 - val_accuracy: 0.8454 - val_auc: 0.9547\n",
      "Epoch 18/500\n",
      "270/270 - 1s - loss: 1.0717e-06 - accuracy: 0.8463 - auc: 0.9550 - val_loss: 8.2506e-07 - val_accuracy: 0.8451 - val_auc: 0.9549\n",
      "Epoch 19/500\n",
      "270/270 - 1s - loss: 1.0410e-06 - accuracy: 0.8461 - auc: 0.9557 - val_loss: 8.5513e-07 - val_accuracy: 0.8463 - val_auc: 0.9551\n",
      "Epoch 20/500\n",
      "270/270 - 1s - loss: 1.0854e-06 - accuracy: 0.8462 - auc: 0.9561 - val_loss: 8.6987e-07 - val_accuracy: 0.8460 - val_auc: 0.9553\n",
      "Epoch 21/500\n",
      "270/270 - 1s - loss: 1.0657e-06 - accuracy: 0.8461 - auc: 0.9555 - val_loss: 8.0539e-07 - val_accuracy: 0.8464 - val_auc: 0.9549\n",
      "Epoch 22/500\n",
      "270/270 - 1s - loss: 1.0738e-06 - accuracy: 0.8465 - auc: 0.9552 - val_loss: 8.5305e-07 - val_accuracy: 0.8453 - val_auc: 0.9541\n",
      "Epoch 23/500\n",
      "270/270 - 1s - loss: 1.0491e-06 - accuracy: 0.8462 - auc: 0.9553 - val_loss: 8.1416e-07 - val_accuracy: 0.8463 - val_auc: 0.9552\n",
      "Epoch 24/500\n",
      "270/270 - 1s - loss: 1.0746e-06 - accuracy: 0.8461 - auc: 0.9555 - val_loss: 8.2399e-07 - val_accuracy: 0.8451 - val_auc: 0.9547\n",
      "Epoch 25/500\n",
      "270/270 - 1s - loss: 1.0814e-06 - accuracy: 0.8461 - auc: 0.9553 - val_loss: 8.3612e-07 - val_accuracy: 0.8456 - val_auc: 0.9551\n",
      "Epoch 26/500\n",
      "270/270 - 1s - loss: 1.0752e-06 - accuracy: 0.8468 - auc: 0.9551 - val_loss: 8.8425e-07 - val_accuracy: 0.8458 - val_auc: 0.9544\n",
      "Epoch 27/500\n",
      "270/270 - 1s - loss: 1.0768e-06 - accuracy: 0.8462 - auc: 0.9560 - val_loss: 7.9748e-07 - val_accuracy: 0.8456 - val_auc: 0.9552\n",
      "Epoch 28/500\n",
      "270/270 - 1s - loss: 1.0919e-06 - accuracy: 0.8464 - auc: 0.9555 - val_loss: 7.9949e-07 - val_accuracy: 0.8461 - val_auc: 0.9547\n",
      "Epoch 29/500\n",
      "270/270 - 1s - loss: 1.0767e-06 - accuracy: 0.8460 - auc: 0.9552 - val_loss: 7.8875e-07 - val_accuracy: 0.8456 - val_auc: 0.9550\n",
      "Epoch 30/500\n",
      "270/270 - 1s - loss: 1.0501e-06 - accuracy: 0.8463 - auc: 0.9561 - val_loss: 8.4109e-07 - val_accuracy: 0.8459 - val_auc: 0.9548\n",
      "Epoch 31/500\n",
      "270/270 - 1s - loss: 1.0701e-06 - accuracy: 0.8462 - auc: 0.9555 - val_loss: 8.1388e-07 - val_accuracy: 0.8460 - val_auc: 0.9547\n",
      "Epoch 32/500\n",
      "270/270 - 1s - loss: 1.0751e-06 - accuracy: 0.8464 - auc: 0.9556 - val_loss: 7.5434e-07 - val_accuracy: 0.8460 - val_auc: 0.9545\n",
      "Epoch 33/500\n",
      "270/270 - 1s - loss: 1.0748e-06 - accuracy: 0.8465 - auc: 0.9557 - val_loss: 8.3860e-07 - val_accuracy: 0.8451 - val_auc: 0.9545\n",
      "Epoch 34/500\n",
      "270/270 - 1s - loss: 1.0741e-06 - accuracy: 0.8470 - auc: 0.9555 - val_loss: 7.7206e-07 - val_accuracy: 0.8461 - val_auc: 0.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:31:28,376] Trial 0 finished with value: 0.8468313687964856 and parameters: {'num_hidden_layers': 3, 'num_features_layer_0': 76, 'num_features_layer_1': 51, 'num_features_layer_2': 51, 'dropout': 0.15000000000000002, 'batch_size': 1024, 'batch_norm': False}. Best is trial 0 with value: 0.8468313687964856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 77)                5390      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 77)                308       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 70)                5460      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 70)                280       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 59)                4189      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 59)                236       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 59)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 114)               6840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 114)               456       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 114)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 133)               15295     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 134       \n",
      "=================================================================\n",
      "Total params: 38,864\n",
      "Trainable params: 38,086\n",
      "Non-trainable params: 778\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0841s). Check your callbacks.\n",
      "539/539 - 15s - loss: 1.3616e-05 - accuracy: 0.7546 - auc_1: 0.8102 - val_loss: 1.3391e-06 - val_accuracy: 0.7657 - val_auc_1: 0.8759\n",
      "Epoch 2/500\n",
      "539/539 - 4s - loss: 2.0124e-06 - accuracy: 0.7680 - auc_1: 0.8775 - val_loss: 1.1287e-06 - val_accuracy: 0.7836 - val_auc_1: 0.8951\n",
      "Epoch 3/500\n",
      "539/539 - 4s - loss: 1.7269e-06 - accuracy: 0.7787 - auc_1: 0.9002 - val_loss: 9.4354e-07 - val_accuracy: 0.7830 - val_auc_1: 0.9089\n",
      "Epoch 4/500\n",
      "539/539 - 4s - loss: 1.5251e-06 - accuracy: 0.7977 - auc_1: 0.9169 - val_loss: 9.8772e-07 - val_accuracy: 0.8190 - val_auc_1: 0.9279\n",
      "Epoch 5/500\n",
      "539/539 - 4s - loss: 1.4249e-06 - accuracy: 0.8057 - auc_1: 0.9289 - val_loss: 9.3083e-07 - val_accuracy: 0.8368 - val_auc_1: 0.9394\n",
      "Epoch 6/500\n",
      "539/539 - 4s - loss: 1.3633e-06 - accuracy: 0.8115 - auc_1: 0.9357 - val_loss: 7.8188e-07 - val_accuracy: 0.8237 - val_auc_1: 0.9385\n",
      "Epoch 7/500\n",
      "539/539 - 4s - loss: 1.3251e-06 - accuracy: 0.8169 - auc_1: 0.9375 - val_loss: 7.8531e-07 - val_accuracy: 0.8186 - val_auc_1: 0.9383\n",
      "Epoch 8/500\n",
      "539/539 - 4s - loss: 1.2454e-06 - accuracy: 0.8204 - auc_1: 0.9391 - val_loss: 8.4663e-07 - val_accuracy: 0.8196 - val_auc_1: 0.9390\n",
      "Epoch 9/500\n",
      "539/539 - 4s - loss: 1.2349e-06 - accuracy: 0.8223 - auc_1: 0.9403 - val_loss: 7.8284e-07 - val_accuracy: 0.8200 - val_auc_1: 0.9391\n",
      "Epoch 10/500\n",
      "539/539 - 4s - loss: 1.2978e-06 - accuracy: 0.8181 - auc_1: 0.9406 - val_loss: 6.9180e-07 - val_accuracy: 0.8131 - val_auc_1: 0.9404\n",
      "Epoch 11/500\n",
      "539/539 - 4s - loss: 1.2441e-06 - accuracy: 0.8220 - auc_1: 0.9424 - val_loss: 7.4998e-07 - val_accuracy: 0.8193 - val_auc_1: 0.9422\n",
      "Epoch 12/500\n",
      "539/539 - 4s - loss: 1.2025e-06 - accuracy: 0.8203 - auc_1: 0.9424 - val_loss: 7.3022e-07 - val_accuracy: 0.8203 - val_auc_1: 0.9428\n",
      "Epoch 13/500\n",
      "539/539 - 4s - loss: 1.2098e-06 - accuracy: 0.8219 - auc_1: 0.9427 - val_loss: 7.5908e-07 - val_accuracy: 0.8215 - val_auc_1: 0.9426\n",
      "Epoch 14/500\n",
      "539/539 - 4s - loss: 1.2207e-06 - accuracy: 0.8226 - auc_1: 0.9431 - val_loss: 8.8346e-07 - val_accuracy: 0.8208 - val_auc_1: 0.9429\n",
      "Epoch 15/500\n",
      "539/539 - 4s - loss: 1.2729e-06 - accuracy: 0.8216 - auc_1: 0.9436 - val_loss: 7.4119e-07 - val_accuracy: 0.8209 - val_auc_1: 0.9424\n",
      "Epoch 16/500\n",
      "539/539 - 4s - loss: 1.2042e-06 - accuracy: 0.8220 - auc_1: 0.9437 - val_loss: 9.0460e-07 - val_accuracy: 0.8209 - val_auc_1: 0.9431\n",
      "Epoch 17/500\n",
      "539/539 - 4s - loss: 1.2233e-06 - accuracy: 0.8220 - auc_1: 0.9429 - val_loss: 8.1054e-07 - val_accuracy: 0.8201 - val_auc_1: 0.9427\n",
      "Epoch 18/500\n",
      "539/539 - 4s - loss: 1.2367e-06 - accuracy: 0.8215 - auc_1: 0.9431 - val_loss: 7.8453e-07 - val_accuracy: 0.8207 - val_auc_1: 0.9433\n",
      "Epoch 19/500\n",
      "539/539 - 4s - loss: 1.2005e-06 - accuracy: 0.8217 - auc_1: 0.9437 - val_loss: 7.1391e-07 - val_accuracy: 0.8206 - val_auc_1: 0.9435\n",
      "Epoch 20/500\n",
      "539/539 - 4s - loss: 1.2455e-06 - accuracy: 0.8220 - auc_1: 0.9439 - val_loss: 7.2473e-07 - val_accuracy: 0.8211 - val_auc_1: 0.9441\n",
      "Epoch 21/500\n",
      "539/539 - 4s - loss: 1.2142e-06 - accuracy: 0.8222 - auc_1: 0.9436 - val_loss: 7.8077e-07 - val_accuracy: 0.8200 - val_auc_1: 0.9437\n",
      "Epoch 22/500\n",
      "539/539 - 4s - loss: 1.1967e-06 - accuracy: 0.8222 - auc_1: 0.9429 - val_loss: 7.1800e-07 - val_accuracy: 0.8206 - val_auc_1: 0.9429\n",
      "Epoch 23/500\n",
      "539/539 - 4s - loss: 1.2636e-06 - accuracy: 0.8220 - auc_1: 0.9434 - val_loss: 7.6465e-07 - val_accuracy: 0.8202 - val_auc_1: 0.9434\n",
      "Epoch 24/500\n",
      "539/539 - 4s - loss: 1.2170e-06 - accuracy: 0.8219 - auc_1: 0.9435 - val_loss: 7.4052e-07 - val_accuracy: 0.8206 - val_auc_1: 0.9438\n",
      "Epoch 25/500\n",
      "539/539 - 4s - loss: 1.2206e-06 - accuracy: 0.8216 - auc_1: 0.9436 - val_loss: 8.2890e-07 - val_accuracy: 0.8206 - val_auc_1: 0.9431\n",
      "Epoch 26/500\n",
      "539/539 - 4s - loss: 1.2257e-06 - accuracy: 0.8216 - auc_1: 0.9438 - val_loss: 7.3744e-07 - val_accuracy: 0.8200 - val_auc_1: 0.9432\n",
      "Epoch 27/500\n",
      "539/539 - 4s - loss: 1.2676e-06 - accuracy: 0.8219 - auc_1: 0.9434 - val_loss: 7.1472e-07 - val_accuracy: 0.8202 - val_auc_1: 0.9429\n",
      "Epoch 28/500\n",
      "539/539 - 4s - loss: 1.2186e-06 - accuracy: 0.8219 - auc_1: 0.9432 - val_loss: 8.2840e-07 - val_accuracy: 0.8206 - val_auc_1: 0.9428\n",
      "Epoch 29/500\n",
      "539/539 - 4s - loss: 1.2331e-06 - accuracy: 0.8216 - auc_1: 0.9438 - val_loss: 7.7024e-07 - val_accuracy: 0.8194 - val_auc_1: 0.9427\n",
      "Epoch 30/500\n",
      "539/539 - 4s - loss: 1.2734e-06 - accuracy: 0.8214 - auc_1: 0.9431 - val_loss: 7.3225e-07 - val_accuracy: 0.8202 - val_auc_1: 0.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:33:51,959] Trial 1 finished with value: 0.8209553013498035 and parameters: {'num_hidden_layers': 5, 'num_features_layer_0': 77, 'num_features_layer_1': 70, 'num_features_layer_2': 59, 'num_features_layer_3': 114, 'num_features_layer_4': 133, 'dropout': 0.37, 'batch_size': 512, 'batch_norm': True}. Best is trial 0 with value: 0.8468313687964856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 68)                4760      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 68)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 49)                3381      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 67)                3350      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 67)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 56)                3808      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 57        \n",
      "=================================================================\n",
      "Total params: 15,632\n",
      "Trainable params: 15,494\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0640s). Check your callbacks.\n",
      "270/270 - 13s - loss: 1.3292e-05 - accuracy: 0.7400 - auc_2: 0.6450 - val_loss: 1.2644e-06 - val_accuracy: 0.7402 - val_auc_2: 0.8540\n",
      "Epoch 2/500\n",
      "270/270 - 2s - loss: 1.8770e-06 - accuracy: 0.7403 - auc_2: 0.8953 - val_loss: 9.0825e-07 - val_accuracy: 0.7402 - val_auc_2: 0.9258\n",
      "Epoch 3/500\n",
      "270/270 - 2s - loss: 1.5378e-06 - accuracy: 0.7403 - auc_2: 0.9346 - val_loss: 8.4443e-07 - val_accuracy: 0.7403 - val_auc_2: 0.9447\n",
      "Epoch 4/500\n",
      "270/270 - 2s - loss: 1.3929e-06 - accuracy: 0.7404 - auc_2: 0.9461 - val_loss: 7.8868e-07 - val_accuracy: 0.7405 - val_auc_2: 0.9486\n",
      "Epoch 5/500\n",
      "270/270 - 2s - loss: 1.3259e-06 - accuracy: 0.7410 - auc_2: 0.9479 - val_loss: 7.4269e-07 - val_accuracy: 0.7419 - val_auc_2: 0.9497\n",
      "Epoch 6/500\n",
      "270/270 - 2s - loss: 1.2055e-06 - accuracy: 0.7503 - auc_2: 0.9523 - val_loss: 7.5639e-07 - val_accuracy: 0.7640 - val_auc_2: 0.9532\n",
      "Epoch 7/500\n",
      "270/270 - 2s - loss: 1.1664e-06 - accuracy: 0.7647 - auc_2: 0.9539 - val_loss: 7.8508e-07 - val_accuracy: 0.7656 - val_auc_2: 0.9527\n",
      "Epoch 8/500\n",
      "270/270 - 2s - loss: 1.1786e-06 - accuracy: 0.7675 - auc_2: 0.9534 - val_loss: 7.5196e-07 - val_accuracy: 0.7684 - val_auc_2: 0.9530\n",
      "Epoch 9/500\n",
      "270/270 - 2s - loss: 1.1744e-06 - accuracy: 0.7699 - auc_2: 0.9542 - val_loss: 7.8250e-07 - val_accuracy: 0.7699 - val_auc_2: 0.9524\n",
      "Epoch 10/500\n",
      "270/270 - 2s - loss: 1.1562e-06 - accuracy: 0.7726 - auc_2: 0.9534 - val_loss: 7.3972e-07 - val_accuracy: 0.7752 - val_auc_2: 0.9532\n",
      "Epoch 11/500\n",
      "270/270 - 2s - loss: 1.1667e-06 - accuracy: 0.7765 - auc_2: 0.9533 - val_loss: 7.3440e-07 - val_accuracy: 0.7790 - val_auc_2: 0.9526\n",
      "Epoch 12/500\n",
      "270/270 - 2s - loss: 1.1247e-06 - accuracy: 0.7790 - auc_2: 0.9535 - val_loss: 7.1952e-07 - val_accuracy: 0.7790 - val_auc_2: 0.9522\n",
      "Epoch 13/500\n",
      "270/270 - 2s - loss: 1.1706e-06 - accuracy: 0.7800 - auc_2: 0.9537 - val_loss: 7.3251e-07 - val_accuracy: 0.7794 - val_auc_2: 0.9526\n",
      "Epoch 14/500\n",
      "270/270 - 2s - loss: 1.1406e-06 - accuracy: 0.7799 - auc_2: 0.9533 - val_loss: 7.5261e-07 - val_accuracy: 0.7801 - val_auc_2: 0.9532\n",
      "Epoch 15/500\n",
      "270/270 - 2s - loss: 1.1602e-06 - accuracy: 0.7807 - auc_2: 0.9535 - val_loss: 7.2219e-07 - val_accuracy: 0.7803 - val_auc_2: 0.9533\n",
      "Epoch 16/500\n",
      "270/270 - 2s - loss: 1.1384e-06 - accuracy: 0.7815 - auc_2: 0.9539 - val_loss: 7.6214e-07 - val_accuracy: 0.7811 - val_auc_2: 0.9530\n",
      "Epoch 17/500\n",
      "270/270 - 2s - loss: 1.1577e-06 - accuracy: 0.7818 - auc_2: 0.9542 - val_loss: 7.8249e-07 - val_accuracy: 0.7813 - val_auc_2: 0.9529\n",
      "Epoch 18/500\n",
      "270/270 - 2s - loss: 1.1512e-06 - accuracy: 0.7810 - auc_2: 0.9534 - val_loss: 6.9721e-07 - val_accuracy: 0.7810 - val_auc_2: 0.9528\n",
      "Epoch 19/500\n",
      "270/270 - 2s - loss: 1.1279e-06 - accuracy: 0.7821 - auc_2: 0.9537 - val_loss: 7.1589e-07 - val_accuracy: 0.7812 - val_auc_2: 0.9529\n",
      "Epoch 20/500\n",
      "270/270 - 2s - loss: 1.1241e-06 - accuracy: 0.7819 - auc_2: 0.9538 - val_loss: 7.3366e-07 - val_accuracy: 0.7814 - val_auc_2: 0.9534\n",
      "Epoch 21/500\n",
      "270/270 - 2s - loss: 1.1543e-06 - accuracy: 0.7824 - auc_2: 0.9539 - val_loss: 7.2701e-07 - val_accuracy: 0.7814 - val_auc_2: 0.9533\n",
      "Epoch 22/500\n",
      "270/270 - 2s - loss: 1.1351e-06 - accuracy: 0.7821 - auc_2: 0.9541 - val_loss: 7.5480e-07 - val_accuracy: 0.7816 - val_auc_2: 0.9534\n",
      "Epoch 23/500\n",
      "270/270 - 2s - loss: 1.1513e-06 - accuracy: 0.7816 - auc_2: 0.9535 - val_loss: 7.4686e-07 - val_accuracy: 0.7820 - val_auc_2: 0.9529\n",
      "Epoch 24/500\n",
      "270/270 - 2s - loss: 1.1492e-06 - accuracy: 0.7820 - auc_2: 0.9533 - val_loss: 7.8948e-07 - val_accuracy: 0.7820 - val_auc_2: 0.9533\n",
      "Epoch 25/500\n",
      "270/270 - 2s - loss: 1.1375e-06 - accuracy: 0.7821 - auc_2: 0.9537 - val_loss: 7.3083e-07 - val_accuracy: 0.7822 - val_auc_2: 0.9537\n",
      "Epoch 26/500\n",
      "270/270 - 2s - loss: 1.1785e-06 - accuracy: 0.7818 - auc_2: 0.9536 - val_loss: 7.2816e-07 - val_accuracy: 0.7820 - val_auc_2: 0.9532\n",
      "Epoch 27/500\n",
      "270/270 - 2s - loss: 1.1277e-06 - accuracy: 0.7825 - auc_2: 0.9544 - val_loss: 7.3905e-07 - val_accuracy: 0.7816 - val_auc_2: 0.9530\n",
      "Epoch 28/500\n",
      "270/270 - 2s - loss: 1.1581e-06 - accuracy: 0.7832 - auc_2: 0.9537 - val_loss: 7.4925e-07 - val_accuracy: 0.7822 - val_auc_2: 0.9530\n",
      "Epoch 29/500\n",
      "270/270 - 2s - loss: 1.1295e-06 - accuracy: 0.7827 - auc_2: 0.9538 - val_loss: 7.5685e-07 - val_accuracy: 0.7820 - val_auc_2: 0.9526\n",
      "Epoch 30/500\n",
      "270/270 - 2s - loss: 1.1329e-06 - accuracy: 0.7827 - auc_2: 0.9544 - val_loss: 7.6622e-07 - val_accuracy: 0.7825 - val_auc_2: 0.9530\n",
      "Epoch 31/500\n",
      "270/270 - 2s - loss: 1.1362e-06 - accuracy: 0.7828 - auc_2: 0.9540 - val_loss: 7.2054e-07 - val_accuracy: 0.7826 - val_auc_2: 0.9530\n",
      "Epoch 32/500\n",
      "270/270 - 2s - loss: 1.1721e-06 - accuracy: 0.7831 - auc_2: 0.9538 - val_loss: 7.4632e-07 - val_accuracy: 0.7821 - val_auc_2: 0.9529\n",
      "Epoch 33/500\n",
      "270/270 - 2s - loss: 1.1397e-06 - accuracy: 0.7830 - auc_2: 0.9540 - val_loss: 7.2167e-07 - val_accuracy: 0.7826 - val_auc_2: 0.9533\n",
      "Epoch 34/500\n",
      "270/270 - 2s - loss: 1.1718e-06 - accuracy: 0.7830 - auc_2: 0.9541 - val_loss: 7.5249e-07 - val_accuracy: 0.7826 - val_auc_2: 0.9530\n",
      "Epoch 35/500\n",
      "270/270 - 2s - loss: 1.1322e-06 - accuracy: 0.7832 - auc_2: 0.9532 - val_loss: 7.6628e-07 - val_accuracy: 0.7828 - val_auc_2: 0.9532\n",
      "Epoch 36/500\n",
      "270/270 - 2s - loss: 1.1559e-06 - accuracy: 0.7827 - auc_2: 0.9539 - val_loss: 7.2554e-07 - val_accuracy: 0.7819 - val_auc_2: 0.9533\n",
      "Epoch 37/500\n",
      "270/270 - 2s - loss: 1.1452e-06 - accuracy: 0.7833 - auc_2: 0.9543 - val_loss: 7.1165e-07 - val_accuracy: 0.7830 - val_auc_2: 0.9531\n",
      "Epoch 38/500\n",
      "270/270 - 2s - loss: 1.1898e-06 - accuracy: 0.7835 - auc_2: 0.9541 - val_loss: 7.4356e-07 - val_accuracy: 0.7833 - val_auc_2: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:35:20,781] Trial 2 finished with value: 0.783027416525307 and parameters: {'num_hidden_layers': 4, 'num_features_layer_0': 68, 'num_features_layer_1': 49, 'num_features_layer_2': 67, 'num_features_layer_3': 56, 'dropout': 0.22999999999999998, 'batch_size': 1024, 'batch_norm': False}. Best is trial 0 with value: 0.8468313687964856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 59)                4130      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 60        \n",
      "=================================================================\n",
      "Total params: 4,466\n",
      "Trainable params: 4,328\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0495s). Check your callbacks.\n",
      "539/539 - 13s - loss: 2.3227e-05 - accuracy: 0.8595 - auc_3: 0.9466 - val_loss: 1.5021e-06 - val_accuracy: 0.7965 - val_auc_3: 0.9751\n",
      "Epoch 2/500\n",
      "539/539 - 1s - loss: 1.6355e-06 - accuracy: 0.7927 - auc_3: 0.9712 - val_loss: 8.5530e-07 - val_accuracy: 0.7990 - val_auc_3: 0.9690\n",
      "Epoch 3/500\n",
      "539/539 - 1s - loss: 1.2942e-06 - accuracy: 0.8117 - auc_3: 0.9672 - val_loss: 7.2455e-07 - val_accuracy: 0.8136 - val_auc_3: 0.9658\n",
      "Epoch 4/500\n",
      "539/539 - 1s - loss: 1.1461e-06 - accuracy: 0.8299 - auc_3: 0.9657 - val_loss: 6.8723e-07 - val_accuracy: 0.8311 - val_auc_3: 0.9644\n",
      "Epoch 5/500\n",
      "539/539 - 1s - loss: 1.0542e-06 - accuracy: 0.8445 - auc_3: 0.9654 - val_loss: 7.0242e-07 - val_accuracy: 0.8522 - val_auc_3: 0.9649\n",
      "Epoch 6/500\n",
      "539/539 - 1s - loss: 9.9607e-07 - accuracy: 0.8572 - auc_3: 0.9650 - val_loss: 7.7793e-07 - val_accuracy: 0.8645 - val_auc_3: 0.9658\n",
      "Epoch 7/500\n",
      "539/539 - 1s - loss: 9.5161e-07 - accuracy: 0.8658 - auc_3: 0.9661 - val_loss: 7.6954e-07 - val_accuracy: 0.8645 - val_auc_3: 0.9659\n",
      "Epoch 8/500\n",
      "539/539 - 1s - loss: 9.5162e-07 - accuracy: 0.8647 - auc_3: 0.9660 - val_loss: 7.5376e-07 - val_accuracy: 0.8642 - val_auc_3: 0.9660\n",
      "Epoch 9/500\n",
      "539/539 - 1s - loss: 9.4054e-07 - accuracy: 0.8653 - auc_3: 0.9656 - val_loss: 7.5758e-07 - val_accuracy: 0.8650 - val_auc_3: 0.9657\n",
      "Epoch 10/500\n",
      "539/539 - 1s - loss: 9.2815e-07 - accuracy: 0.8677 - auc_3: 0.9662 - val_loss: 7.7946e-07 - val_accuracy: 0.8679 - val_auc_3: 0.9660\n",
      "Epoch 11/500\n",
      "539/539 - 1s - loss: 9.2334e-07 - accuracy: 0.8689 - auc_3: 0.9659 - val_loss: 7.8397e-07 - val_accuracy: 0.8687 - val_auc_3: 0.9661\n",
      "Epoch 12/500\n",
      "539/539 - 1s - loss: 9.1901e-07 - accuracy: 0.8688 - auc_3: 0.9658 - val_loss: 7.7644e-07 - val_accuracy: 0.8681 - val_auc_3: 0.9658\n",
      "Epoch 13/500\n",
      "539/539 - 1s - loss: 9.2331e-07 - accuracy: 0.8689 - auc_3: 0.9660 - val_loss: 7.6807e-07 - val_accuracy: 0.8673 - val_auc_3: 0.9654\n",
      "Epoch 14/500\n",
      "539/539 - 1s - loss: 9.2183e-07 - accuracy: 0.8690 - auc_3: 0.9659 - val_loss: 7.8209e-07 - val_accuracy: 0.8686 - val_auc_3: 0.9659\n",
      "Epoch 15/500\n",
      "539/539 - 1s - loss: 9.2204e-07 - accuracy: 0.8694 - auc_3: 0.9659 - val_loss: 7.7419e-07 - val_accuracy: 0.8681 - val_auc_3: 0.9657\n",
      "Epoch 16/500\n",
      "539/539 - 1s - loss: 9.1470e-07 - accuracy: 0.8697 - auc_3: 0.9659 - val_loss: 7.8018e-07 - val_accuracy: 0.8688 - val_auc_3: 0.9659\n",
      "Epoch 17/500\n",
      "539/539 - 1s - loss: 9.1467e-07 - accuracy: 0.8694 - auc_3: 0.9660 - val_loss: 7.7803e-07 - val_accuracy: 0.8689 - val_auc_3: 0.9659\n",
      "Epoch 18/500\n",
      "539/539 - 1s - loss: 9.1955e-07 - accuracy: 0.8695 - auc_3: 0.9659 - val_loss: 7.8042e-07 - val_accuracy: 0.8688 - val_auc_3: 0.9659\n",
      "Epoch 19/500\n",
      "539/539 - 1s - loss: 9.1777e-07 - accuracy: 0.8694 - auc_3: 0.9660 - val_loss: 7.7992e-07 - val_accuracy: 0.8690 - val_auc_3: 0.9659\n",
      "Epoch 20/500\n",
      "539/539 - 1s - loss: 9.1344e-07 - accuracy: 0.8696 - auc_3: 0.9661 - val_loss: 7.8524e-07 - val_accuracy: 0.8694 - val_auc_3: 0.9661\n",
      "Epoch 21/500\n",
      "539/539 - 1s - loss: 9.1612e-07 - accuracy: 0.8699 - auc_3: 0.9659 - val_loss: 7.8717e-07 - val_accuracy: 0.8696 - val_auc_3: 0.9660\n",
      "Epoch 22/500\n",
      "539/539 - 1s - loss: 9.1945e-07 - accuracy: 0.8694 - auc_3: 0.9663 - val_loss: 7.7743e-07 - val_accuracy: 0.8687 - val_auc_3: 0.9658\n",
      "Epoch 23/500\n",
      "539/539 - 1s - loss: 9.1259e-07 - accuracy: 0.8697 - auc_3: 0.9662 - val_loss: 7.8108e-07 - val_accuracy: 0.8693 - val_auc_3: 0.9661\n",
      "Epoch 24/500\n",
      "539/539 - 1s - loss: 9.2007e-07 - accuracy: 0.8697 - auc_3: 0.9663 - val_loss: 7.7518e-07 - val_accuracy: 0.8687 - val_auc_3: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:36:15,146] Trial 3 finished with value: 0.8686659997390284 and parameters: {'num_hidden_layers': 1, 'num_features_layer_0': 59, 'dropout': 0.41, 'batch_size': 512, 'batch_norm': True}. Best is trial 3 with value: 0.8686659997390284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 92)                6440      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 130)               12090     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 131       \n",
      "=================================================================\n",
      "Total params: 18,937\n",
      "Trainable params: 18,799\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0429s). Check your callbacks.\n",
      "539/539 - 13s - loss: 1.0129e-05 - accuracy: 0.7413 - auc_4: 0.8730 - val_loss: 8.1466e-07 - val_accuracy: 0.7620 - val_auc_4: 0.9531\n",
      "Epoch 2/500\n",
      "539/539 - 2s - loss: 1.3765e-06 - accuracy: 0.7922 - auc_4: 0.9528 - val_loss: 6.8579e-07 - val_accuracy: 0.8068 - val_auc_4: 0.9560\n",
      "Epoch 3/500\n",
      "539/539 - 2s - loss: 1.1268e-06 - accuracy: 0.8304 - auc_4: 0.9589 - val_loss: 9.2401e-07 - val_accuracy: 0.8704 - val_auc_4: 0.9638\n",
      "Epoch 4/500\n",
      "539/539 - 2s - loss: 1.0127e-06 - accuracy: 0.8567 - auc_4: 0.9590 - val_loss: 7.9469e-07 - val_accuracy: 0.8607 - val_auc_4: 0.9588\n",
      "Epoch 5/500\n",
      "539/539 - 2s - loss: 9.6611e-07 - accuracy: 0.8610 - auc_4: 0.9612 - val_loss: 6.5010e-07 - val_accuracy: 0.8605 - val_auc_4: 0.9600\n",
      "Epoch 6/500\n",
      "539/539 - 2s - loss: 9.0085e-07 - accuracy: 0.8738 - auc_4: 0.9627 - val_loss: 7.6108e-07 - val_accuracy: 0.8744 - val_auc_4: 0.9614\n",
      "Epoch 7/500\n",
      "539/539 - 2s - loss: 8.5472e-07 - accuracy: 0.8797 - auc_4: 0.9637 - val_loss: 7.6438e-07 - val_accuracy: 0.8804 - val_auc_4: 0.9634\n",
      "Epoch 8/500\n",
      "539/539 - 2s - loss: 8.5492e-07 - accuracy: 0.8826 - auc_4: 0.9642 - val_loss: 7.7307e-07 - val_accuracy: 0.8822 - val_auc_4: 0.9638\n",
      "Epoch 9/500\n",
      "539/539 - 2s - loss: 8.3755e-07 - accuracy: 0.8852 - auc_4: 0.9648 - val_loss: 8.5605e-07 - val_accuracy: 0.8863 - val_auc_4: 0.9654\n",
      "Epoch 10/500\n",
      "539/539 - 2s - loss: 8.3391e-07 - accuracy: 0.8859 - auc_4: 0.9652 - val_loss: 8.4692e-07 - val_accuracy: 0.8860 - val_auc_4: 0.9653\n",
      "Epoch 11/500\n",
      "539/539 - 2s - loss: 8.3726e-07 - accuracy: 0.8854 - auc_4: 0.9655 - val_loss: 8.5727e-07 - val_accuracy: 0.8840 - val_auc_4: 0.9645\n",
      "Epoch 12/500\n",
      "539/539 - 2s - loss: 8.3241e-07 - accuracy: 0.8848 - auc_4: 0.9653 - val_loss: 8.3324e-07 - val_accuracy: 0.8847 - val_auc_4: 0.9649\n",
      "Epoch 13/500\n",
      "539/539 - 2s - loss: 8.3959e-07 - accuracy: 0.8852 - auc_4: 0.9653 - val_loss: 8.2938e-07 - val_accuracy: 0.8849 - val_auc_4: 0.9648\n",
      "Epoch 14/500\n",
      "539/539 - 2s - loss: 8.3340e-07 - accuracy: 0.8853 - auc_4: 0.9651 - val_loss: 8.9331e-07 - val_accuracy: 0.8859 - val_auc_4: 0.9654\n",
      "Epoch 15/500\n",
      "539/539 - 2s - loss: 8.3012e-07 - accuracy: 0.8866 - auc_4: 0.9651 - val_loss: 8.3357e-07 - val_accuracy: 0.8858 - val_auc_4: 0.9650\n",
      "Epoch 16/500\n",
      "539/539 - 2s - loss: 8.4260e-07 - accuracy: 0.8861 - auc_4: 0.9655 - val_loss: 8.0184e-07 - val_accuracy: 0.8863 - val_auc_4: 0.9650\n",
      "Epoch 17/500\n",
      "539/539 - 2s - loss: 8.2983e-07 - accuracy: 0.8860 - auc_4: 0.9655 - val_loss: 8.0149e-07 - val_accuracy: 0.8854 - val_auc_4: 0.9652\n",
      "Epoch 18/500\n",
      "539/539 - 2s - loss: 8.2624e-07 - accuracy: 0.8863 - auc_4: 0.9655 - val_loss: 7.4819e-07 - val_accuracy: 0.8852 - val_auc_4: 0.9652\n",
      "Epoch 19/500\n",
      "539/539 - 2s - loss: 8.3537e-07 - accuracy: 0.8869 - auc_4: 0.9651 - val_loss: 8.3466e-07 - val_accuracy: 0.8857 - val_auc_4: 0.9651\n",
      "Epoch 20/500\n",
      "539/539 - 2s - loss: 8.2846e-07 - accuracy: 0.8862 - auc_4: 0.9655 - val_loss: 7.9463e-07 - val_accuracy: 0.8865 - val_auc_4: 0.9649\n",
      "Epoch 21/500\n",
      "539/539 - 2s - loss: 8.2790e-07 - accuracy: 0.8860 - auc_4: 0.9654 - val_loss: 8.2005e-07 - val_accuracy: 0.8856 - val_auc_4: 0.9653\n",
      "Epoch 22/500\n",
      "539/539 - 2s - loss: 8.2540e-07 - accuracy: 0.8865 - auc_4: 0.9653 - val_loss: 8.2374e-07 - val_accuracy: 0.8859 - val_auc_4: 0.9649\n",
      "Epoch 23/500\n",
      "539/539 - 2s - loss: 8.2913e-07 - accuracy: 0.8864 - auc_4: 0.9654 - val_loss: 9.0882e-07 - val_accuracy: 0.8850 - val_auc_4: 0.9649\n",
      "Epoch 24/500\n",
      "539/539 - 2s - loss: 8.1532e-07 - accuracy: 0.8866 - auc_4: 0.9657 - val_loss: 8.4013e-07 - val_accuracy: 0.8853 - val_auc_4: 0.9649\n",
      "Epoch 25/500\n",
      "539/539 - 2s - loss: 8.2656e-07 - accuracy: 0.8869 - auc_4: 0.9653 - val_loss: 8.1031e-07 - val_accuracy: 0.8855 - val_auc_4: 0.9648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:37:23,286] Trial 4 finished with value: 0.8857958911458107 and parameters: {'num_hidden_layers': 2, 'num_features_layer_0': 92, 'num_features_layer_1': 130, 'dropout': 0.1, 'batch_size': 512, 'batch_norm': False}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 125)               8750      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 149)               18774     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 150       \n",
      "=================================================================\n",
      "Total params: 27,950\n",
      "Trainable params: 27,812\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0452s). Check your callbacks.\n",
      "270/270 - 12s - loss: 1.7091e-05 - accuracy: 0.7336 - auc_5: 0.6958 - val_loss: 1.3646e-06 - val_accuracy: 0.7403 - val_auc_5: 0.9004\n",
      "Epoch 2/500\n",
      "270/270 - 1s - loss: 1.8232e-06 - accuracy: 0.7473 - auc_5: 0.9265 - val_loss: 9.9465e-07 - val_accuracy: 0.7688 - val_auc_5: 0.9461\n",
      "Epoch 3/500\n",
      "270/270 - 1s - loss: 1.5132e-06 - accuracy: 0.7773 - auc_5: 0.9451 - val_loss: 8.3173e-07 - val_accuracy: 0.7893 - val_auc_5: 0.9476\n",
      "Epoch 4/500\n",
      "270/270 - 1s - loss: 1.3610e-06 - accuracy: 0.7999 - auc_5: 0.9494 - val_loss: 8.4001e-07 - val_accuracy: 0.8153 - val_auc_5: 0.9513\n",
      "Epoch 5/500\n",
      "270/270 - 1s - loss: 1.2627e-06 - accuracy: 0.8164 - auc_5: 0.9526 - val_loss: 6.3588e-07 - val_accuracy: 0.8051 - val_auc_5: 0.9489\n",
      "Epoch 6/500\n",
      "270/270 - 1s - loss: 1.1852e-06 - accuracy: 0.8253 - auc_5: 0.9542 - val_loss: 7.1834e-07 - val_accuracy: 0.8258 - val_auc_5: 0.9525\n",
      "Epoch 7/500\n",
      "270/270 - 1s - loss: 1.1261e-06 - accuracy: 0.8305 - auc_5: 0.9541 - val_loss: 7.9505e-07 - val_accuracy: 0.8307 - val_auc_5: 0.9545\n",
      "Epoch 8/500\n",
      "270/270 - 1s - loss: 1.1123e-06 - accuracy: 0.8336 - auc_5: 0.9548 - val_loss: 7.9617e-07 - val_accuracy: 0.8352 - val_auc_5: 0.9552\n",
      "Epoch 9/500\n",
      "270/270 - 1s - loss: 1.1101e-06 - accuracy: 0.8355 - auc_5: 0.9555 - val_loss: 7.1704e-07 - val_accuracy: 0.8370 - val_auc_5: 0.9554\n",
      "Epoch 10/500\n",
      "270/270 - 1s - loss: 1.1234e-06 - accuracy: 0.8380 - auc_5: 0.9561 - val_loss: 7.3345e-07 - val_accuracy: 0.8385 - val_auc_5: 0.9557\n",
      "Epoch 11/500\n",
      "270/270 - 1s - loss: 1.1016e-06 - accuracy: 0.8397 - auc_5: 0.9560 - val_loss: 8.5483e-07 - val_accuracy: 0.8387 - val_auc_5: 0.9557\n",
      "Epoch 12/500\n",
      "270/270 - 1s - loss: 1.1078e-06 - accuracy: 0.8387 - auc_5: 0.9561 - val_loss: 7.3545e-07 - val_accuracy: 0.8378 - val_auc_5: 0.9552\n",
      "Epoch 13/500\n",
      "270/270 - 1s - loss: 1.1023e-06 - accuracy: 0.8385 - auc_5: 0.9560 - val_loss: 8.8688e-07 - val_accuracy: 0.8383 - val_auc_5: 0.9554\n",
      "Epoch 14/500\n",
      "270/270 - 1s - loss: 1.0908e-06 - accuracy: 0.8392 - auc_5: 0.9561 - val_loss: 7.8094e-07 - val_accuracy: 0.8387 - val_auc_5: 0.9554\n",
      "Epoch 15/500\n",
      "270/270 - 1s - loss: 1.0881e-06 - accuracy: 0.8391 - auc_5: 0.9561 - val_loss: 6.9162e-07 - val_accuracy: 0.8388 - val_auc_5: 0.9553\n",
      "Epoch 16/500\n",
      "270/270 - 1s - loss: 1.0876e-06 - accuracy: 0.8400 - auc_5: 0.9560 - val_loss: 8.0804e-07 - val_accuracy: 0.8389 - val_auc_5: 0.9555\n",
      "Epoch 17/500\n",
      "270/270 - 1s - loss: 1.0638e-06 - accuracy: 0.8405 - auc_5: 0.9564 - val_loss: 7.7164e-07 - val_accuracy: 0.8396 - val_auc_5: 0.9558\n",
      "Epoch 18/500\n",
      "270/270 - 1s - loss: 1.0878e-06 - accuracy: 0.8402 - auc_5: 0.9559 - val_loss: 8.3784e-07 - val_accuracy: 0.8400 - val_auc_5: 0.9560\n",
      "Epoch 19/500\n",
      "270/270 - 1s - loss: 1.0879e-06 - accuracy: 0.8404 - auc_5: 0.9567 - val_loss: 7.9524e-07 - val_accuracy: 0.8397 - val_auc_5: 0.9557\n",
      "Epoch 20/500\n",
      "270/270 - 1s - loss: 1.0925e-06 - accuracy: 0.8406 - auc_5: 0.9560 - val_loss: 8.2221e-07 - val_accuracy: 0.8397 - val_auc_5: 0.9549\n",
      "Epoch 21/500\n",
      "270/270 - 1s - loss: 1.0913e-06 - accuracy: 0.8407 - auc_5: 0.9563 - val_loss: 7.8677e-07 - val_accuracy: 0.8402 - val_auc_5: 0.9558\n",
      "Epoch 22/500\n",
      "270/270 - 1s - loss: 1.1072e-06 - accuracy: 0.8402 - auc_5: 0.9565 - val_loss: 8.1562e-07 - val_accuracy: 0.8397 - val_auc_5: 0.9561\n",
      "Epoch 23/500\n",
      "270/270 - 1s - loss: 1.0931e-06 - accuracy: 0.8401 - auc_5: 0.9559 - val_loss: 7.8154e-07 - val_accuracy: 0.8403 - val_auc_5: 0.9561\n",
      "Epoch 24/500\n",
      "270/270 - 1s - loss: 1.0723e-06 - accuracy: 0.8406 - auc_5: 0.9562 - val_loss: 8.0404e-07 - val_accuracy: 0.8399 - val_auc_5: 0.9558\n",
      "Epoch 25/500\n",
      "270/270 - 1s - loss: 1.0769e-06 - accuracy: 0.8407 - auc_5: 0.9562 - val_loss: 7.7251e-07 - val_accuracy: 0.8403 - val_auc_5: 0.9557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:38:24,107] Trial 5 finished with value: 0.8401910892668145 and parameters: {'num_hidden_layers': 2, 'num_features_layer_0': 125, 'num_features_layer_1': 149, 'dropout': 0.4, 'batch_size': 1024, 'batch_norm': False}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 94)                6580      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 94)                376       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 94)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 123)               11685     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 123)               492       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 123)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 106)               13144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 106)               424       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 106)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 41)                4387      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 42        \n",
      "=================================================================\n",
      "Total params: 37,406\n",
      "Trainable params: 36,622\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0749s). Check your callbacks.\n",
      "270/270 - 15s - loss: 4.7101e-05 - accuracy: 0.8588 - auc_6: 0.9093 - val_loss: 6.9875e-06 - val_accuracy: 0.8916 - val_auc_6: 0.9623\n",
      "Epoch 2/500\n",
      "270/270 - 3s - loss: 3.0377e-06 - accuracy: 0.8430 - auc_6: 0.9431 - val_loss: 2.2592e-06 - val_accuracy: 0.8337 - val_auc_6: 0.9439\n",
      "Epoch 3/500\n",
      "270/270 - 3s - loss: 2.0973e-06 - accuracy: 0.8120 - auc_6: 0.9362 - val_loss: 1.2537e-06 - val_accuracy: 0.8089 - val_auc_6: 0.9342\n",
      "Epoch 4/500\n",
      "270/270 - 3s - loss: 1.8948e-06 - accuracy: 0.7951 - auc_6: 0.9295 - val_loss: 1.1063e-06 - val_accuracy: 0.7888 - val_auc_6: 0.9281\n",
      "Epoch 5/500\n",
      "270/270 - 3s - loss: 1.6840e-06 - accuracy: 0.7911 - auc_6: 0.9284 - val_loss: 8.6512e-07 - val_accuracy: 0.7917 - val_auc_6: 0.9277\n",
      "Epoch 6/500\n",
      "270/270 - 3s - loss: 1.5132e-06 - accuracy: 0.7982 - auc_6: 0.9295 - val_loss: 1.0058e-06 - val_accuracy: 0.8118 - val_auc_6: 0.9323\n",
      "Epoch 7/500\n",
      "270/270 - 3s - loss: 1.5972e-06 - accuracy: 0.8096 - auc_6: 0.9317 - val_loss: 8.9235e-07 - val_accuracy: 0.8087 - val_auc_6: 0.9315\n",
      "Epoch 8/500\n",
      "270/270 - 3s - loss: 1.4642e-06 - accuracy: 0.8088 - auc_6: 0.9325 - val_loss: 9.6908e-07 - val_accuracy: 0.8082 - val_auc_6: 0.9317\n",
      "Epoch 9/500\n",
      "270/270 - 3s - loss: 1.5294e-06 - accuracy: 0.8090 - auc_6: 0.9321 - val_loss: 9.5718e-07 - val_accuracy: 0.8084 - val_auc_6: 0.9324\n",
      "Epoch 10/500\n",
      "270/270 - 3s - loss: 1.4585e-06 - accuracy: 0.8096 - auc_6: 0.9323 - val_loss: 9.5240e-07 - val_accuracy: 0.8098 - val_auc_6: 0.9328\n",
      "Epoch 11/500\n",
      "270/270 - 3s - loss: 1.5288e-06 - accuracy: 0.8074 - auc_6: 0.9331 - val_loss: 8.4669e-07 - val_accuracy: 0.8068 - val_auc_6: 0.9321\n",
      "Epoch 12/500\n",
      "270/270 - 3s - loss: 1.4428e-06 - accuracy: 0.8081 - auc_6: 0.9327 - val_loss: 9.3323e-07 - val_accuracy: 0.8064 - val_auc_6: 0.9323\n",
      "Epoch 13/500\n",
      "270/270 - 3s - loss: 1.4931e-06 - accuracy: 0.8078 - auc_6: 0.9333 - val_loss: 1.0037e-06 - val_accuracy: 0.8074 - val_auc_6: 0.9327\n",
      "Epoch 14/500\n",
      "270/270 - 3s - loss: 1.5662e-06 - accuracy: 0.8077 - auc_6: 0.9339 - val_loss: 9.0340e-07 - val_accuracy: 0.8069 - val_auc_6: 0.9331\n",
      "Epoch 15/500\n",
      "270/270 - 3s - loss: 1.4688e-06 - accuracy: 0.8072 - auc_6: 0.9331 - val_loss: 1.0050e-06 - val_accuracy: 0.8070 - val_auc_6: 0.9326\n",
      "Epoch 16/500\n",
      "270/270 - 3s - loss: 1.5022e-06 - accuracy: 0.8071 - auc_6: 0.9333 - val_loss: 8.6660e-07 - val_accuracy: 0.8065 - val_auc_6: 0.9330\n",
      "Epoch 17/500\n",
      "270/270 - 3s - loss: 1.4970e-06 - accuracy: 0.8073 - auc_6: 0.9330 - val_loss: 8.9477e-07 - val_accuracy: 0.8067 - val_auc_6: 0.9330\n",
      "Epoch 18/500\n",
      "270/270 - 3s - loss: 1.5042e-06 - accuracy: 0.8071 - auc_6: 0.9333 - val_loss: 8.9947e-07 - val_accuracy: 0.8066 - val_auc_6: 0.9322\n",
      "Epoch 19/500\n",
      "270/270 - 3s - loss: 1.5047e-06 - accuracy: 0.8071 - auc_6: 0.9328 - val_loss: 8.5848e-07 - val_accuracy: 0.8066 - val_auc_6: 0.9329\n",
      "Epoch 20/500\n",
      "270/270 - 3s - loss: 1.4718e-06 - accuracy: 0.8079 - auc_6: 0.9331 - val_loss: 8.7816e-07 - val_accuracy: 0.8072 - val_auc_6: 0.9325\n",
      "Epoch 21/500\n",
      "270/270 - 3s - loss: 1.4941e-06 - accuracy: 0.8076 - auc_6: 0.9331 - val_loss: 1.0402e-06 - val_accuracy: 0.8066 - val_auc_6: 0.9319\n",
      "Epoch 22/500\n",
      "270/270 - 3s - loss: 1.4613e-06 - accuracy: 0.8076 - auc_6: 0.9333 - val_loss: 9.0764e-07 - val_accuracy: 0.8065 - val_auc_6: 0.9320\n",
      "Epoch 23/500\n",
      "270/270 - 3s - loss: 1.4826e-06 - accuracy: 0.8074 - auc_6: 0.9333 - val_loss: 9.2947e-07 - val_accuracy: 0.8071 - val_auc_6: 0.9331\n",
      "Epoch 24/500\n",
      "270/270 - 3s - loss: 1.4903e-06 - accuracy: 0.8078 - auc_6: 0.9329 - val_loss: 9.9260e-07 - val_accuracy: 0.8070 - val_auc_6: 0.9336\n",
      "Epoch 25/500\n",
      "270/270 - 3s - loss: 1.6259e-06 - accuracy: 0.8074 - auc_6: 0.9337 - val_loss: 9.1551e-07 - val_accuracy: 0.8076 - val_auc_6: 0.9329\n",
      "Epoch 26/500\n",
      "270/270 - 3s - loss: 1.5290e-06 - accuracy: 0.8076 - auc_6: 0.9325 - val_loss: 9.4834e-07 - val_accuracy: 0.8064 - val_auc_6: 0.9326\n",
      "Epoch 27/500\n",
      "270/270 - 3s - loss: 1.4958e-06 - accuracy: 0.8074 - auc_6: 0.9331 - val_loss: 1.0121e-06 - val_accuracy: 0.8065 - val_auc_6: 0.9332\n",
      "Epoch 28/500\n",
      "270/270 - 3s - loss: 1.4789e-06 - accuracy: 0.8076 - auc_6: 0.9339 - val_loss: 8.4230e-07 - val_accuracy: 0.8064 - val_auc_6: 0.9332\n",
      "Epoch 29/500\n",
      "270/270 - 3s - loss: 1.5298e-06 - accuracy: 0.8069 - auc_6: 0.9333 - val_loss: 8.6397e-07 - val_accuracy: 0.8060 - val_auc_6: 0.9327\n",
      "Epoch 30/500\n",
      "270/270 - 3s - loss: 1.4470e-06 - accuracy: 0.8070 - auc_6: 0.9329 - val_loss: 1.0201e-06 - val_accuracy: 0.8069 - val_auc_6: 0.9326\n",
      "Epoch 31/500\n",
      "270/270 - 3s - loss: 1.4922e-06 - accuracy: 0.8072 - auc_6: 0.9324 - val_loss: 8.5389e-07 - val_accuracy: 0.8063 - val_auc_6: 0.9325\n",
      "Epoch 32/500\n",
      "270/270 - 3s - loss: 1.4768e-06 - accuracy: 0.8074 - auc_6: 0.9329 - val_loss: 8.9594e-07 - val_accuracy: 0.8067 - val_auc_6: 0.9332\n",
      "Epoch 33/500\n",
      "270/270 - 3s - loss: 1.4739e-06 - accuracy: 0.8070 - auc_6: 0.9330 - val_loss: 9.4854e-07 - val_accuracy: 0.8072 - val_auc_6: 0.9325\n",
      "Epoch 34/500\n",
      "270/270 - 3s - loss: 1.4656e-06 - accuracy: 0.8073 - auc_6: 0.9330 - val_loss: 9.1901e-07 - val_accuracy: 0.8076 - val_auc_6: 0.9331\n",
      "Epoch 35/500\n",
      "270/270 - 3s - loss: 1.5022e-06 - accuracy: 0.8069 - auc_6: 0.9336 - val_loss: 8.6156e-07 - val_accuracy: 0.8069 - val_auc_6: 0.9334\n",
      "Epoch 36/500\n",
      "270/270 - 3s - loss: 1.4459e-06 - accuracy: 0.8073 - auc_6: 0.9344 - val_loss: 8.3317e-07 - val_accuracy: 0.8071 - val_auc_6: 0.9332\n",
      "Epoch 37/500\n",
      "270/270 - 3s - loss: 1.5232e-06 - accuracy: 0.8080 - auc_6: 0.9333 - val_loss: 9.7291e-07 - val_accuracy: 0.8069 - val_auc_6: 0.9323\n",
      "Epoch 38/500\n",
      "270/270 - 3s - loss: 1.4278e-06 - accuracy: 0.8074 - auc_6: 0.9329 - val_loss: 8.7731e-07 - val_accuracy: 0.8063 - val_auc_6: 0.9337\n",
      "Epoch 39/500\n",
      "270/270 - 3s - loss: 1.5533e-06 - accuracy: 0.8069 - auc_6: 0.9331 - val_loss: 9.1217e-07 - val_accuracy: 0.8072 - val_auc_6: 0.9335\n",
      "Epoch 40/500\n",
      "270/270 - 3s - loss: 1.4808e-06 - accuracy: 0.8072 - auc_6: 0.9334 - val_loss: 8.9895e-07 - val_accuracy: 0.8067 - val_auc_6: 0.9327\n",
      "Epoch 41/500\n",
      "270/270 - 3s - loss: 1.5276e-06 - accuracy: 0.8073 - auc_6: 0.9333 - val_loss: 9.2664e-07 - val_accuracy: 0.8071 - val_auc_6: 0.9332\n",
      "Epoch 42/500\n",
      "270/270 - 3s - loss: 1.4241e-06 - accuracy: 0.8081 - auc_6: 0.9336 - val_loss: 9.0079e-07 - val_accuracy: 0.8060 - val_auc_6: 0.9321\n",
      "Epoch 43/500\n",
      "270/270 - 3s - loss: 1.4231e-06 - accuracy: 0.8070 - auc_6: 0.9334 - val_loss: 7.6189e-07 - val_accuracy: 0.8062 - val_auc_6: 0.9317\n",
      "Epoch 44/500\n",
      "270/270 - 3s - loss: 1.4699e-06 - accuracy: 0.8069 - auc_6: 0.9334 - val_loss: 8.7651e-07 - val_accuracy: 0.8068 - val_auc_6: 0.9332\n",
      "Epoch 45/500\n",
      "270/270 - 3s - loss: 1.4287e-06 - accuracy: 0.8076 - auc_6: 0.9335 - val_loss: 8.8302e-07 - val_accuracy: 0.8065 - val_auc_6: 0.9320\n",
      "Epoch 46/500\n",
      "270/270 - 3s - loss: 1.4835e-06 - accuracy: 0.8070 - auc_6: 0.9329 - val_loss: 9.2703e-07 - val_accuracy: 0.8055 - val_auc_6: 0.9322\n",
      "Epoch 47/500\n",
      "270/270 - 3s - loss: 1.4354e-06 - accuracy: 0.8072 - auc_6: 0.9333 - val_loss: 9.3591e-07 - val_accuracy: 0.8064 - val_auc_6: 0.9330\n",
      "Epoch 48/500\n",
      "270/270 - 3s - loss: 1.5594e-06 - accuracy: 0.8070 - auc_6: 0.9332 - val_loss: 9.4001e-07 - val_accuracy: 0.8068 - val_auc_6: 0.9327\n",
      "Epoch 49/500\n",
      "270/270 - 3s - loss: 1.4664e-06 - accuracy: 0.8073 - auc_6: 0.9335 - val_loss: 9.4542e-07 - val_accuracy: 0.8068 - val_auc_6: 0.9318\n",
      "Epoch 50/500\n",
      "270/270 - 3s - loss: 1.4251e-06 - accuracy: 0.8072 - auc_6: 0.9333 - val_loss: 8.6024e-07 - val_accuracy: 0.8064 - val_auc_6: 0.9323\n",
      "Epoch 51/500\n",
      "270/270 - 3s - loss: 1.4997e-06 - accuracy: 0.8070 - auc_6: 0.9332 - val_loss: 8.6267e-07 - val_accuracy: 0.8063 - val_auc_6: 0.9328\n",
      "Epoch 52/500\n",
      "270/270 - 3s - loss: 1.4519e-06 - accuracy: 0.8071 - auc_6: 0.9332 - val_loss: 9.2374e-07 - val_accuracy: 0.8060 - val_auc_6: 0.9327\n",
      "Epoch 53/500\n",
      "270/270 - 3s - loss: 1.4539e-06 - accuracy: 0.8067 - auc_6: 0.9328 - val_loss: 8.6655e-07 - val_accuracy: 0.8069 - val_auc_6: 0.9329\n",
      "Epoch 54/500\n",
      "270/270 - 3s - loss: 1.4299e-06 - accuracy: 0.8076 - auc_6: 0.9338 - val_loss: 8.6755e-07 - val_accuracy: 0.8065 - val_auc_6: 0.9328\n",
      "Epoch 55/500\n",
      "270/270 - 3s - loss: 1.4324e-06 - accuracy: 0.8069 - auc_6: 0.9338 - val_loss: 9.6643e-07 - val_accuracy: 0.8063 - val_auc_6: 0.9326\n",
      "Epoch 56/500\n",
      "270/270 - 3s - loss: 1.4235e-06 - accuracy: 0.8073 - auc_6: 0.9334 - val_loss: 8.4277e-07 - val_accuracy: 0.8065 - val_auc_6: 0.9331\n",
      "Epoch 57/500\n",
      "270/270 - 3s - loss: 1.4744e-06 - accuracy: 0.8072 - auc_6: 0.9335 - val_loss: 8.4477e-07 - val_accuracy: 0.8070 - val_auc_6: 0.9329\n",
      "Epoch 58/500\n",
      "270/270 - 3s - loss: 1.4915e-06 - accuracy: 0.8079 - auc_6: 0.9331 - val_loss: 9.3848e-07 - val_accuracy: 0.8059 - val_auc_6: 0.9323\n",
      "Epoch 59/500\n",
      "270/270 - 3s - loss: 1.4651e-06 - accuracy: 0.8071 - auc_6: 0.9333 - val_loss: 9.1820e-07 - val_accuracy: 0.8066 - val_auc_6: 0.9318\n",
      "Epoch 60/500\n",
      "270/270 - 3s - loss: 1.4655e-06 - accuracy: 0.8069 - auc_6: 0.9335 - val_loss: 8.2580e-07 - val_accuracy: 0.8067 - val_auc_6: 0.9329\n",
      "Epoch 61/500\n",
      "270/270 - 3s - loss: 1.4894e-06 - accuracy: 0.8078 - auc_6: 0.9330 - val_loss: 9.0967e-07 - val_accuracy: 0.8073 - val_auc_6: 0.9315\n",
      "Epoch 62/500\n",
      "270/270 - 3s - loss: 1.4941e-06 - accuracy: 0.8078 - auc_6: 0.9330 - val_loss: 9.0606e-07 - val_accuracy: 0.8067 - val_auc_6: 0.9326\n",
      "Epoch 63/500\n",
      "270/270 - 3s - loss: 1.5069e-06 - accuracy: 0.8075 - auc_6: 0.9333 - val_loss: 7.9593e-07 - val_accuracy: 0.8066 - val_auc_6: 0.9335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:41:51,757] Trial 6 finished with value: 0.80700056543865 and parameters: {'num_hidden_layers': 4, 'num_features_layer_0': 94, 'num_features_layer_1': 123, 'num_features_layer_2': 106, 'num_features_layer_3': 41, 'dropout': 0.5, 'batch_size': 1024, 'batch_norm': True}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               8960      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 23)                2967      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 65)                1560      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 39)                2574      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 39)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 104)               4160      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 105       \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,464\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0605s). Check your callbacks.\n",
      "270/270 - 14s - loss: 1.9305e-05 - accuracy: 0.7377 - auc_7: 0.6838 - val_loss: 1.3644e-06 - val_accuracy: 0.7402 - val_auc_7: 0.8600\n",
      "Epoch 2/500\n",
      "270/270 - 2s - loss: 2.1625e-06 - accuracy: 0.7403 - auc_7: 0.8913 - val_loss: 1.0021e-06 - val_accuracy: 0.7402 - val_auc_7: 0.9177\n",
      "Epoch 3/500\n",
      "270/270 - 2s - loss: 1.7920e-06 - accuracy: 0.7403 - auc_7: 0.9260 - val_loss: 9.1601e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9384\n",
      "Epoch 4/500\n",
      "270/270 - 2s - loss: 1.5949e-06 - accuracy: 0.7403 - auc_7: 0.9413 - val_loss: 8.3445e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9417\n",
      "Epoch 5/500\n",
      "270/270 - 2s - loss: 1.4486e-06 - accuracy: 0.7403 - auc_7: 0.9457 - val_loss: 7.8058e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9482\n",
      "Epoch 6/500\n",
      "270/270 - 2s - loss: 1.3645e-06 - accuracy: 0.7403 - auc_7: 0.9500 - val_loss: 7.4998e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9483\n",
      "Epoch 7/500\n",
      "270/270 - 2s - loss: 1.3218e-06 - accuracy: 0.7403 - auc_7: 0.9495 - val_loss: 7.6239e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9498\n",
      "Epoch 8/500\n",
      "270/270 - 2s - loss: 1.3014e-06 - accuracy: 0.7403 - auc_7: 0.9505 - val_loss: 7.4223e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9498\n",
      "Epoch 9/500\n",
      "270/270 - 2s - loss: 1.2978e-06 - accuracy: 0.7403 - auc_7: 0.9518 - val_loss: 7.1890e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9506\n",
      "Epoch 10/500\n",
      "270/270 - 2s - loss: 1.2942e-06 - accuracy: 0.7403 - auc_7: 0.9517 - val_loss: 7.8931e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9513\n",
      "Epoch 11/500\n",
      "270/270 - 2s - loss: 1.2708e-06 - accuracy: 0.7403 - auc_7: 0.9517 - val_loss: 7.9571e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9517\n",
      "Epoch 12/500\n",
      "270/270 - 2s - loss: 1.3018e-06 - accuracy: 0.7403 - auc_7: 0.9519 - val_loss: 7.6969e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9518\n",
      "Epoch 13/500\n",
      "270/270 - 2s - loss: 1.2656e-06 - accuracy: 0.7403 - auc_7: 0.9522 - val_loss: 7.5938e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9514\n",
      "Epoch 14/500\n",
      "270/270 - 2s - loss: 1.2757e-06 - accuracy: 0.7403 - auc_7: 0.9518 - val_loss: 7.5290e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9511\n",
      "Epoch 15/500\n",
      "270/270 - 2s - loss: 1.2512e-06 - accuracy: 0.7403 - auc_7: 0.9527 - val_loss: 7.5337e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9517\n",
      "Epoch 16/500\n",
      "270/270 - 2s - loss: 1.2758e-06 - accuracy: 0.7403 - auc_7: 0.9523 - val_loss: 7.4373e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9519\n",
      "Epoch 17/500\n",
      "270/270 - 2s - loss: 1.2946e-06 - accuracy: 0.7403 - auc_7: 0.9520 - val_loss: 7.6032e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9513\n",
      "Epoch 18/500\n",
      "270/270 - 2s - loss: 1.2577e-06 - accuracy: 0.7403 - auc_7: 0.9519 - val_loss: 7.7529e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9519\n",
      "Epoch 19/500\n",
      "270/270 - 2s - loss: 1.2550e-06 - accuracy: 0.7403 - auc_7: 0.9524 - val_loss: 7.3247e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9516\n",
      "Epoch 20/500\n",
      "270/270 - 2s - loss: 1.2998e-06 - accuracy: 0.7403 - auc_7: 0.9523 - val_loss: 7.6393e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9520\n",
      "Epoch 21/500\n",
      "270/270 - 2s - loss: 1.2565e-06 - accuracy: 0.7403 - auc_7: 0.9525 - val_loss: 8.0588e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9515\n",
      "Epoch 22/500\n",
      "270/270 - 2s - loss: 1.2741e-06 - accuracy: 0.7403 - auc_7: 0.9529 - val_loss: 7.9322e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9520\n",
      "Epoch 23/500\n",
      "270/270 - 2s - loss: 1.2489e-06 - accuracy: 0.7403 - auc_7: 0.9522 - val_loss: 7.6193e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9511\n",
      "Epoch 24/500\n",
      "270/270 - 2s - loss: 1.2624e-06 - accuracy: 0.7403 - auc_7: 0.9528 - val_loss: 7.9096e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9517\n",
      "Epoch 25/500\n",
      "270/270 - 2s - loss: 1.2804e-06 - accuracy: 0.7403 - auc_7: 0.9520 - val_loss: 7.6604e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9522\n",
      "Epoch 26/500\n",
      "270/270 - 2s - loss: 1.2654e-06 - accuracy: 0.7403 - auc_7: 0.9522 - val_loss: 7.7825e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9514\n",
      "Epoch 27/500\n",
      "270/270 - 2s - loss: 1.2538e-06 - accuracy: 0.7403 - auc_7: 0.9526 - val_loss: 7.6852e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9523\n",
      "Epoch 28/500\n",
      "270/270 - 2s - loss: 1.3012e-06 - accuracy: 0.7403 - auc_7: 0.9520 - val_loss: 7.4409e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9520\n",
      "Epoch 29/500\n",
      "270/270 - 2s - loss: 1.2562e-06 - accuracy: 0.7403 - auc_7: 0.9527 - val_loss: 7.1027e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9515\n",
      "Epoch 30/500\n",
      "270/270 - 2s - loss: 1.2713e-06 - accuracy: 0.7403 - auc_7: 0.9523 - val_loss: 7.6508e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9514\n",
      "Epoch 31/500\n",
      "270/270 - 2s - loss: 1.2658e-06 - accuracy: 0.7403 - auc_7: 0.9527 - val_loss: 7.3964e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9518\n",
      "Epoch 32/500\n",
      "270/270 - 2s - loss: 1.2498e-06 - accuracy: 0.7403 - auc_7: 0.9524 - val_loss: 7.4515e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9518\n",
      "Epoch 33/500\n",
      "270/270 - 2s - loss: 1.2620e-06 - accuracy: 0.7403 - auc_7: 0.9524 - val_loss: 7.6014e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9520\n",
      "Epoch 34/500\n",
      "270/270 - 2s - loss: 1.2586e-06 - accuracy: 0.7403 - auc_7: 0.9529 - val_loss: 7.7680e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9514\n",
      "Epoch 35/500\n",
      "270/270 - 2s - loss: 1.2411e-06 - accuracy: 0.7403 - auc_7: 0.9525 - val_loss: 7.6142e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9531\n",
      "Epoch 36/500\n",
      "270/270 - 2s - loss: 1.2632e-06 - accuracy: 0.7403 - auc_7: 0.9528 - val_loss: 7.5249e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9516\n",
      "Epoch 37/500\n",
      "270/270 - 2s - loss: 1.2579e-06 - accuracy: 0.7403 - auc_7: 0.9520 - val_loss: 7.5732e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9521\n",
      "Epoch 38/500\n",
      "270/270 - 2s - loss: 1.2778e-06 - accuracy: 0.7403 - auc_7: 0.9528 - val_loss: 7.3948e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9520\n",
      "Epoch 39/500\n",
      "270/270 - 2s - loss: 1.2560e-06 - accuracy: 0.7403 - auc_7: 0.9529 - val_loss: 7.5918e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9519\n",
      "Epoch 40/500\n",
      "270/270 - 2s - loss: 1.2365e-06 - accuracy: 0.7403 - auc_7: 0.9525 - val_loss: 7.6907e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9516\n",
      "Epoch 41/500\n",
      "270/270 - 2s - loss: 1.2721e-06 - accuracy: 0.7403 - auc_7: 0.9522 - val_loss: 7.6302e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9517\n",
      "Epoch 42/500\n",
      "270/270 - 2s - loss: 1.2749e-06 - accuracy: 0.7403 - auc_7: 0.9525 - val_loss: 7.1443e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9527\n",
      "Epoch 43/500\n",
      "270/270 - 2s - loss: 1.2879e-06 - accuracy: 0.7403 - auc_7: 0.9525 - val_loss: 7.8618e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9515\n",
      "Epoch 44/500\n",
      "270/270 - 2s - loss: 1.2825e-06 - accuracy: 0.7403 - auc_7: 0.9523 - val_loss: 7.6830e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9522\n",
      "Epoch 45/500\n",
      "270/270 - 2s - loss: 1.2715e-06 - accuracy: 0.7403 - auc_7: 0.9522 - val_loss: 7.5773e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9521\n",
      "Epoch 46/500\n",
      "270/270 - 2s - loss: 1.2665e-06 - accuracy: 0.7403 - auc_7: 0.9528 - val_loss: 7.6320e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9527\n",
      "Epoch 47/500\n",
      "270/270 - 2s - loss: 1.2753e-06 - accuracy: 0.7403 - auc_7: 0.9525 - val_loss: 7.4088e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9522\n",
      "Epoch 48/500\n",
      "270/270 - 2s - loss: 1.2803e-06 - accuracy: 0.7403 - auc_7: 0.9529 - val_loss: 7.7190e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9521\n",
      "Epoch 49/500\n",
      "270/270 - 2s - loss: 1.3124e-06 - accuracy: 0.7403 - auc_7: 0.9527 - val_loss: 7.3403e-07 - val_accuracy: 0.7402 - val_auc_7: 0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:43:52,081] Trial 7 finished with value: 0.7402498078958433 and parameters: {'num_hidden_layers': 5, 'num_features_layer_0': 128, 'num_features_layer_1': 23, 'num_features_layer_2': 65, 'num_features_layer_3': 39, 'num_features_layer_4': 104, 'dropout': 0.33999999999999997, 'batch_size': 1024, 'batch_norm': False}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 129)               9030      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 130       \n",
      "=================================================================\n",
      "Total params: 9,436\n",
      "Trainable params: 9,298\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0490s). Check your callbacks.\n",
      "539/539 - 13s - loss: 1.2944e-05 - accuracy: 0.8184 - auc_8: 0.9497 - val_loss: 1.1176e-06 - val_accuracy: 0.7776 - val_auc_8: 0.9767\n",
      "Epoch 2/500\n",
      "539/539 - 1s - loss: 1.3966e-06 - accuracy: 0.7888 - auc_8: 0.9737 - val_loss: 7.6843e-07 - val_accuracy: 0.7926 - val_auc_8: 0.9695\n",
      "Epoch 3/500\n",
      "539/539 - 1s - loss: 1.1751e-06 - accuracy: 0.8134 - auc_8: 0.9690 - val_loss: 7.2845e-07 - val_accuracy: 0.8231 - val_auc_8: 0.9683\n",
      "Epoch 4/500\n",
      "539/539 - 1s - loss: 1.0702e-06 - accuracy: 0.8333 - auc_8: 0.9677 - val_loss: 7.3299e-07 - val_accuracy: 0.8436 - val_auc_8: 0.9679\n",
      "Epoch 5/500\n",
      "539/539 - 1s - loss: 9.9495e-07 - accuracy: 0.8509 - auc_8: 0.9674 - val_loss: 7.1346e-07 - val_accuracy: 0.8453 - val_auc_8: 0.9645\n",
      "Epoch 6/500\n",
      "539/539 - 1s - loss: 9.3733e-07 - accuracy: 0.8609 - auc_8: 0.9664 - val_loss: 7.3274e-07 - val_accuracy: 0.8652 - val_auc_8: 0.9671\n",
      "Epoch 7/500\n",
      "539/539 - 1s - loss: 8.9914e-07 - accuracy: 0.8668 - auc_8: 0.9677 - val_loss: 7.6751e-07 - val_accuracy: 0.8688 - val_auc_8: 0.9677\n",
      "Epoch 8/500\n",
      "539/539 - 1s - loss: 8.9399e-07 - accuracy: 0.8701 - auc_8: 0.9680 - val_loss: 7.6656e-07 - val_accuracy: 0.8689 - val_auc_8: 0.9674\n",
      "Epoch 9/500\n",
      "539/539 - 1s - loss: 8.9115e-07 - accuracy: 0.8700 - auc_8: 0.9680 - val_loss: 7.8585e-07 - val_accuracy: 0.8712 - val_auc_8: 0.9679\n",
      "Epoch 10/500\n",
      "539/539 - 1s - loss: 8.8121e-07 - accuracy: 0.8725 - auc_8: 0.9681 - val_loss: 7.8920e-07 - val_accuracy: 0.8726 - val_auc_8: 0.9677\n",
      "Epoch 11/500\n",
      "539/539 - 1s - loss: 8.7473e-07 - accuracy: 0.8747 - auc_8: 0.9677 - val_loss: 7.7993e-07 - val_accuracy: 0.8721 - val_auc_8: 0.9673\n",
      "Epoch 12/500\n",
      "539/539 - 1s - loss: 8.6590e-07 - accuracy: 0.8737 - auc_8: 0.9678 - val_loss: 8.0053e-07 - val_accuracy: 0.8740 - val_auc_8: 0.9678\n",
      "Epoch 13/500\n",
      "539/539 - 1s - loss: 8.6944e-07 - accuracy: 0.8741 - auc_8: 0.9677 - val_loss: 7.9456e-07 - val_accuracy: 0.8734 - val_auc_8: 0.9677\n",
      "Epoch 14/500\n",
      "539/539 - 1s - loss: 8.6389e-07 - accuracy: 0.8746 - auc_8: 0.9680 - val_loss: 7.8983e-07 - val_accuracy: 0.8730 - val_auc_8: 0.9675\n",
      "Epoch 15/500\n",
      "539/539 - 1s - loss: 8.6098e-07 - accuracy: 0.8749 - auc_8: 0.9679 - val_loss: 8.0152e-07 - val_accuracy: 0.8742 - val_auc_8: 0.9677\n",
      "Epoch 16/500\n",
      "539/539 - 1s - loss: 8.6891e-07 - accuracy: 0.8756 - auc_8: 0.9679 - val_loss: 7.9912e-07 - val_accuracy: 0.8741 - val_auc_8: 0.9676\n",
      "Epoch 17/500\n",
      "539/539 - 1s - loss: 8.6524e-07 - accuracy: 0.8752 - auc_8: 0.9680 - val_loss: 8.0474e-07 - val_accuracy: 0.8749 - val_auc_8: 0.9680\n",
      "Epoch 18/500\n",
      "539/539 - 1s - loss: 8.6549e-07 - accuracy: 0.8756 - auc_8: 0.9681 - val_loss: 8.0225e-07 - val_accuracy: 0.8745 - val_auc_8: 0.9678\n",
      "Epoch 19/500\n",
      "539/539 - 1s - loss: 8.7107e-07 - accuracy: 0.8754 - auc_8: 0.9680 - val_loss: 8.0501e-07 - val_accuracy: 0.8746 - val_auc_8: 0.9678\n",
      "Epoch 20/500\n",
      "539/539 - 1s - loss: 8.6611e-07 - accuracy: 0.8757 - auc_8: 0.9678 - val_loss: 7.9547e-07 - val_accuracy: 0.8735 - val_auc_8: 0.9674\n",
      "Epoch 21/500\n",
      "539/539 - 1s - loss: 8.6641e-07 - accuracy: 0.8757 - auc_8: 0.9681 - val_loss: 8.0561e-07 - val_accuracy: 0.8748 - val_auc_8: 0.9678\n",
      "Epoch 22/500\n",
      "539/539 - 1s - loss: 8.6720e-07 - accuracy: 0.8756 - auc_8: 0.9681 - val_loss: 7.9512e-07 - val_accuracy: 0.8738 - val_auc_8: 0.9676\n",
      "Epoch 23/500\n",
      "539/539 - 1s - loss: 8.6680e-07 - accuracy: 0.8755 - auc_8: 0.9679 - val_loss: 7.9193e-07 - val_accuracy: 0.8737 - val_auc_8: 0.9675\n",
      "Epoch 24/500\n",
      "539/539 - 1s - loss: 8.7066e-07 - accuracy: 0.8755 - auc_8: 0.9680 - val_loss: 8.0267e-07 - val_accuracy: 0.8745 - val_auc_8: 0.9677\n",
      "Epoch 25/500\n",
      "539/539 - 1s - loss: 8.6673e-07 - accuracy: 0.8757 - auc_8: 0.9679 - val_loss: 8.0593e-07 - val_accuracy: 0.8747 - val_auc_8: 0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:44:48,909] Trial 8 finished with value: 0.8747082191582213 and parameters: {'num_hidden_layers': 1, 'num_features_layer_0': 129, 'dropout': 0.19, 'batch_size': 512, 'batch_norm': True}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 99)                6930      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 99)                396       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 99)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 37)                3700      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 38        \n",
      "=================================================================\n",
      "Total params: 11,340\n",
      "Trainable params: 11,004\n",
      "Non-trainable params: 336\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0540s). Check your callbacks.\n",
      "539/539 - 14s - loss: 3.3446e-05 - accuracy: 0.9083 - auc_9: 0.9572 - val_loss: 2.1241e-06 - val_accuracy: 0.8884 - val_auc_9: 0.9751\n",
      "Epoch 2/500\n",
      "539/539 - 2s - loss: 1.5367e-06 - accuracy: 0.8528 - auc_9: 0.9683 - val_loss: 8.3748e-07 - val_accuracy: 0.8226 - val_auc_9: 0.9636\n",
      "Epoch 3/500\n",
      "539/539 - 2s - loss: 1.1789e-06 - accuracy: 0.8335 - auc_9: 0.9607 - val_loss: 7.4410e-07 - val_accuracy: 0.8354 - val_auc_9: 0.9590\n",
      "Epoch 4/500\n",
      "539/539 - 2s - loss: 1.1266e-06 - accuracy: 0.8422 - auc_9: 0.9572 - val_loss: 6.9058e-07 - val_accuracy: 0.8450 - val_auc_9: 0.9558\n",
      "Epoch 5/500\n",
      "539/539 - 2s - loss: 1.0092e-06 - accuracy: 0.8581 - auc_9: 0.9577 - val_loss: 7.5926e-07 - val_accuracy: 0.8581 - val_auc_9: 0.9583\n",
      "Epoch 6/500\n",
      "539/539 - 2s - loss: 9.8152e-07 - accuracy: 0.8650 - auc_9: 0.9587 - val_loss: 7.1443e-07 - val_accuracy: 0.8525 - val_auc_9: 0.9580\n",
      "Epoch 7/500\n",
      "539/539 - 2s - loss: 9.4526e-07 - accuracy: 0.8582 - auc_9: 0.9590 - val_loss: 7.4581e-07 - val_accuracy: 0.8623 - val_auc_9: 0.9588\n",
      "Epoch 8/500\n",
      "539/539 - 2s - loss: 9.1860e-07 - accuracy: 0.8660 - auc_9: 0.9601 - val_loss: 7.2815e-07 - val_accuracy: 0.8684 - val_auc_9: 0.9598\n",
      "Epoch 9/500\n",
      "539/539 - 2s - loss: 9.3133e-07 - accuracy: 0.8694 - auc_9: 0.9609 - val_loss: 8.6614e-07 - val_accuracy: 0.8707 - val_auc_9: 0.9607\n",
      "Epoch 10/500\n",
      "539/539 - 2s - loss: 9.2766e-07 - accuracy: 0.8723 - auc_9: 0.9609 - val_loss: 7.3041e-07 - val_accuracy: 0.8687 - val_auc_9: 0.9601\n",
      "Epoch 11/500\n",
      "539/539 - 2s - loss: 9.1619e-07 - accuracy: 0.8727 - auc_9: 0.9611 - val_loss: 8.2251e-07 - val_accuracy: 0.8721 - val_auc_9: 0.9606\n",
      "Epoch 12/500\n",
      "539/539 - 2s - loss: 8.8935e-07 - accuracy: 0.8732 - auc_9: 0.9609 - val_loss: 7.4867e-07 - val_accuracy: 0.8728 - val_auc_9: 0.9612\n",
      "Epoch 13/500\n",
      "539/539 - 2s - loss: 8.9286e-07 - accuracy: 0.8739 - auc_9: 0.9614 - val_loss: 8.4294e-07 - val_accuracy: 0.8732 - val_auc_9: 0.9607\n",
      "Epoch 14/500\n",
      "539/539 - 2s - loss: 8.9107e-07 - accuracy: 0.8743 - auc_9: 0.9615 - val_loss: 8.0144e-07 - val_accuracy: 0.8746 - val_auc_9: 0.9615\n",
      "Epoch 15/500\n",
      "539/539 - 2s - loss: 9.0099e-07 - accuracy: 0.8745 - auc_9: 0.9613 - val_loss: 8.4330e-07 - val_accuracy: 0.8738 - val_auc_9: 0.9607\n",
      "Epoch 16/500\n",
      "539/539 - 2s - loss: 8.7998e-07 - accuracy: 0.8752 - auc_9: 0.9611 - val_loss: 8.6620e-07 - val_accuracy: 0.8749 - val_auc_9: 0.9610\n",
      "Epoch 17/500\n",
      "539/539 - 2s - loss: 8.8874e-07 - accuracy: 0.8757 - auc_9: 0.9615 - val_loss: 7.9972e-07 - val_accuracy: 0.8750 - val_auc_9: 0.9607\n",
      "Epoch 18/500\n",
      "539/539 - 2s - loss: 8.9704e-07 - accuracy: 0.8757 - auc_9: 0.9614 - val_loss: 7.9782e-07 - val_accuracy: 0.8748 - val_auc_9: 0.9617\n",
      "Epoch 19/500\n",
      "539/539 - 2s - loss: 9.0347e-07 - accuracy: 0.8758 - auc_9: 0.9616 - val_loss: 8.0647e-07 - val_accuracy: 0.8748 - val_auc_9: 0.9609\n",
      "Epoch 20/500\n",
      "539/539 - 2s - loss: 9.0404e-07 - accuracy: 0.8756 - auc_9: 0.9615 - val_loss: 8.6654e-07 - val_accuracy: 0.8744 - val_auc_9: 0.9609\n",
      "Epoch 21/500\n",
      "539/539 - 2s - loss: 8.9279e-07 - accuracy: 0.8758 - auc_9: 0.9611 - val_loss: 8.2039e-07 - val_accuracy: 0.8753 - val_auc_9: 0.9611\n",
      "Epoch 22/500\n",
      "539/539 - 2s - loss: 9.0476e-07 - accuracy: 0.8760 - auc_9: 0.9613 - val_loss: 7.7086e-07 - val_accuracy: 0.8754 - val_auc_9: 0.9612\n",
      "Epoch 23/500\n",
      "539/539 - 2s - loss: 9.0633e-07 - accuracy: 0.8756 - auc_9: 0.9608 - val_loss: 7.6291e-07 - val_accuracy: 0.8751 - val_auc_9: 0.9613\n",
      "Epoch 24/500\n",
      "539/539 - 2s - loss: 8.7415e-07 - accuracy: 0.8757 - auc_9: 0.9614 - val_loss: 8.6491e-07 - val_accuracy: 0.8759 - val_auc_9: 0.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:45:59,170] Trial 9 finished with value: 0.8755563771330811 and parameters: {'num_hidden_layers': 2, 'num_features_layer_0': 99, 'num_features_layer_1': 37, 'dropout': 0.18, 'batch_size': 512, 'batch_norm': True}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 22)                1540      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 111)               2553      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 112       \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,343\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0477s). Check your callbacks.\n",
      "1078/1078 - 14s - loss: 1.1154e-05 - accuracy: 0.7378 - auc_10: 0.8993 - val_loss: 8.7940e-07 - val_accuracy: 0.7402 - val_auc_10: 0.9419\n",
      "Epoch 2/500\n",
      "1078/1078 - 2s - loss: 1.4960e-06 - accuracy: 0.7403 - auc_10: 0.9494 - val_loss: 7.3346e-07 - val_accuracy: 0.7402 - val_auc_10: 0.9571\n",
      "Epoch 3/500\n",
      "1078/1078 - 2s - loss: 1.2805e-06 - accuracy: 0.7403 - auc_10: 0.9599 - val_loss: 6.9007e-07 - val_accuracy: 0.7406 - val_auc_10: 0.9631\n",
      "Epoch 4/500\n",
      "1078/1078 - 2s - loss: 1.1531e-06 - accuracy: 0.7621 - auc_10: 0.9619 - val_loss: 6.8315e-07 - val_accuracy: 0.7983 - val_auc_10: 0.9628\n",
      "Epoch 5/500\n",
      "1078/1078 - 2s - loss: 1.0436e-06 - accuracy: 0.8267 - auc_10: 0.9632 - val_loss: 8.3939e-07 - val_accuracy: 0.8516 - val_auc_10: 0.9620\n",
      "Epoch 6/500\n",
      "1078/1078 - 2s - loss: 9.9384e-07 - accuracy: 0.8501 - auc_10: 0.9639 - val_loss: 8.4900e-07 - val_accuracy: 0.8666 - val_auc_10: 0.9658\n",
      "Epoch 7/500\n",
      "1078/1078 - 2s - loss: 9.0634e-07 - accuracy: 0.8690 - auc_10: 0.9657 - val_loss: 9.0355e-07 - val_accuracy: 0.8712 - val_auc_10: 0.9675\n",
      "Epoch 8/500\n",
      "1078/1078 - 2s - loss: 9.0377e-07 - accuracy: 0.8711 - auc_10: 0.9661 - val_loss: 8.6407e-07 - val_accuracy: 0.8682 - val_auc_10: 0.9656\n",
      "Epoch 9/500\n",
      "1078/1078 - 2s - loss: 9.0009e-07 - accuracy: 0.8704 - auc_10: 0.9651 - val_loss: 9.3196e-07 - val_accuracy: 0.8702 - val_auc_10: 0.9658\n",
      "Epoch 10/500\n",
      "1078/1078 - 2s - loss: 9.2343e-07 - accuracy: 0.8681 - auc_10: 0.9653 - val_loss: 9.0426e-07 - val_accuracy: 0.8700 - val_auc_10: 0.9669\n",
      "Epoch 11/500\n",
      "1078/1078 - 2s - loss: 9.0499e-07 - accuracy: 0.8703 - auc_10: 0.9664 - val_loss: 8.5756e-07 - val_accuracy: 0.8685 - val_auc_10: 0.9673\n",
      "Epoch 12/500\n",
      "1078/1078 - 2s - loss: 8.7977e-07 - accuracy: 0.8695 - auc_10: 0.9667 - val_loss: 8.7707e-07 - val_accuracy: 0.8682 - val_auc_10: 0.9666\n",
      "Epoch 13/500\n",
      "1078/1078 - 2s - loss: 9.0263e-07 - accuracy: 0.8697 - auc_10: 0.9663 - val_loss: 8.9400e-07 - val_accuracy: 0.8678 - val_auc_10: 0.9667\n",
      "Epoch 14/500\n",
      "1078/1078 - 2s - loss: 9.0508e-07 - accuracy: 0.8690 - auc_10: 0.9670 - val_loss: 8.6222e-07 - val_accuracy: 0.8684 - val_auc_10: 0.9670\n",
      "Epoch 15/500\n",
      "1078/1078 - 2s - loss: 8.9131e-07 - accuracy: 0.8700 - auc_10: 0.9664 - val_loss: 8.2195e-07 - val_accuracy: 0.8686 - val_auc_10: 0.9669\n",
      "Epoch 16/500\n",
      "1078/1078 - 2s - loss: 8.7428e-07 - accuracy: 0.8708 - auc_10: 0.9670 - val_loss: 8.6454e-07 - val_accuracy: 0.8695 - val_auc_10: 0.9673\n",
      "Epoch 17/500\n",
      "1078/1078 - 2s - loss: 9.0626e-07 - accuracy: 0.8709 - auc_10: 0.9670 - val_loss: 8.7382e-07 - val_accuracy: 0.8689 - val_auc_10: 0.9668\n",
      "Epoch 18/500\n",
      "1078/1078 - 2s - loss: 8.8776e-07 - accuracy: 0.8709 - auc_10: 0.9666 - val_loss: 8.0561e-07 - val_accuracy: 0.8685 - val_auc_10: 0.9666\n",
      "Epoch 19/500\n",
      "1078/1078 - 2s - loss: 8.8654e-07 - accuracy: 0.8710 - auc_10: 0.9670 - val_loss: 8.4285e-07 - val_accuracy: 0.8701 - val_auc_10: 0.9671\n",
      "Epoch 20/500\n",
      "1078/1078 - 2s - loss: 8.8194e-07 - accuracy: 0.8709 - auc_10: 0.9672 - val_loss: 8.0268e-07 - val_accuracy: 0.8700 - val_auc_10: 0.9674\n",
      "Epoch 21/500\n",
      "1078/1078 - 2s - loss: 8.8143e-07 - accuracy: 0.8708 - auc_10: 0.9670 - val_loss: 9.2855e-07 - val_accuracy: 0.8699 - val_auc_10: 0.9673\n",
      "Epoch 22/500\n",
      "1078/1078 - 2s - loss: 8.8475e-07 - accuracy: 0.8705 - auc_10: 0.9670 - val_loss: 9.4393e-07 - val_accuracy: 0.8699 - val_auc_10: 0.9674\n",
      "Epoch 23/500\n",
      "1078/1078 - 2s - loss: 8.8737e-07 - accuracy: 0.8706 - auc_10: 0.9669 - val_loss: 8.6486e-07 - val_accuracy: 0.8694 - val_auc_10: 0.9675\n",
      "Epoch 24/500\n",
      "1078/1078 - 2s - loss: 8.8787e-07 - accuracy: 0.8707 - auc_10: 0.9669 - val_loss: 8.2160e-07 - val_accuracy: 0.8706 - val_auc_10: 0.9679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:47:15,567] Trial 10 finished with value: 0.870601539732939 and parameters: {'num_hidden_layers': 2, 'num_features_layer_0': 22, 'num_features_layer_1': 111, 'dropout': 0.060000000000000005, 'batch_size': 256, 'batch_norm': False}. Best is trial 4 with value: 0.8857958911458107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 103)               7210      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 103)               412       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 149)               15496     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 150       \n",
      "=================================================================\n",
      "Total params: 23,544\n",
      "Trainable params: 23,200\n",
      "Non-trainable params: 344\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0530s). Check your callbacks.\n",
      "539/539 - 14s - loss: 7.4334e-06 - accuracy: 0.8327 - auc_11: 0.9492 - val_loss: 9.8268e-07 - val_accuracy: 0.8346 - val_auc_11: 0.9698\n",
      "Epoch 2/500\n",
      "539/539 - 2s - loss: 1.1346e-06 - accuracy: 0.8388 - auc_11: 0.9638 - val_loss: 6.0257e-07 - val_accuracy: 0.8178 - val_auc_11: 0.9581\n",
      "Epoch 3/500\n",
      "539/539 - 2s - loss: 9.9418e-07 - accuracy: 0.8603 - auc_11: 0.9611 - val_loss: 6.7587e-07 - val_accuracy: 0.8572 - val_auc_11: 0.9584\n",
      "Epoch 4/500\n",
      "539/539 - 2s - loss: 9.1953e-07 - accuracy: 0.8724 - auc_11: 0.9600 - val_loss: 7.5108e-07 - val_accuracy: 0.8747 - val_auc_11: 0.9618\n",
      "Epoch 5/500\n",
      "539/539 - 2s - loss: 8.5933e-07 - accuracy: 0.8845 - auc_11: 0.9625 - val_loss: 7.1915e-07 - val_accuracy: 0.8720 - val_auc_11: 0.9612\n",
      "Epoch 6/500\n",
      "539/539 - 2s - loss: 8.1379e-07 - accuracy: 0.8893 - auc_11: 0.9639 - val_loss: 8.5468e-07 - val_accuracy: 0.8828 - val_auc_11: 0.9641\n",
      "Epoch 7/500\n",
      "539/539 - 2s - loss: 7.8632e-07 - accuracy: 0.8899 - auc_11: 0.9647 - val_loss: 9.8750e-07 - val_accuracy: 0.8909 - val_auc_11: 0.9661\n",
      "Epoch 8/500\n",
      "539/539 - 2s - loss: 7.8407e-07 - accuracy: 0.8931 - auc_11: 0.9658 - val_loss: 8.8884e-07 - val_accuracy: 0.8926 - val_auc_11: 0.9665\n",
      "Epoch 9/500\n",
      "539/539 - 2s - loss: 7.7582e-07 - accuracy: 0.8950 - auc_11: 0.9660 - val_loss: 9.3735e-07 - val_accuracy: 0.8956 - val_auc_11: 0.9669\n",
      "Epoch 10/500\n",
      "539/539 - 2s - loss: 7.6689e-07 - accuracy: 0.8972 - auc_11: 0.9662 - val_loss: 9.6966e-07 - val_accuracy: 0.8957 - val_auc_11: 0.9664\n",
      "Epoch 11/500\n",
      "539/539 - 2s - loss: 7.7264e-07 - accuracy: 0.8970 - auc_11: 0.9668 - val_loss: 8.8779e-07 - val_accuracy: 0.8967 - val_auc_11: 0.9671\n",
      "Epoch 12/500\n",
      "539/539 - 2s - loss: 7.5091e-07 - accuracy: 0.8987 - auc_11: 0.9670 - val_loss: 9.7462e-07 - val_accuracy: 0.8974 - val_auc_11: 0.9672\n",
      "Epoch 13/500\n",
      "539/539 - 2s - loss: 7.4921e-07 - accuracy: 0.8996 - auc_11: 0.9672 - val_loss: 1.0062e-06 - val_accuracy: 0.8990 - val_auc_11: 0.9678\n",
      "Epoch 14/500\n",
      "539/539 - 2s - loss: 7.4750e-07 - accuracy: 0.8998 - auc_11: 0.9672 - val_loss: 9.2144e-07 - val_accuracy: 0.8977 - val_auc_11: 0.9674\n",
      "Epoch 15/500\n",
      "539/539 - 2s - loss: 7.5734e-07 - accuracy: 0.8999 - auc_11: 0.9670 - val_loss: 9.6043e-07 - val_accuracy: 0.8987 - val_auc_11: 0.9675\n",
      "Epoch 16/500\n",
      "539/539 - 2s - loss: 7.5463e-07 - accuracy: 0.9000 - auc_11: 0.9671 - val_loss: 8.9884e-07 - val_accuracy: 0.8983 - val_auc_11: 0.9673\n",
      "Epoch 17/500\n",
      "539/539 - 2s - loss: 7.5493e-07 - accuracy: 0.9000 - auc_11: 0.9671 - val_loss: 9.6324e-07 - val_accuracy: 0.8978 - val_auc_11: 0.9673\n",
      "Epoch 18/500\n",
      "539/539 - 2s - loss: 7.6045e-07 - accuracy: 0.9000 - auc_11: 0.9673 - val_loss: 9.5757e-07 - val_accuracy: 0.8988 - val_auc_11: 0.9677\n",
      "Epoch 19/500\n",
      "539/539 - 2s - loss: 7.5206e-07 - accuracy: 0.8996 - auc_11: 0.9674 - val_loss: 9.7712e-07 - val_accuracy: 0.8990 - val_auc_11: 0.9677\n",
      "Epoch 20/500\n",
      "539/539 - 2s - loss: 7.4850e-07 - accuracy: 0.9000 - auc_11: 0.9669 - val_loss: 9.7107e-07 - val_accuracy: 0.8984 - val_auc_11: 0.9674\n",
      "Epoch 21/500\n",
      "539/539 - 2s - loss: 7.5569e-07 - accuracy: 0.9001 - auc_11: 0.9672 - val_loss: 9.3108e-07 - val_accuracy: 0.8979 - val_auc_11: 0.9671\n",
      "Epoch 22/500\n",
      "539/539 - 2s - loss: 7.5255e-07 - accuracy: 0.9000 - auc_11: 0.9669 - val_loss: 9.7837e-07 - val_accuracy: 0.8983 - val_auc_11: 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:48:27,844] Trial 11 finished with value: 0.898525509982167 and parameters: {'num_hidden_layers': 2, 'num_features_layer_0': 103, 'num_features_layer_1': 149, 'dropout': 0.07, 'batch_size': 512, 'batch_norm': True}. Best is trial 11 with value: 0.898525509982167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Model: \"functional_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 112)               7840      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 148)               16724     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 148)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 150)               22350     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 47,341\n",
      "Trainable params: 47,203\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0497s). Check your callbacks.\n",
      "539/539 - 14s - loss: 5.3706e-06 - accuracy: 0.7438 - auc_12: 0.8446 - val_loss: 7.3314e-07 - val_accuracy: 0.7536 - val_auc_12: 0.9445\n",
      "Epoch 2/500\n",
      "539/539 - 3s - loss: 1.2545e-06 - accuracy: 0.8034 - auc_12: 0.9530 - val_loss: 7.3106e-07 - val_accuracy: 0.8524 - val_auc_12: 0.9628\n",
      "Epoch 3/500\n",
      "539/539 - 3s - loss: 1.0719e-06 - accuracy: 0.8488 - auc_12: 0.9568 - val_loss: 5.8513e-07 - val_accuracy: 0.8406 - val_auc_12: 0.9553\n",
      "Epoch 4/500\n",
      "539/539 - 3s - loss: 9.3891e-07 - accuracy: 0.8701 - auc_12: 0.9598 - val_loss: 6.7072e-07 - val_accuracy: 0.8669 - val_auc_12: 0.9594\n",
      "Epoch 5/500\n",
      "539/539 - 3s - loss: 9.0774e-07 - accuracy: 0.8755 - auc_12: 0.9624 - val_loss: 8.4370e-07 - val_accuracy: 0.8662 - val_auc_12: 0.9635\n",
      "Epoch 6/500\n",
      "539/539 - 3s - loss: 8.4330e-07 - accuracy: 0.8863 - auc_12: 0.9653 - val_loss: 7.3874e-07 - val_accuracy: 0.8855 - val_auc_12: 0.9638\n",
      "Epoch 7/500\n",
      "539/539 - 3s - loss: 7.8599e-07 - accuracy: 0.8932 - auc_12: 0.9664 - val_loss: 9.0773e-07 - val_accuracy: 0.8967 - val_auc_12: 0.9669\n",
      "Epoch 8/500\n",
      "539/539 - 3s - loss: 7.7892e-07 - accuracy: 0.8969 - auc_12: 0.9669 - val_loss: 9.2415e-07 - val_accuracy: 0.8964 - val_auc_12: 0.9666\n",
      "Epoch 9/500\n",
      "539/539 - 3s - loss: 7.7994e-07 - accuracy: 0.8970 - auc_12: 0.9670 - val_loss: 8.7364e-07 - val_accuracy: 0.9000 - val_auc_12: 0.9673\n",
      "Epoch 10/500\n",
      "539/539 - 3s - loss: 7.8813e-07 - accuracy: 0.8979 - auc_12: 0.9666 - val_loss: 8.4366e-07 - val_accuracy: 0.8949 - val_auc_12: 0.9664\n",
      "Epoch 11/500\n",
      "539/539 - 3s - loss: 7.5844e-07 - accuracy: 0.8995 - auc_12: 0.9676 - val_loss: 9.7834e-07 - val_accuracy: 0.9019 - val_auc_12: 0.9684\n",
      "Epoch 12/500\n",
      "539/539 - 3s - loss: 7.6978e-07 - accuracy: 0.9028 - auc_12: 0.9689 - val_loss: 1.0057e-06 - val_accuracy: 0.9011 - val_auc_12: 0.9678\n",
      "Epoch 13/500\n",
      "539/539 - 3s - loss: 7.6657e-07 - accuracy: 0.9022 - auc_12: 0.9683 - val_loss: 9.3086e-07 - val_accuracy: 0.9009 - val_auc_12: 0.9680\n",
      "Epoch 14/500\n",
      "539/539 - 3s - loss: 7.5953e-07 - accuracy: 0.9013 - auc_12: 0.9680 - val_loss: 9.6900e-07 - val_accuracy: 0.9005 - val_auc_12: 0.9677\n",
      "Epoch 15/500\n",
      "539/539 - 3s - loss: 7.6938e-07 - accuracy: 0.9012 - auc_12: 0.9678 - val_loss: 9.1001e-07 - val_accuracy: 0.8999 - val_auc_12: 0.9675\n",
      "Epoch 16/500\n",
      "539/539 - 3s - loss: 7.6106e-07 - accuracy: 0.9011 - auc_12: 0.9674 - val_loss: 9.0966e-07 - val_accuracy: 0.9000 - val_auc_12: 0.9675\n",
      "Epoch 17/500\n",
      "539/539 - 3s - loss: 7.6059e-07 - accuracy: 0.9012 - auc_12: 0.9677 - val_loss: 9.4039e-07 - val_accuracy: 0.9005 - val_auc_12: 0.9676\n",
      "Epoch 18/500\n",
      "539/539 - 3s - loss: 7.5278e-07 - accuracy: 0.9009 - auc_12: 0.9680 - val_loss: 9.4192e-07 - val_accuracy: 0.9000 - val_auc_12: 0.9674\n",
      "Epoch 19/500\n",
      "539/539 - 3s - loss: 7.5625e-07 - accuracy: 0.9013 - auc_12: 0.9679 - val_loss: 9.4557e-07 - val_accuracy: 0.9000 - val_auc_12: 0.9678\n",
      "Epoch 20/500\n",
      "539/539 - 3s - loss: 7.7006e-07 - accuracy: 0.9012 - auc_12: 0.9677 - val_loss: 9.9976e-07 - val_accuracy: 0.8998 - val_auc_12: 0.9674\n",
      "Epoch 21/500\n",
      "539/539 - 3s - loss: 7.5682e-07 - accuracy: 0.9009 - auc_12: 0.9679 - val_loss: 9.6755e-07 - val_accuracy: 0.8991 - val_auc_12: 0.9670\n",
      "Epoch 22/500\n",
      "539/539 - 3s - loss: 7.6528e-07 - accuracy: 0.9013 - auc_12: 0.9681 - val_loss: 9.8606e-07 - val_accuracy: 0.9013 - val_auc_12: 0.9679\n",
      "Epoch 23/500\n",
      "539/539 - 3s - loss: 7.7004e-07 - accuracy: 0.9009 - auc_12: 0.9676 - val_loss: 9.6412e-07 - val_accuracy: 0.8989 - val_auc_12: 0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:49:53,476] Trial 12 finished with value: 0.8988009800936598 and parameters: {'num_hidden_layers': 3, 'num_features_layer_0': 112, 'num_features_layer_1': 148, 'num_features_layer_2': 150, 'dropout': 0.05, 'batch_size': 512, 'batch_norm': False}. Best is trial 12 with value: 0.8988009800936598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "Model: \"functional_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 115)               8050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 115)               460       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 115)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 150)               17400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 144)               21744     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 145       \n",
      "=================================================================\n",
      "Total params: 48,675\n",
      "Trainable params: 48,007\n",
      "Non-trainable params: 668\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0644s). Check your callbacks.\n",
      "1078/1078 - 16s - loss: 5.0202e-06 - accuracy: 0.8320 - auc_13: 0.9417 - val_loss: 8.9575e-07 - val_accuracy: 0.8348 - val_auc_13: 0.9507\n",
      "Epoch 2/500\n",
      "1078/1078 - 4s - loss: 1.1366e-06 - accuracy: 0.8475 - auc_13: 0.9495 - val_loss: 8.1624e-07 - val_accuracy: 0.7666 - val_auc_13: 0.8659\n",
      "Epoch 3/500\n",
      "1078/1078 - 4s - loss: 1.0336e-06 - accuracy: 0.8575 - auc_13: 0.9469 - val_loss: 7.5128e-07 - val_accuracy: 0.8683 - val_auc_13: 0.9553\n",
      "Epoch 4/500\n",
      "1078/1078 - 4s - loss: 8.9806e-07 - accuracy: 0.8804 - auc_13: 0.9592 - val_loss: 6.9560e-07 - val_accuracy: 0.8408 - val_auc_13: 0.9484\n",
      "Epoch 5/500\n",
      "1078/1078 - 4s - loss: 8.8114e-07 - accuracy: 0.8816 - auc_13: 0.9581 - val_loss: 1.0073e-06 - val_accuracy: 0.8844 - val_auc_13: 0.9654\n",
      "Epoch 6/500\n",
      "1078/1078 - 4s - loss: 8.1607e-07 - accuracy: 0.8897 - auc_13: 0.9647 - val_loss: 6.9295e-07 - val_accuracy: 0.8844 - val_auc_13: 0.9646\n",
      "Epoch 7/500\n",
      "1078/1078 - 4s - loss: 7.2033e-07 - accuracy: 0.8984 - auc_13: 0.9672 - val_loss: 8.5518e-07 - val_accuracy: 0.9032 - val_auc_13: 0.9687\n",
      "Epoch 8/500\n",
      "1078/1078 - 4s - loss: 7.0165e-07 - accuracy: 0.9060 - auc_13: 0.9684 - val_loss: 8.8724e-07 - val_accuracy: 0.9041 - val_auc_13: 0.9687\n",
      "Epoch 9/500\n",
      "1078/1078 - 4s - loss: 6.8521e-07 - accuracy: 0.9085 - auc_13: 0.9691 - val_loss: 1.0605e-06 - val_accuracy: 0.9085 - val_auc_13: 0.9689\n",
      "Epoch 10/500\n",
      "1078/1078 - 4s - loss: 6.7422e-07 - accuracy: 0.9130 - auc_13: 0.9694 - val_loss: 9.5031e-07 - val_accuracy: 0.9077 - val_auc_13: 0.9685\n",
      "Epoch 11/500\n",
      "1078/1078 - 4s - loss: 6.6923e-07 - accuracy: 0.9132 - auc_13: 0.9696 - val_loss: 1.0637e-06 - val_accuracy: 0.9115 - val_auc_13: 0.9691\n",
      "Epoch 12/500\n",
      "1078/1078 - 4s - loss: 6.5655e-07 - accuracy: 0.9137 - auc_13: 0.9697 - val_loss: 1.0394e-06 - val_accuracy: 0.9124 - val_auc_13: 0.9698\n",
      "Epoch 13/500\n",
      "1078/1078 - 4s - loss: 6.6321e-07 - accuracy: 0.9142 - auc_13: 0.9696 - val_loss: 1.0711e-06 - val_accuracy: 0.9112 - val_auc_13: 0.9688\n",
      "Epoch 14/500\n",
      "1078/1078 - 4s - loss: 6.7748e-07 - accuracy: 0.9134 - auc_13: 0.9696 - val_loss: 1.1118e-06 - val_accuracy: 0.9124 - val_auc_13: 0.9700\n",
      "Epoch 15/500\n",
      "1078/1078 - 4s - loss: 6.6784e-07 - accuracy: 0.9134 - auc_13: 0.9694 - val_loss: 1.0762e-06 - val_accuracy: 0.9112 - val_auc_13: 0.9694\n",
      "Epoch 16/500\n",
      "1078/1078 - 4s - loss: 6.6962e-07 - accuracy: 0.9135 - auc_13: 0.9697 - val_loss: 1.0912e-06 - val_accuracy: 0.9119 - val_auc_13: 0.9694\n",
      "Epoch 17/500\n",
      "1078/1078 - 4s - loss: 6.6429e-07 - accuracy: 0.9135 - auc_13: 0.9697 - val_loss: 1.1237e-06 - val_accuracy: 0.9122 - val_auc_13: 0.9700\n",
      "Epoch 18/500\n",
      "1078/1078 - 4s - loss: 6.5058e-07 - accuracy: 0.9140 - auc_13: 0.9696 - val_loss: 1.0082e-06 - val_accuracy: 0.9119 - val_auc_13: 0.9699\n",
      "Epoch 19/500\n",
      "1078/1078 - 4s - loss: 6.6541e-07 - accuracy: 0.9140 - auc_13: 0.9694 - val_loss: 1.0200e-06 - val_accuracy: 0.9118 - val_auc_13: 0.9696\n",
      "Epoch 20/500\n",
      "1078/1078 - 4s - loss: 6.5825e-07 - accuracy: 0.9137 - auc_13: 0.9695 - val_loss: 1.0576e-06 - val_accuracy: 0.9119 - val_auc_13: 0.9694\n",
      "Epoch 21/500\n",
      "1078/1078 - 4s - loss: 6.6241e-07 - accuracy: 0.9138 - auc_13: 0.9696 - val_loss: 1.0640e-06 - val_accuracy: 0.9128 - val_auc_13: 0.9698\n",
      "Epoch 22/500\n",
      "1078/1078 - 4s - loss: 6.6214e-07 - accuracy: 0.9140 - auc_13: 0.9699 - val_loss: 1.0365e-06 - val_accuracy: 0.9116 - val_auc_13: 0.9692\n",
      "Epoch 23/500\n",
      "1078/1078 - 4s - loss: 6.5755e-07 - accuracy: 0.9140 - auc_13: 0.9699 - val_loss: 1.0894e-06 - val_accuracy: 0.9134 - val_auc_13: 0.9698\n",
      "Epoch 24/500\n",
      "1078/1078 - 4s - loss: 6.6262e-07 - accuracy: 0.9140 - auc_13: 0.9699 - val_loss: 1.0327e-06 - val_accuracy: 0.9128 - val_auc_13: 0.9701\n",
      "Epoch 25/500\n",
      "1078/1078 - 4s - loss: 6.5832e-07 - accuracy: 0.9139 - auc_13: 0.9702 - val_loss: 1.0659e-06 - val_accuracy: 0.9120 - val_auc_13: 0.9695\n",
      "Epoch 26/500\n",
      "1078/1078 - 4s - loss: 6.5713e-07 - accuracy: 0.9139 - auc_13: 0.9697 - val_loss: 1.0286e-06 - val_accuracy: 0.9117 - val_auc_13: 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:52:13,474] Trial 13 finished with value: 0.9121649051077958 and parameters: {'num_hidden_layers': 3, 'num_features_layer_0': 115, 'num_features_layer_1': 150, 'num_features_layer_2': 144, 'dropout': 0.060000000000000005, 'batch_size': 256, 'batch_norm': True}. Best is trial 13 with value: 0.9121649051077958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Model: \"functional_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 145)               10150     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 145)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 97)                14162     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 97)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 149)               14602     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 150       \n",
      "=================================================================\n",
      "Total params: 39,340\n",
      "Trainable params: 39,202\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0544s). Check your callbacks.\n",
      "1078/1078 - 15s - loss: 3.4296e-06 - accuracy: 0.7656 - auc_14: 0.9096 - val_loss: 6.5570e-07 - val_accuracy: 0.8140 - val_auc_14: 0.9577\n",
      "Epoch 2/500\n",
      "1078/1078 - 3s - loss: 1.0631e-06 - accuracy: 0.8451 - auc_14: 0.9576 - val_loss: 6.4841e-07 - val_accuracy: 0.8740 - val_auc_14: 0.9601\n",
      "Epoch 3/500\n",
      "1078/1078 - 3s - loss: 9.3212e-07 - accuracy: 0.8695 - auc_14: 0.9625 - val_loss: 7.3982e-07 - val_accuracy: 0.8694 - val_auc_14: 0.9629\n",
      "Epoch 4/500\n",
      "1078/1078 - 3s - loss: 8.5522e-07 - accuracy: 0.8864 - auc_14: 0.9637 - val_loss: 6.6440e-07 - val_accuracy: 0.8609 - val_auc_14: 0.9541\n",
      "Epoch 5/500\n",
      "1078/1078 - 3s - loss: 8.0034e-07 - accuracy: 0.8941 - auc_14: 0.9665 - val_loss: 1.1779e-06 - val_accuracy: 0.9094 - val_auc_14: 0.9714\n",
      "Epoch 6/500\n",
      "1078/1078 - 3s - loss: 7.7362e-07 - accuracy: 0.8965 - auc_14: 0.9679 - val_loss: 1.2712e-06 - val_accuracy: 0.9046 - val_auc_14: 0.9724\n",
      "Epoch 7/500\n",
      "1078/1078 - 3s - loss: 7.2595e-07 - accuracy: 0.9040 - auc_14: 0.9715 - val_loss: 1.1307e-06 - val_accuracy: 0.9089 - val_auc_14: 0.9718\n",
      "Epoch 8/500\n",
      "1078/1078 - 3s - loss: 7.1873e-07 - accuracy: 0.9068 - auc_14: 0.9711 - val_loss: 9.8020e-07 - val_accuracy: 0.9060 - val_auc_14: 0.9704\n",
      "Epoch 9/500\n",
      "1078/1078 - 3s - loss: 7.0536e-07 - accuracy: 0.9084 - auc_14: 0.9709 - val_loss: 1.0472e-06 - val_accuracy: 0.9051 - val_auc_14: 0.9692\n",
      "Epoch 10/500\n",
      "1078/1078 - 3s - loss: 6.9249e-07 - accuracy: 0.9095 - auc_14: 0.9710 - val_loss: 1.0675e-06 - val_accuracy: 0.9143 - val_auc_14: 0.9721\n",
      "Epoch 11/500\n",
      "1078/1078 - 3s - loss: 6.9375e-07 - accuracy: 0.9118 - auc_14: 0.9710 - val_loss: 1.0196e-06 - val_accuracy: 0.9107 - val_auc_14: 0.9712\n",
      "Epoch 12/500\n",
      "1078/1078 - 3s - loss: 6.7799e-07 - accuracy: 0.9119 - auc_14: 0.9712 - val_loss: 1.1498e-06 - val_accuracy: 0.9122 - val_auc_14: 0.9715\n",
      "Epoch 13/500\n",
      "1078/1078 - 3s - loss: 6.6822e-07 - accuracy: 0.9129 - auc_14: 0.9720 - val_loss: 1.0844e-06 - val_accuracy: 0.9124 - val_auc_14: 0.9716\n",
      "Epoch 14/500\n",
      "1078/1078 - 3s - loss: 6.9026e-07 - accuracy: 0.9131 - auc_14: 0.9714 - val_loss: 1.0992e-06 - val_accuracy: 0.9116 - val_auc_14: 0.9710\n",
      "Epoch 15/500\n",
      "1078/1078 - 3s - loss: 6.6663e-07 - accuracy: 0.9134 - auc_14: 0.9717 - val_loss: 1.1218e-06 - val_accuracy: 0.9133 - val_auc_14: 0.9717\n",
      "Epoch 16/500\n",
      "1078/1078 - 3s - loss: 6.7143e-07 - accuracy: 0.9149 - auc_14: 0.9717 - val_loss: 1.2140e-06 - val_accuracy: 0.9143 - val_auc_14: 0.9719\n",
      "Epoch 17/500\n",
      "1078/1078 - 3s - loss: 6.7019e-07 - accuracy: 0.9147 - auc_14: 0.9717 - val_loss: 1.1725e-06 - val_accuracy: 0.9138 - val_auc_14: 0.9715\n",
      "Epoch 18/500\n",
      "1078/1078 - 3s - loss: 6.7179e-07 - accuracy: 0.9144 - auc_14: 0.9715 - val_loss: 1.1550e-06 - val_accuracy: 0.9127 - val_auc_14: 0.9712\n",
      "Epoch 19/500\n",
      "1078/1078 - 3s - loss: 6.7387e-07 - accuracy: 0.9149 - auc_14: 0.9717 - val_loss: 1.1618e-06 - val_accuracy: 0.9145 - val_auc_14: 0.9718\n",
      "Epoch 20/500\n",
      "1078/1078 - 3s - loss: 6.6830e-07 - accuracy: 0.9148 - auc_14: 0.9717 - val_loss: 1.0675e-06 - val_accuracy: 0.9135 - val_auc_14: 0.9713\n",
      "Epoch 21/500\n",
      "1078/1078 - 3s - loss: 6.7529e-07 - accuracy: 0.9151 - auc_14: 0.9717 - val_loss: 1.1260e-06 - val_accuracy: 0.9126 - val_auc_14: 0.9709\n",
      "Epoch 22/500\n",
      "1078/1078 - 3s - loss: 6.6633e-07 - accuracy: 0.9149 - auc_14: 0.9714 - val_loss: 1.1923e-06 - val_accuracy: 0.9146 - val_auc_14: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:53:54,323] Trial 14 finished with value: 0.914027952967103 and parameters: {'num_hidden_layers': 3, 'num_features_layer_0': 145, 'num_features_layer_1': 97, 'num_features_layer_2': 149, 'dropout': 0.05, 'batch_size': 256, 'batch_norm': False}. Best is trial 14 with value: 0.914027952967103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Model: \"functional_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 145)               10150     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 145)               580       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 145)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 96)                14016     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 96)                384       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 148)               14356     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 148)               592       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 148)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 148)               22052     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 149       \n",
      "=================================================================\n",
      "Total params: 62,555\n",
      "Trainable params: 61,639\n",
      "Non-trainable params: 916\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0045s vs `on_train_batch_end` time: 0.0813s). Check your callbacks.\n",
      "1078/1078 - 17s - loss: 7.4864e-06 - accuracy: 0.8331 - auc_15: 0.9348 - val_loss: 9.0440e-07 - val_accuracy: 0.8445 - val_auc_15: 0.9482\n",
      "Epoch 2/500\n",
      "1078/1078 - 5s - loss: 1.2056e-06 - accuracy: 0.8401 - auc_15: 0.9465 - val_loss: 6.8622e-07 - val_accuracy: 0.8195 - val_auc_15: 0.9441\n",
      "Epoch 3/500\n",
      "1078/1078 - 5s - loss: 1.0967e-06 - accuracy: 0.8547 - auc_15: 0.9473 - val_loss: 6.9737e-07 - val_accuracy: 0.8385 - val_auc_15: 0.9426\n",
      "Epoch 4/500\n",
      "1078/1078 - 5s - loss: 9.8709e-07 - accuracy: 0.8672 - auc_15: 0.9534 - val_loss: 8.9472e-07 - val_accuracy: 0.8558 - val_auc_15: 0.9495\n",
      "Epoch 5/500\n",
      "1078/1078 - 5s - loss: 9.6263e-07 - accuracy: 0.8714 - auc_15: 0.9533 - val_loss: 1.0755e-06 - val_accuracy: 0.9008 - val_auc_15: 0.9594\n",
      "Epoch 6/500\n",
      "1078/1078 - 5s - loss: 9.5385e-07 - accuracy: 0.8746 - auc_15: 0.9538 - val_loss: 9.6902e-07 - val_accuracy: 0.8990 - val_auc_15: 0.9647\n",
      "Epoch 7/500\n",
      "1078/1078 - 5s - loss: 8.1824e-07 - accuracy: 0.8945 - auc_15: 0.9637 - val_loss: 1.0041e-06 - val_accuracy: 0.8934 - val_auc_15: 0.9639\n",
      "Epoch 8/500\n",
      "1078/1078 - 5s - loss: 8.1344e-07 - accuracy: 0.8956 - auc_15: 0.9632 - val_loss: 8.4531e-07 - val_accuracy: 0.8888 - val_auc_15: 0.9623\n",
      "Epoch 9/500\n",
      "1078/1078 - 5s - loss: 8.0539e-07 - accuracy: 0.8933 - auc_15: 0.9638 - val_loss: 8.5940e-07 - val_accuracy: 0.8923 - val_auc_15: 0.9647\n",
      "Epoch 10/500\n",
      "1078/1078 - 5s - loss: 7.8133e-07 - accuracy: 0.8994 - auc_15: 0.9643 - val_loss: 9.8249e-07 - val_accuracy: 0.8960 - val_auc_15: 0.9639\n",
      "Epoch 11/500\n",
      "1078/1078 - 5s - loss: 7.7163e-07 - accuracy: 0.8985 - auc_15: 0.9644 - val_loss: 1.1920e-06 - val_accuracy: 0.9005 - val_auc_15: 0.9648\n",
      "Epoch 12/500\n",
      "1078/1078 - 5s - loss: 7.5835e-07 - accuracy: 0.9008 - auc_15: 0.9650 - val_loss: 1.0619e-06 - val_accuracy: 0.8981 - val_auc_15: 0.9644\n",
      "Epoch 13/500\n",
      "1078/1078 - 5s - loss: 7.6344e-07 - accuracy: 0.9020 - auc_15: 0.9651 - val_loss: 1.0342e-06 - val_accuracy: 0.9001 - val_auc_15: 0.9652\n",
      "Epoch 14/500\n",
      "1078/1078 - 5s - loss: 7.7216e-07 - accuracy: 0.9014 - auc_15: 0.9654 - val_loss: 1.0967e-06 - val_accuracy: 0.9000 - val_auc_15: 0.9652\n",
      "Epoch 15/500\n",
      "1078/1078 - 6s - loss: 7.6008e-07 - accuracy: 0.9017 - auc_15: 0.9652 - val_loss: 1.0340e-06 - val_accuracy: 0.8990 - val_auc_15: 0.9644\n",
      "Epoch 16/500\n",
      "1078/1078 - 6s - loss: 7.5087e-07 - accuracy: 0.9025 - auc_15: 0.9651 - val_loss: 1.1021e-06 - val_accuracy: 0.9002 - val_auc_15: 0.9643\n",
      "Epoch 17/500\n",
      "1078/1078 - 6s - loss: 7.4903e-07 - accuracy: 0.9024 - auc_15: 0.9653 - val_loss: 1.0253e-06 - val_accuracy: 0.8994 - val_auc_15: 0.9647\n",
      "Epoch 18/500\n",
      "1078/1078 - 5s - loss: 7.4608e-07 - accuracy: 0.9028 - auc_15: 0.9659 - val_loss: 1.0702e-06 - val_accuracy: 0.9011 - val_auc_15: 0.9655\n",
      "Epoch 19/500\n",
      "1078/1078 - 5s - loss: 7.4905e-07 - accuracy: 0.9027 - auc_15: 0.9655 - val_loss: 1.0210e-06 - val_accuracy: 0.9001 - val_auc_15: 0.9647\n",
      "Epoch 20/500\n",
      "1078/1078 - 6s - loss: 7.4504e-07 - accuracy: 0.9027 - auc_15: 0.9658 - val_loss: 9.0547e-07 - val_accuracy: 0.9005 - val_auc_15: 0.9651\n",
      "Epoch 21/500\n",
      "1078/1078 - 5s - loss: 7.4434e-07 - accuracy: 0.9026 - auc_15: 0.9655 - val_loss: 1.0653e-06 - val_accuracy: 0.9001 - val_auc_15: 0.9644\n",
      "Epoch 22/500\n",
      "1078/1078 - 5s - loss: 7.5102e-07 - accuracy: 0.9028 - auc_15: 0.9653 - val_loss: 9.6471e-07 - val_accuracy: 0.9001 - val_auc_15: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:56:22,788] Trial 15 finished with value: 0.9002073275049657 and parameters: {'num_hidden_layers': 4, 'num_features_layer_0': 145, 'num_features_layer_1': 96, 'num_features_layer_2': 148, 'num_features_layer_3': 148, 'dropout': 0.12000000000000001, 'batch_size': 256, 'batch_norm': True}. Best is trial 14 with value: 0.914027952967103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Model: \"functional_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 145)               10150     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 145)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 79)                11534     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 79)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 118)               9440      \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 119       \n",
      "=================================================================\n",
      "Total params: 31,519\n",
      "Trainable params: 31,381\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0610s). Check your callbacks.\n",
      "1078/1078 - 15s - loss: 3.7453e-06 - accuracy: 0.7410 - auc_16: 0.8804 - val_loss: 7.5270e-07 - val_accuracy: 0.7521 - val_auc_16: 0.9543\n",
      "Epoch 2/500\n",
      "1078/1078 - 3s - loss: 1.1842e-06 - accuracy: 0.8033 - auc_16: 0.9539 - val_loss: 7.2571e-07 - val_accuracy: 0.8441 - val_auc_16: 0.9575\n",
      "Epoch 3/500\n",
      "1078/1078 - 3s - loss: 1.1310e-06 - accuracy: 0.8210 - auc_16: 0.9558 - val_loss: 7.6403e-07 - val_accuracy: 0.8637 - val_auc_16: 0.9618\n",
      "Epoch 4/500\n",
      "1078/1078 - 3s - loss: 1.0093e-06 - accuracy: 0.8550 - auc_16: 0.9590 - val_loss: 6.5086e-07 - val_accuracy: 0.8408 - val_auc_16: 0.9593\n",
      "Epoch 5/500\n",
      "1078/1078 - 3s - loss: 9.2721e-07 - accuracy: 0.8696 - auc_16: 0.9616 - val_loss: 8.5843e-07 - val_accuracy: 0.8845 - val_auc_16: 0.9636\n",
      "Epoch 6/500\n",
      "1078/1078 - 3s - loss: 8.9457e-07 - accuracy: 0.8814 - auc_16: 0.9631 - val_loss: 6.4261e-07 - val_accuracy: 0.8625 - val_auc_16: 0.9627\n",
      "Epoch 7/500\n",
      "1078/1078 - 3s - loss: 8.5317e-07 - accuracy: 0.8750 - auc_16: 0.9668 - val_loss: 8.0472e-07 - val_accuracy: 0.8806 - val_auc_16: 0.9666\n",
      "Epoch 8/500\n",
      "1078/1078 - 3s - loss: 8.2718e-07 - accuracy: 0.8833 - auc_16: 0.9669 - val_loss: 8.8663e-07 - val_accuracy: 0.8867 - val_auc_16: 0.9670\n",
      "Epoch 9/500\n",
      "1078/1078 - 3s - loss: 8.3656e-07 - accuracy: 0.8862 - auc_16: 0.9664 - val_loss: 9.2956e-07 - val_accuracy: 0.8847 - val_auc_16: 0.9660\n",
      "Epoch 10/500\n",
      "1078/1078 - 3s - loss: 8.1149e-07 - accuracy: 0.8862 - auc_16: 0.9669 - val_loss: 8.6924e-07 - val_accuracy: 0.8891 - val_auc_16: 0.9666\n",
      "Epoch 11/500\n",
      "1078/1078 - 3s - loss: 8.3594e-07 - accuracy: 0.8887 - auc_16: 0.9664 - val_loss: 8.7409e-07 - val_accuracy: 0.8868 - val_auc_16: 0.9656\n",
      "Epoch 12/500\n",
      "1078/1078 - 3s - loss: 8.1530e-07 - accuracy: 0.8881 - auc_16: 0.9658 - val_loss: 8.7823e-07 - val_accuracy: 0.8870 - val_auc_16: 0.9661\n",
      "Epoch 13/500\n",
      "1078/1078 - 3s - loss: 8.1250e-07 - accuracy: 0.8889 - auc_16: 0.9660 - val_loss: 7.8802e-07 - val_accuracy: 0.8876 - val_auc_16: 0.9658\n",
      "Epoch 14/500\n",
      "1078/1078 - 3s - loss: 8.3247e-07 - accuracy: 0.8892 - auc_16: 0.9663 - val_loss: 8.9729e-07 - val_accuracy: 0.8865 - val_auc_16: 0.9660\n",
      "Epoch 15/500\n",
      "1078/1078 - 3s - loss: 8.2343e-07 - accuracy: 0.8886 - auc_16: 0.9664 - val_loss: 8.2176e-07 - val_accuracy: 0.8872 - val_auc_16: 0.9667\n",
      "Epoch 16/500\n",
      "1078/1078 - 3s - loss: 8.0988e-07 - accuracy: 0.8891 - auc_16: 0.9667 - val_loss: 8.0275e-07 - val_accuracy: 0.8888 - val_auc_16: 0.9672\n",
      "Epoch 17/500\n",
      "1078/1078 - 3s - loss: 8.0561e-07 - accuracy: 0.8893 - auc_16: 0.9665 - val_loss: 8.7086e-07 - val_accuracy: 0.8875 - val_auc_16: 0.9661\n",
      "Epoch 18/500\n",
      "1078/1078 - 3s - loss: 8.2099e-07 - accuracy: 0.8890 - auc_16: 0.9666 - val_loss: 8.3889e-07 - val_accuracy: 0.8871 - val_auc_16: 0.9664\n",
      "Epoch 19/500\n",
      "1078/1078 - 3s - loss: 8.1258e-07 - accuracy: 0.8893 - auc_16: 0.9670 - val_loss: 8.8453e-07 - val_accuracy: 0.8883 - val_auc_16: 0.9669\n",
      "Epoch 20/500\n",
      "1078/1078 - 3s - loss: 8.1275e-07 - accuracy: 0.8893 - auc_16: 0.9667 - val_loss: 9.5251e-07 - val_accuracy: 0.8871 - val_auc_16: 0.9664\n",
      "Epoch 21/500\n",
      "1078/1078 - 3s - loss: 8.1193e-07 - accuracy: 0.8885 - auc_16: 0.9671 - val_loss: 8.3406e-07 - val_accuracy: 0.8878 - val_auc_16: 0.9666\n",
      "Epoch 22/500\n",
      "1078/1078 - 3s - loss: 8.2253e-07 - accuracy: 0.8895 - auc_16: 0.9674 - val_loss: 8.1596e-07 - val_accuracy: 0.8885 - val_auc_16: 0.9669\n",
      "Epoch 23/500\n",
      "1078/1078 - 3s - loss: 8.0136e-07 - accuracy: 0.8893 - auc_16: 0.9671 - val_loss: 9.6641e-07 - val_accuracy: 0.8893 - val_auc_16: 0.9672\n",
      "Epoch 24/500\n",
      "1078/1078 - 3s - loss: 8.1271e-07 - accuracy: 0.8890 - auc_16: 0.9668 - val_loss: 1.0876e-06 - val_accuracy: 0.8889 - val_auc_16: 0.9671\n",
      "Epoch 25/500\n",
      "1078/1078 - 3s - loss: 8.0955e-07 - accuracy: 0.8893 - auc_16: 0.9668 - val_loss: 9.3642e-07 - val_accuracy: 0.8892 - val_auc_16: 0.9673\n",
      "Epoch 26/500\n",
      "1078/1078 - 3s - loss: 8.1080e-07 - accuracy: 0.8894 - auc_16: 0.9669 - val_loss: 8.5426e-07 - val_accuracy: 0.8877 - val_auc_16: 0.9666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 15:58:14,980] Trial 16 finished with value: 0.8875646992301335 and parameters: {'num_hidden_layers': 3, 'num_features_layer_0': 145, 'num_features_layer_1': 79, 'num_features_layer_2': 118, 'dropout': 0.27, 'batch_size': 256, 'batch_norm': False}. Best is trial 14 with value: 0.914027952967103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Model: \"functional_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 142)               9940      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 142)               568       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 142)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 99)                14157     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 99)                396       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 99)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 130)               13000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 130)               520       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 130)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 96)                12576     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 51,530\n",
      "Trainable params: 50,650\n",
      "Non-trainable params: 880\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0701s). Check your callbacks.\n",
      "1078/1078 - 17s - loss: 7.6673e-06 - accuracy: 0.8288 - auc_17: 0.9281 - val_loss: 8.6379e-07 - val_accuracy: 0.8122 - val_auc_17: 0.9430\n",
      "Epoch 2/500\n",
      "1078/1078 - 6s - loss: 1.2527e-06 - accuracy: 0.8224 - auc_17: 0.9451 - val_loss: 8.4556e-07 - val_accuracy: 0.8349 - val_auc_17: 0.9479\n",
      "Epoch 3/500\n",
      "1078/1078 - 6s - loss: 1.2215e-06 - accuracy: 0.8360 - auc_17: 0.9453 - val_loss: 9.5370e-07 - val_accuracy: 0.8771 - val_auc_17: 0.9574\n",
      "Epoch 4/500\n",
      "1078/1078 - 6s - loss: 1.0053e-06 - accuracy: 0.8641 - auc_17: 0.9520 - val_loss: 7.2653e-07 - val_accuracy: 0.8807 - val_auc_17: 0.9546\n",
      "Epoch 5/500\n",
      "1078/1078 - 5s - loss: 9.6269e-07 - accuracy: 0.8753 - auc_17: 0.9548 - val_loss: 1.0041e-06 - val_accuracy: 0.8812 - val_auc_17: 0.9580\n",
      "Epoch 6/500\n",
      "1078/1078 - 6s - loss: 8.4755e-07 - accuracy: 0.8920 - auc_17: 0.9616 - val_loss: 1.1108e-06 - val_accuracy: 0.8946 - val_auc_17: 0.9635\n",
      "Epoch 7/500\n",
      "1078/1078 - 6s - loss: 7.9090e-07 - accuracy: 0.8972 - auc_17: 0.9634 - val_loss: 1.0319e-06 - val_accuracy: 0.8944 - val_auc_17: 0.9647\n",
      "Epoch 8/500\n",
      "1078/1078 - 5s - loss: 7.8407e-07 - accuracy: 0.8985 - auc_17: 0.9643 - val_loss: 8.3435e-07 - val_accuracy: 0.8951 - val_auc_17: 0.9630\n",
      "Epoch 9/500\n",
      "1078/1078 - 5s - loss: 7.7259e-07 - accuracy: 0.9017 - auc_17: 0.9636 - val_loss: 8.9130e-07 - val_accuracy: 0.9004 - val_auc_17: 0.9639\n",
      "Epoch 10/500\n",
      "1078/1078 - 5s - loss: 7.6732e-07 - accuracy: 0.9035 - auc_17: 0.9644 - val_loss: 1.1214e-06 - val_accuracy: 0.8980 - val_auc_17: 0.9647\n",
      "Epoch 11/500\n",
      "1078/1078 - 5s - loss: 7.5641e-07 - accuracy: 0.9026 - auc_17: 0.9645 - val_loss: 1.0864e-06 - val_accuracy: 0.9014 - val_auc_17: 0.9646\n",
      "Epoch 12/500\n",
      "1078/1078 - 5s - loss: 7.7696e-07 - accuracy: 0.9042 - auc_17: 0.9653 - val_loss: 8.1750e-07 - val_accuracy: 0.9027 - val_auc_17: 0.9655\n",
      "Epoch 13/500\n",
      "1078/1078 - 5s - loss: 7.4710e-07 - accuracy: 0.9044 - auc_17: 0.9653 - val_loss: 9.7190e-07 - val_accuracy: 0.9030 - val_auc_17: 0.9661\n",
      "Epoch 14/500\n",
      "1078/1078 - 5s - loss: 7.4702e-07 - accuracy: 0.9052 - auc_17: 0.9654 - val_loss: 1.0157e-06 - val_accuracy: 0.9042 - val_auc_17: 0.9660\n",
      "Epoch 15/500\n",
      "1078/1078 - 5s - loss: 7.5039e-07 - accuracy: 0.9045 - auc_17: 0.9651 - val_loss: 8.8920e-07 - val_accuracy: 0.9024 - val_auc_17: 0.9651\n",
      "Epoch 16/500\n",
      "1078/1078 - 5s - loss: 7.4572e-07 - accuracy: 0.9050 - auc_17: 0.9649 - val_loss: 1.0216e-06 - val_accuracy: 0.9036 - val_auc_17: 0.9660\n",
      "Epoch 17/500\n",
      "1078/1078 - 5s - loss: 7.3822e-07 - accuracy: 0.9054 - auc_17: 0.9658 - val_loss: 9.6342e-07 - val_accuracy: 0.9022 - val_auc_17: 0.9647\n",
      "Epoch 18/500\n",
      "1078/1078 - 5s - loss: 7.3161e-07 - accuracy: 0.9051 - auc_17: 0.9652 - val_loss: 1.1146e-06 - val_accuracy: 0.9029 - val_auc_17: 0.9655\n",
      "Epoch 19/500\n",
      "1078/1078 - 5s - loss: 7.4936e-07 - accuracy: 0.9053 - auc_17: 0.9653 - val_loss: 9.1087e-07 - val_accuracy: 0.9019 - val_auc_17: 0.9643\n",
      "Epoch 20/500\n",
      "1078/1078 - 5s - loss: 7.4224e-07 - accuracy: 0.9056 - auc_17: 0.9652 - val_loss: 9.9369e-07 - val_accuracy: 0.9035 - val_auc_17: 0.9659\n",
      "Epoch 21/500\n",
      "1078/1078 - 5s - loss: 7.6160e-07 - accuracy: 0.9057 - auc_17: 0.9654 - val_loss: 9.7619e-07 - val_accuracy: 0.9024 - val_auc_17: 0.9654\n",
      "Epoch 22/500\n",
      "1078/1078 - 5s - loss: 7.4482e-07 - accuracy: 0.9052 - auc_17: 0.9655 - val_loss: 1.0076e-06 - val_accuracy: 0.9023 - val_auc_17: 0.9653\n",
      "Epoch 23/500\n",
      "1078/1078 - 5s - loss: 7.3135e-07 - accuracy: 0.9055 - auc_17: 0.9650 - val_loss: 1.0072e-06 - val_accuracy: 0.9021 - val_auc_17: 0.9647\n",
      "Epoch 24/500\n",
      "1078/1078 - 5s - loss: 7.4792e-07 - accuracy: 0.9058 - auc_17: 0.9653 - val_loss: 1.0376e-06 - val_accuracy: 0.9023 - val_auc_17: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 16:00:51,384] Trial 17 finished with value: 0.9027916721035768 and parameters: {'num_hidden_layers': 4, 'num_features_layer_0': 142, 'num_features_layer_1': 99, 'num_features_layer_2': 130, 'num_features_layer_3': 96, 'dropout': 0.12000000000000001, 'batch_size': 256, 'batch_norm': True}. Best is trial 14 with value: 0.914027952967103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Model: \"functional_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 114)               7980      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 114)               456       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 114)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 134)               15410     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 134)               536       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 134)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 28)                3780      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 28,467\n",
      "Trainable params: 27,833\n",
      "Non-trainable params: 634\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0740s). Check your callbacks.\n",
      "1078/1078 - 15s - loss: 3.7830e-06 - accuracy: 0.8322 - auc_18: 0.9435 - val_loss: 7.3335e-07 - val_accuracy: 0.8326 - val_auc_18: 0.9455\n",
      "Epoch 2/500\n",
      "1078/1078 - 4s - loss: 1.0794e-06 - accuracy: 0.8540 - auc_18: 0.9485 - val_loss: 8.7333e-07 - val_accuracy: 0.8678 - val_auc_18: 0.9592\n",
      "Epoch 3/500\n",
      "1078/1078 - 4s - loss: 1.0212e-06 - accuracy: 0.8691 - auc_18: 0.9528 - val_loss: 8.5861e-07 - val_accuracy: 0.8771 - val_auc_18: 0.9556\n",
      "Epoch 4/500\n",
      "1078/1078 - 4s - loss: 8.7626e-07 - accuracy: 0.8872 - auc_18: 0.9601 - val_loss: 9.9579e-07 - val_accuracy: 0.8830 - val_auc_18: 0.9560\n",
      "Epoch 5/500\n",
      "1078/1078 - 4s - loss: 8.3594e-07 - accuracy: 0.8894 - auc_18: 0.9632 - val_loss: 7.9338e-07 - val_accuracy: 0.8627 - val_auc_18: 0.9600\n",
      "Epoch 6/500\n",
      "1078/1078 - 4s - loss: 7.7042e-07 - accuracy: 0.8977 - auc_18: 0.9651 - val_loss: 7.1309e-07 - val_accuracy: 0.8858 - val_auc_18: 0.9630\n",
      "Epoch 7/500\n",
      "1078/1078 - 4s - loss: 7.2652e-07 - accuracy: 0.9000 - auc_18: 0.9666 - val_loss: 1.0147e-06 - val_accuracy: 0.9041 - val_auc_18: 0.9685\n",
      "Epoch 8/500\n",
      "1078/1078 - 4s - loss: 6.8658e-07 - accuracy: 0.9088 - auc_18: 0.9696 - val_loss: 1.1353e-06 - val_accuracy: 0.9082 - val_auc_18: 0.9685\n",
      "Epoch 9/500\n",
      "1078/1078 - 4s - loss: 6.8641e-07 - accuracy: 0.9113 - auc_18: 0.9692 - val_loss: 1.1137e-06 - val_accuracy: 0.9115 - val_auc_18: 0.9688\n",
      "Epoch 10/500\n",
      "1078/1078 - 4s - loss: 6.6950e-07 - accuracy: 0.9141 - auc_18: 0.9694 - val_loss: 1.2694e-06 - val_accuracy: 0.9140 - val_auc_18: 0.9694\n",
      "Epoch 11/500\n",
      "1078/1078 - 4s - loss: 6.5689e-07 - accuracy: 0.9160 - auc_18: 0.9689 - val_loss: 1.3238e-06 - val_accuracy: 0.9177 - val_auc_18: 0.9705\n",
      "Epoch 12/500\n",
      "1078/1078 - 4s - loss: 6.5808e-07 - accuracy: 0.9178 - auc_18: 0.9699 - val_loss: 1.2325e-06 - val_accuracy: 0.9169 - val_auc_18: 0.9699\n",
      "Epoch 13/500\n",
      "1078/1078 - 4s - loss: 6.5427e-07 - accuracy: 0.9176 - auc_18: 0.9700 - val_loss: 1.3158e-06 - val_accuracy: 0.9160 - val_auc_18: 0.9694\n",
      "Epoch 14/500\n",
      "1078/1078 - 4s - loss: 6.5946e-07 - accuracy: 0.9175 - auc_18: 0.9699 - val_loss: 1.2744e-06 - val_accuracy: 0.9164 - val_auc_18: 0.9697\n",
      "Epoch 15/500\n",
      "1078/1078 - 4s - loss: 6.5838e-07 - accuracy: 0.9168 - auc_18: 0.9697 - val_loss: 1.2808e-06 - val_accuracy: 0.9160 - val_auc_18: 0.9698\n",
      "Epoch 16/500\n",
      "1078/1078 - 4s - loss: 6.5823e-07 - accuracy: 0.9174 - auc_18: 0.9695 - val_loss: 1.3156e-06 - val_accuracy: 0.9163 - val_auc_18: 0.9698\n",
      "Epoch 17/500\n",
      "1078/1078 - 4s - loss: 6.5475e-07 - accuracy: 0.9172 - auc_18: 0.9698 - val_loss: 1.2064e-06 - val_accuracy: 0.9159 - val_auc_18: 0.9699\n",
      "Epoch 18/500\n",
      "1078/1078 - 4s - loss: 6.5605e-07 - accuracy: 0.9174 - auc_18: 0.9700 - val_loss: 1.3431e-06 - val_accuracy: 0.9159 - val_auc_18: 0.9697\n",
      "Epoch 19/500\n",
      "1078/1078 - 4s - loss: 6.5459e-07 - accuracy: 0.9173 - auc_18: 0.9697 - val_loss: 1.1961e-06 - val_accuracy: 0.9149 - val_auc_18: 0.9694\n",
      "Epoch 20/500\n",
      "1078/1078 - 4s - loss: 6.6067e-07 - accuracy: 0.9175 - auc_18: 0.9702 - val_loss: 1.2627e-06 - val_accuracy: 0.9169 - val_auc_18: 0.9698\n",
      "Epoch 21/500\n",
      "1078/1078 - 4s - loss: 6.5024e-07 - accuracy: 0.9172 - auc_18: 0.9698 - val_loss: 1.2451e-06 - val_accuracy: 0.9151 - val_auc_18: 0.9690\n",
      "Epoch 22/500\n",
      "1078/1078 - 4s - loss: 6.5820e-07 - accuracy: 0.9177 - auc_18: 0.9698 - val_loss: 1.2807e-06 - val_accuracy: 0.9154 - val_auc_18: 0.9695\n",
      "Epoch 23/500\n",
      "1078/1078 - 4s - loss: 6.4822e-07 - accuracy: 0.9170 - auc_18: 0.9701 - val_loss: 1.3606e-06 - val_accuracy: 0.9168 - val_auc_18: 0.9705\n",
      "Epoch 24/500\n",
      "1078/1078 - 4s - loss: 6.5912e-07 - accuracy: 0.9173 - auc_18: 0.9699 - val_loss: 1.2105e-06 - val_accuracy: 0.9157 - val_auc_18: 0.9695\n",
      "Epoch 25/500\n",
      "1078/1078 - 4s - loss: 6.5968e-07 - accuracy: 0.9169 - auc_18: 0.9698 - val_loss: 1.2022e-06 - val_accuracy: 0.9158 - val_auc_18: 0.9698\n",
      "Epoch 26/500\n",
      "1078/1078 - 4s - loss: 6.5447e-07 - accuracy: 0.9174 - auc_18: 0.9702 - val_loss: 1.2665e-06 - val_accuracy: 0.9165 - val_auc_18: 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 16:03:00,737] Trial 18 finished with value: 0.9164383164426659 and parameters: {'num_hidden_layers': 3, 'num_features_layer_0': 114, 'num_features_layer_1': 134, 'num_features_layer_2': 28, 'dropout': 0.05, 'batch_size': 256, 'batch_norm': True}. Best is trial 18 with value: 0.9164383164426659.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Model: \"functional_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 69)                276       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 51)                3570      \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 127)               6604      \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 127)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 33)                4224      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 150)               5100      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 19,925\n",
      "Trainable params: 19,787\n",
      "Non-trainable params: 138\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0570s). Check your callbacks.\n",
      "1078/1078 - 15s - loss: 5.9104e-06 - accuracy: 0.7399 - auc_19: 0.7826 - val_loss: 9.9960e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9148\n",
      "Epoch 2/500\n",
      "1078/1078 - 4s - loss: 1.6025e-06 - accuracy: 0.7403 - auc_19: 0.9334 - val_loss: 7.6196e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9434\n",
      "Epoch 3/500\n",
      "1078/1078 - 4s - loss: 1.3787e-06 - accuracy: 0.7403 - auc_19: 0.9478 - val_loss: 7.6189e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9531\n",
      "Epoch 4/500\n",
      "1078/1078 - 4s - loss: 1.3019e-06 - accuracy: 0.7403 - auc_19: 0.9542 - val_loss: 6.9499e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9543\n",
      "Epoch 5/500\n",
      "1078/1078 - 4s - loss: 1.2057e-06 - accuracy: 0.7403 - auc_19: 0.9566 - val_loss: 6.2658e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9478\n",
      "Epoch 6/500\n",
      "1078/1078 - 4s - loss: 1.1763e-06 - accuracy: 0.7403 - auc_19: 0.9562 - val_loss: 6.6111e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9535\n",
      "Epoch 7/500\n",
      "1078/1078 - 4s - loss: 1.1206e-06 - accuracy: 0.7403 - auc_19: 0.9583 - val_loss: 6.9334e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9591\n",
      "Epoch 8/500\n",
      "1078/1078 - 4s - loss: 1.1231e-06 - accuracy: 0.7403 - auc_19: 0.9594 - val_loss: 6.9412e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9598\n",
      "Epoch 9/500\n",
      "1078/1078 - 4s - loss: 1.1309e-06 - accuracy: 0.7403 - auc_19: 0.9601 - val_loss: 6.8312e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9595\n",
      "Epoch 10/500\n",
      "1078/1078 - 4s - loss: 1.0935e-06 - accuracy: 0.7403 - auc_19: 0.9595 - val_loss: 7.0026e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9629\n",
      "Epoch 11/500\n",
      "1078/1078 - 4s - loss: 1.0707e-06 - accuracy: 0.7403 - auc_19: 0.9615 - val_loss: 7.0896e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9611\n",
      "Epoch 12/500\n",
      "1078/1078 - 4s - loss: 1.0753e-06 - accuracy: 0.7403 - auc_19: 0.9609 - val_loss: 7.2123e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9610\n",
      "Epoch 13/500\n",
      "1078/1078 - 4s - loss: 1.1165e-06 - accuracy: 0.7403 - auc_19: 0.9608 - val_loss: 7.1767e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9596\n",
      "Epoch 14/500\n",
      "1078/1078 - 4s - loss: 1.0769e-06 - accuracy: 0.7403 - auc_19: 0.9604 - val_loss: 7.0228e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9604\n",
      "Epoch 15/500\n",
      "1078/1078 - 4s - loss: 1.0586e-06 - accuracy: 0.7403 - auc_19: 0.9608 - val_loss: 7.1438e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9611\n",
      "Epoch 16/500\n",
      "1078/1078 - 4s - loss: 1.0554e-06 - accuracy: 0.7403 - auc_19: 0.9606 - val_loss: 7.1266e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9606\n",
      "Epoch 17/500\n",
      "1078/1078 - 4s - loss: 1.0920e-06 - accuracy: 0.7403 - auc_19: 0.9609 - val_loss: 6.8921e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9604\n",
      "Epoch 18/500\n",
      "1078/1078 - 4s - loss: 1.0892e-06 - accuracy: 0.7403 - auc_19: 0.9605 - val_loss: 7.3095e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9603\n",
      "Epoch 19/500\n",
      "1078/1078 - 4s - loss: 1.1039e-06 - accuracy: 0.7403 - auc_19: 0.9602 - val_loss: 7.3676e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9610\n",
      "Epoch 20/500\n",
      "1078/1078 - 4s - loss: 1.0827e-06 - accuracy: 0.7403 - auc_19: 0.9613 - val_loss: 7.1279e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9604\n",
      "Epoch 21/500\n",
      "1078/1078 - 4s - loss: 1.0769e-06 - accuracy: 0.7403 - auc_19: 0.9605 - val_loss: 6.9083e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9596\n",
      "Epoch 22/500\n",
      "1078/1078 - 4s - loss: 1.0919e-06 - accuracy: 0.7403 - auc_19: 0.9612 - val_loss: 7.0470e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9602\n",
      "Epoch 23/500\n",
      "1078/1078 - 4s - loss: 1.0768e-06 - accuracy: 0.7403 - auc_19: 0.9605 - val_loss: 7.1428e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9616\n",
      "Epoch 24/500\n",
      "1078/1078 - 4s - loss: 1.0681e-06 - accuracy: 0.7403 - auc_19: 0.9606 - val_loss: 6.9944e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9599\n",
      "Epoch 25/500\n",
      "1078/1078 - 4s - loss: 1.0699e-06 - accuracy: 0.7403 - auc_19: 0.9607 - val_loss: 7.2390e-07 - val_accuracy: 0.7402 - val_auc_19: 0.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-11 16:04:58,069] Trial 19 finished with value: 0.7402498078958433 and parameters: {'num_hidden_layers': 4, 'num_features_layer_0': 51, 'num_features_layer_1': 127, 'num_features_layer_2': 33, 'num_features_layer_3': 150, 'dropout': 0.48, 'batch_size': 256, 'batch_norm': False}. Best is trial 18 with value: 0.9164383164426659.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_study_df(study_df):\n",
    "    \"\"\"\n",
    "    This function processes the study dataframe so that it is more concise and easier to read\n",
    "    \n",
    "    study_df -> pandas dataframe: optuna default study dataframe\n",
    "    \n",
    "    return -> pandas dataframe: processes study dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    study_df.rename(columns={\"number\": \"trial\", \"value\": \"val_acc\"}, inplace=True)\n",
    "    study_df.drop([\"datetime_start\", \"datetime_complete\"], axis=1, inplace=True)\n",
    "    param_name_dict = {}\n",
    "    for column in study_df:\n",
    "        if \"params\" in column:\n",
    "            param_name_dict[column] = column.replace(\"params_\", \"\")\n",
    "    study_df.rename(columns=param_name_dict, inplace=True)\n",
    "    \n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>duration</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>num_features_layer_0</th>\n",
       "      <th>num_features_layer_1</th>\n",
       "      <th>num_features_layer_2</th>\n",
       "      <th>num_features_layer_3</th>\n",
       "      <th>num_features_layer_4</th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.846831</td>\n",
       "      <td>0 days 00:01:13.198452</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.15</td>\n",
       "      <td>76</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.820955</td>\n",
       "      <td>0 days 00:02:23.581527</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>0.37</td>\n",
       "      <td>77</td>\n",
       "      <td>70.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.783027</td>\n",
       "      <td>0 days 00:01:28.820673</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.23</td>\n",
       "      <td>68</td>\n",
       "      <td>49.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.868666</td>\n",
       "      <td>0 days 00:00:54.362678</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>0.41</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.885796</td>\n",
       "      <td>0 days 00:01:08.137980</td>\n",
       "      <td>False</td>\n",
       "      <td>512</td>\n",
       "      <td>0.10</td>\n",
       "      <td>92</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.840191</td>\n",
       "      <td>0 days 00:01:00.819084</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.40</td>\n",
       "      <td>125</td>\n",
       "      <td>149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.807001</td>\n",
       "      <td>0 days 00:03:27.648310</td>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.50</td>\n",
       "      <td>94</td>\n",
       "      <td>123.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.740250</td>\n",
       "      <td>0 days 00:02:00.323031</td>\n",
       "      <td>False</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.34</td>\n",
       "      <td>128</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.874708</td>\n",
       "      <td>0 days 00:00:56.825841</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>0.19</td>\n",
       "      <td>129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.875556</td>\n",
       "      <td>0 days 00:01:10.258657</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>0.18</td>\n",
       "      <td>99</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0 days 00:01:16.395093</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.06</td>\n",
       "      <td>22</td>\n",
       "      <td>111.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.898526</td>\n",
       "      <td>0 days 00:01:12.275427</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>0.07</td>\n",
       "      <td>103</td>\n",
       "      <td>149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.898801</td>\n",
       "      <td>0 days 00:01:25.630772</td>\n",
       "      <td>False</td>\n",
       "      <td>512</td>\n",
       "      <td>0.05</td>\n",
       "      <td>112</td>\n",
       "      <td>148.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.912165</td>\n",
       "      <td>0 days 00:02:19.995752</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>0.06</td>\n",
       "      <td>115</td>\n",
       "      <td>150.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.914028</td>\n",
       "      <td>0 days 00:01:40.847059</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>145</td>\n",
       "      <td>97.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.900207</td>\n",
       "      <td>0 days 00:02:28.463364</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>0.12</td>\n",
       "      <td>145</td>\n",
       "      <td>96.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.887565</td>\n",
       "      <td>0 days 00:01:52.190297</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.27</td>\n",
       "      <td>145</td>\n",
       "      <td>79.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.902792</td>\n",
       "      <td>0 days 00:02:36.402432</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>0.12</td>\n",
       "      <td>142</td>\n",
       "      <td>99.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.916438</td>\n",
       "      <td>0 days 00:02:09.352215</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>114</td>\n",
       "      <td>134.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.740250</td>\n",
       "      <td>0 days 00:01:57.329254</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.48</td>\n",
       "      <td>51</td>\n",
       "      <td>127.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial   val_acc               duration  batch_norm  batch_size  dropout  \\\n",
       "0       0  0.846831 0 days 00:01:13.198452       False        1024     0.15   \n",
       "1       1  0.820955 0 days 00:02:23.581527        True         512     0.37   \n",
       "2       2  0.783027 0 days 00:01:28.820673       False        1024     0.23   \n",
       "3       3  0.868666 0 days 00:00:54.362678        True         512     0.41   \n",
       "4       4  0.885796 0 days 00:01:08.137980       False         512     0.10   \n",
       "5       5  0.840191 0 days 00:01:00.819084       False        1024     0.40   \n",
       "6       6  0.807001 0 days 00:03:27.648310        True        1024     0.50   \n",
       "7       7  0.740250 0 days 00:02:00.323031       False        1024     0.34   \n",
       "8       8  0.874708 0 days 00:00:56.825841        True         512     0.19   \n",
       "9       9  0.875556 0 days 00:01:10.258657        True         512     0.18   \n",
       "10     10  0.870602 0 days 00:01:16.395093       False         256     0.06   \n",
       "11     11  0.898526 0 days 00:01:12.275427        True         512     0.07   \n",
       "12     12  0.898801 0 days 00:01:25.630772       False         512     0.05   \n",
       "13     13  0.912165 0 days 00:02:19.995752        True         256     0.06   \n",
       "14     14  0.914028 0 days 00:01:40.847059       False         256     0.05   \n",
       "15     15  0.900207 0 days 00:02:28.463364        True         256     0.12   \n",
       "16     16  0.887565 0 days 00:01:52.190297       False         256     0.27   \n",
       "17     17  0.902792 0 days 00:02:36.402432        True         256     0.12   \n",
       "18     18  0.916438 0 days 00:02:09.352215        True         256     0.05   \n",
       "19     19  0.740250 0 days 00:01:57.329254       False         256     0.48   \n",
       "\n",
       "    num_features_layer_0  num_features_layer_1  num_features_layer_2  \\\n",
       "0                     76                  51.0                  51.0   \n",
       "1                     77                  70.0                  59.0   \n",
       "2                     68                  49.0                  67.0   \n",
       "3                     59                   NaN                   NaN   \n",
       "4                     92                 130.0                   NaN   \n",
       "5                    125                 149.0                   NaN   \n",
       "6                     94                 123.0                 106.0   \n",
       "7                    128                  23.0                  65.0   \n",
       "8                    129                   NaN                   NaN   \n",
       "9                     99                  37.0                   NaN   \n",
       "10                    22                 111.0                   NaN   \n",
       "11                   103                 149.0                   NaN   \n",
       "12                   112                 148.0                 150.0   \n",
       "13                   115                 150.0                 144.0   \n",
       "14                   145                  97.0                 149.0   \n",
       "15                   145                  96.0                 148.0   \n",
       "16                   145                  79.0                 118.0   \n",
       "17                   142                  99.0                 130.0   \n",
       "18                   114                 134.0                  28.0   \n",
       "19                    51                 127.0                  33.0   \n",
       "\n",
       "    num_features_layer_3  num_features_layer_4  num_hidden_layers     state  \n",
       "0                    NaN                   NaN                  3  COMPLETE  \n",
       "1                  114.0                 133.0                  5  COMPLETE  \n",
       "2                   56.0                   NaN                  4  COMPLETE  \n",
       "3                    NaN                   NaN                  1  COMPLETE  \n",
       "4                    NaN                   NaN                  2  COMPLETE  \n",
       "5                    NaN                   NaN                  2  COMPLETE  \n",
       "6                   41.0                   NaN                  4  COMPLETE  \n",
       "7                   39.0                 104.0                  5  COMPLETE  \n",
       "8                    NaN                   NaN                  1  COMPLETE  \n",
       "9                    NaN                   NaN                  2  COMPLETE  \n",
       "10                   NaN                   NaN                  2  COMPLETE  \n",
       "11                   NaN                   NaN                  2  COMPLETE  \n",
       "12                   NaN                   NaN                  3  COMPLETE  \n",
       "13                   NaN                   NaN                  3  COMPLETE  \n",
       "14                   NaN                   NaN                  3  COMPLETE  \n",
       "15                 148.0                   NaN                  4  COMPLETE  \n",
       "16                   NaN                   NaN                  3  COMPLETE  \n",
       "17                  96.0                   NaN                  4  COMPLETE  \n",
       "18                   NaN                   NaN                  3  COMPLETE  \n",
       "19                 150.0                   NaN                  4  COMPLETE  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_df = process_study_df(study.trials_dataframe())\n",
    "study_df.to_hdf(\"optuna_studies/study_1.h5\", key=\"study\")\n",
    "study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Study Hyperparameter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1152 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(30, 16))\n",
    "\n",
    "for i, column in enumerate(study_df.drop([\"trial\", \"val_acc\", \"duration\", \"state\"], axis=1)):\n",
    "    row, col = i // 3, i % 3\n",
    "    axs[row, col].set_title(f\"Val_acc variation with {column}\")\n",
    "    axs[row, col].scatter(study_df[column].values, study_df[\"val_acc\"].values)\n",
    "    axs[row, col].set_xlabel(column)\n",
    "    axs[row, col].set_ylabel(\"Val Acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
