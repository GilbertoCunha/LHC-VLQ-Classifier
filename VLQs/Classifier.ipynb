{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import and rearrange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FatJet_Multi</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet2_PT</th>\n",
       "      <th>FatJet3_PT</th>\n",
       "      <th>FatJet4_PT</th>\n",
       "      <th>FatJet5_PT</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet2_Mass</th>\n",
       "      <th>FatJet3_Mass</th>\n",
       "      <th>FatJet4_Mass</th>\n",
       "      <th>...</th>\n",
       "      <th>Electron1_Eta</th>\n",
       "      <th>Electron2_Eta</th>\n",
       "      <th>Electron1_Phi</th>\n",
       "      <th>Electron2_Phi</th>\n",
       "      <th>MissingET_MET</th>\n",
       "      <th>MissingET_Phi</th>\n",
       "      <th>ScalarHT_HT</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>Label</th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>222.386703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.281700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.745476</td>\n",
       "      <td>-2.622154</td>\n",
       "      <td>646.912720</td>\n",
       "      <td>1.916818e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>ttbarZ_2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>355.976532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.403839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303101</td>\n",
       "      <td>1.311433</td>\n",
       "      <td>-3.124268</td>\n",
       "      <td>2.662745</td>\n",
       "      <td>62.061245</td>\n",
       "      <td>0.126687</td>\n",
       "      <td>738.070923</td>\n",
       "      <td>1.916818e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>ttbarZ_2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>379.277893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.015945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443531</td>\n",
       "      <td>0.041256</td>\n",
       "      <td>0.257705</td>\n",
       "      <td>0.629130</td>\n",
       "      <td>154.406555</td>\n",
       "      <td>2.711888</td>\n",
       "      <td>1017.925659</td>\n",
       "      <td>1.916818e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>ttbarZ_2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>315.526428</td>\n",
       "      <td>210.032394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.493019</td>\n",
       "      <td>69.408371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.304909</td>\n",
       "      <td>1.464204</td>\n",
       "      <td>784.078552</td>\n",
       "      <td>1.916818e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>ttbarZ_2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>339.583130</td>\n",
       "      <td>219.172821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116.564987</td>\n",
       "      <td>78.811958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.853302</td>\n",
       "      <td>0.038497</td>\n",
       "      <td>775.420654</td>\n",
       "      <td>1.916818e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>ttbarZ_2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249910</th>\n",
       "      <td>4</td>\n",
       "      <td>666.686401</td>\n",
       "      <td>607.264709</td>\n",
       "      <td>450.049072</td>\n",
       "      <td>391.603912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>115.184601</td>\n",
       "      <td>179.727097</td>\n",
       "      <td>114.864006</td>\n",
       "      <td>64.223465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.573494</td>\n",
       "      <td>-2.934912</td>\n",
       "      <td>2303.999756</td>\n",
       "      <td>1.774938e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>mch45_HG_13TeV_wohg_HQ1400_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249918</th>\n",
       "      <td>4</td>\n",
       "      <td>928.090820</td>\n",
       "      <td>700.795898</td>\n",
       "      <td>401.137665</td>\n",
       "      <td>232.056839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>346.194672</td>\n",
       "      <td>102.790993</td>\n",
       "      <td>94.033356</td>\n",
       "      <td>23.929482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>365.781525</td>\n",
       "      <td>-2.217049</td>\n",
       "      <td>2508.777588</td>\n",
       "      <td>1.774938e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>mch45_HG_13TeV_wohg_HQ1400_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249942</th>\n",
       "      <td>2</td>\n",
       "      <td>1385.916504</td>\n",
       "      <td>1031.829224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>139.179977</td>\n",
       "      <td>158.596802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>226.095474</td>\n",
       "      <td>-0.110796</td>\n",
       "      <td>2874.203857</td>\n",
       "      <td>1.774938e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>mch45_HG_13TeV_wohg_HQ1400_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249974</th>\n",
       "      <td>5</td>\n",
       "      <td>967.123413</td>\n",
       "      <td>555.906433</td>\n",
       "      <td>533.325867</td>\n",
       "      <td>327.264191</td>\n",
       "      <td>263.725922</td>\n",
       "      <td>173.991745</td>\n",
       "      <td>96.630959</td>\n",
       "      <td>131.944000</td>\n",
       "      <td>63.603821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>329.906006</td>\n",
       "      <td>0.181167</td>\n",
       "      <td>2831.758057</td>\n",
       "      <td>1.774938e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>mch45_HG_13TeV_wohg_HQ1400_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249977</th>\n",
       "      <td>4</td>\n",
       "      <td>978.741028</td>\n",
       "      <td>378.995026</td>\n",
       "      <td>363.905457</td>\n",
       "      <td>255.313583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.309860</td>\n",
       "      <td>102.929085</td>\n",
       "      <td>133.128799</td>\n",
       "      <td>33.423523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>461.540375</td>\n",
       "      <td>0.095589</td>\n",
       "      <td>1985.404297</td>\n",
       "      <td>1.774938e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>mch45_HG_13TeV_wohg_HQ1400_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>827676 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FatJet_Multi   FatJet1_PT   FatJet2_PT  FatJet3_PT  FatJet4_PT  \\\n",
       "6                  1   222.386703     0.000000    0.000000    0.000000   \n",
       "21                 1   355.976532     0.000000    0.000000    0.000000   \n",
       "22                 1   379.277893     0.000000    0.000000    0.000000   \n",
       "27                 2   315.526428   210.032394    0.000000    0.000000   \n",
       "28                 2   339.583130   219.172821    0.000000    0.000000   \n",
       "...              ...          ...          ...         ...         ...   \n",
       "249910             4   666.686401   607.264709  450.049072  391.603912   \n",
       "249918             4   928.090820   700.795898  401.137665  232.056839   \n",
       "249942             2  1385.916504  1031.829224    0.000000    0.000000   \n",
       "249974             5   967.123413   555.906433  533.325867  327.264191   \n",
       "249977             4   978.741028   378.995026  363.905457  255.313583   \n",
       "\n",
       "        FatJet5_PT  FatJet1_Mass  FatJet2_Mass  FatJet3_Mass  FatJet4_Mass  \\\n",
       "6         0.000000     89.281700      0.000000      0.000000      0.000000   \n",
       "21        0.000000     92.403839      0.000000      0.000000      0.000000   \n",
       "22        0.000000     92.015945      0.000000      0.000000      0.000000   \n",
       "27        0.000000     79.493019     69.408371      0.000000      0.000000   \n",
       "28        0.000000    116.564987     78.811958      0.000000      0.000000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "249910    0.000000    115.184601    179.727097    114.864006     64.223465   \n",
       "249918    0.000000    346.194672    102.790993     94.033356     23.929482   \n",
       "249942    0.000000    139.179977    158.596802      0.000000      0.000000   \n",
       "249974  263.725922    173.991745     96.630959    131.944000     63.603821   \n",
       "249977    0.000000    110.309860    102.929085    133.128799     33.423523   \n",
       "\n",
       "        ...  Electron1_Eta  Electron2_Eta  Electron1_Phi  Electron2_Phi  \\\n",
       "6       ...       0.000000       0.000000       0.000000       0.000000   \n",
       "21      ...       1.303101       1.311433      -3.124268       2.662745   \n",
       "22      ...      -0.443531       0.041256       0.257705       0.629130   \n",
       "27      ...       0.000000       0.000000       0.000000       0.000000   \n",
       "28      ...       0.000000       0.000000       0.000000       0.000000   \n",
       "...     ...            ...            ...            ...            ...   \n",
       "249910  ...       0.000000       0.000000       0.000000       0.000000   \n",
       "249918  ...       0.000000       0.000000       0.000000       0.000000   \n",
       "249942  ...       0.000000       0.000000       0.000000       0.000000   \n",
       "249974  ...       0.000000       0.000000       0.000000       0.000000   \n",
       "249977  ...       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "        MissingET_MET  MissingET_Phi  ScalarHT_HT   gen_weights  Label  \\\n",
       "6           74.745476      -2.622154   646.912720  1.916818e-06      0   \n",
       "21          62.061245       0.126687   738.070923  1.916818e-06      0   \n",
       "22         154.406555       2.711888  1017.925659  1.916818e-06      0   \n",
       "27          88.304909       1.464204   784.078552  1.916818e-06      0   \n",
       "28          99.853302       0.038497   775.420654  1.916818e-06      0   \n",
       "...               ...            ...          ...           ...    ...   \n",
       "249910     112.573494      -2.934912  2303.999756  1.774938e-07      1   \n",
       "249918     365.781525      -2.217049  2508.777588  1.774938e-07      1   \n",
       "249942     226.095474      -0.110796  2874.203857  1.774938e-07      1   \n",
       "249974     329.906006       0.181167  2831.758057  1.774938e-07      1   \n",
       "249977     461.540375       0.095589  1985.404297  1.774938e-07      1   \n",
       "\n",
       "                                 Sample  \n",
       "6                             ttbarZ_2L  \n",
       "21                            ttbarZ_2L  \n",
       "22                            ttbarZ_2L  \n",
       "27                            ttbarZ_2L  \n",
       "28                            ttbarZ_2L  \n",
       "...                                 ...  \n",
       "249910  mch45_HG_13TeV_wohg_HQ1400_test  \n",
       "249918  mch45_HG_13TeV_wohg_HQ1400_test  \n",
       "249942  mch45_HG_13TeV_wohg_HQ1400_test  \n",
       "249974  mch45_HG_13TeV_wohg_HQ1400_test  \n",
       "249977  mch45_HG_13TeV_wohg_HQ1400_test  \n",
       "\n",
       "[827676 rows x 72 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "bkgd = pd.read_hdf(\"data/preprocessed/bkgd.h5\", key=\"bkgd\")\n",
    "vlq = pd.read_hdf(\"data/preprocessed/vlq.h5\", key=\"vlq\")\n",
    "X_train = pd.concat([bkgd, vlq])\n",
    "del bkgd, vlq\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train, test and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train.drop([\"Label\"], axis=1), X_train[\"Label\"], \n",
    "                                                    test_size=1/3, random_state=56)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train data\n",
    "X_train.to_hdf(\"data/classifier/train.h5\", key=\"X\")\n",
    "y_train.to_hdf(\"data/classifier/train.h5\", key=\"y\")\n",
    "\n",
    "# Save validation data\n",
    "X_val.to_hdf(\"data/classifier/validation.h5\", key=\"X\")\n",
    "y_val.to_hdf(\"data/classifier/validation.h5\", key=\"y\")\n",
    "\n",
    "# Save test data\n",
    "X_test.to_hdf(\"data/classifier/test.h5\", key=\"X\")\n",
    "y_test.to_hdf(\"data/classifier/test.h5\", key=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data samples\n",
    "train_samples, val_samples, test_samples = X_train[\"Sample\"], X_val[\"Sample\"], X_test[\"Sample\"]\n",
    "\n",
    "# Get data weights\n",
    "train_weights, val_weights, test_weights = X_train[\"gen_weights\"], X_val[\"gen_weights\"], X_test[\"gen_weights\"]\n",
    "\n",
    "# Remove sample and weight columns\n",
    "X_train.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "X_val.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "X_test.drop([\"Sample\", \"gen_weights\"], axis=1, inplace=True)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {\n",
    "    0: 1,\n",
    "    1: len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_samples):\n",
    "        self.means = np.mean(data_samples, axis=0, keepdims=True)\n",
    "        self.stds = np.std(data_samples, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means) / (self.stds + keras.backend.epsilon())\n",
    "    \n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 69)]              0         \n",
      "_________________________________________________________________\n",
      "standardization_4 (Standardi (None, 69)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 80)                5600      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 50)                4050      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 11,211\n",
      "Trainable params: 11,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(69,))\n",
    "std_inputs = std_layer(inputs)\n",
    "fc1 = keras.layers.Dense(80, activation=\"relu\")(std_inputs)\n",
    "d1 = keras.layers.Dropout(0.3)(fc1, training=True)\n",
    "fc2 = keras.layers.Dense(50, activation=\"relu\")(d1)\n",
    "d2 = keras.layers.Dropout(0.3)(fc2, training=True)\n",
    "fc3 = keras.layers.Dense(30, activation=\"relu\")(d2)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(fc3)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\", keras.metrics.AUC()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "name = \"Hidden:80, 50, 30|BatchS:256|Dropout:0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "TB = keras.callbacks.TensorBoard(\"logs/\" + name, write_images=True)\n",
    "\n",
    "# Early Stopping\n",
    "ES = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, verbose=2, mode=\"min\")\n",
    "\n",
    "# Model Checkpoint\n",
    "MC = keras.callbacks.ModelCheckpoint(\"models/\" + name + \".h5\", save_best_only=True, monitor=\"val_loss\",\n",
    "                                     mode=\"min\")\n",
    "\n",
    "# Reduce LR on Plateau\n",
    "LR = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6, mode=\"min\", \n",
    "                                       min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "   1/1078 [..............................] - ETA: 0s - loss: 4.2584e-04 - accuracy: 0.4102 - auc_4: 0.4733WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0441s). Check your callbacks.\n",
      "1078/1078 [==============================] - 15s 14ms/step - loss: 5.9939e-05 - accuracy: 0.7270 - auc_4: 0.6193 - val_loss: 3.1624e-06 - val_accuracy: 0.7403 - val_auc_4: 0.7572\n",
      "Epoch 2/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 3.2481e-06 - accuracy: 0.7403 - auc_4: 0.7828 - val_loss: 1.5041e-06 - val_accuracy: 0.7403 - val_auc_4: 0.8135\n",
      "Epoch 3/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 2.4448e-06 - accuracy: 0.7403 - auc_4: 0.8474 - val_loss: 1.1557e-06 - val_accuracy: 0.7403 - val_auc_4: 0.8739\n",
      "Epoch 4/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 2.1059e-06 - accuracy: 0.7405 - auc_4: 0.8924 - val_loss: 9.9042e-07 - val_accuracy: 0.7405 - val_auc_4: 0.9052\n",
      "Epoch 5/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.9034e-06 - accuracy: 0.7410 - auc_4: 0.9156 - val_loss: 8.7901e-07 - val_accuracy: 0.7413 - val_auc_4: 0.9225\n",
      "Epoch 6/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.7075e-06 - accuracy: 0.7427 - auc_4: 0.9301 - val_loss: 8.7218e-07 - val_accuracy: 0.7432 - val_auc_4: 0.9329\n",
      "Epoch 7/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5833e-06 - accuracy: 0.7462 - auc_4: 0.9365 - val_loss: 8.1156e-07 - val_accuracy: 0.7486 - val_auc_4: 0.9365\n",
      "Epoch 8/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5487e-06 - accuracy: 0.7493 - auc_4: 0.9386 - val_loss: 8.4225e-07 - val_accuracy: 0.7494 - val_auc_4: 0.9378\n",
      "Epoch 9/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4937e-06 - accuracy: 0.7503 - auc_4: 0.9391 - val_loss: 7.9404e-07 - val_accuracy: 0.7511 - val_auc_4: 0.9398\n",
      "Epoch 10/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5410e-06 - accuracy: 0.7515 - auc_4: 0.9400 - val_loss: 8.6677e-07 - val_accuracy: 0.7517 - val_auc_4: 0.9408\n",
      "Epoch 11/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5016e-06 - accuracy: 0.7519 - auc_4: 0.9412 - val_loss: 8.2306e-07 - val_accuracy: 0.7528 - val_auc_4: 0.9416\n",
      "Epoch 12/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5328e-06 - accuracy: 0.7532 - auc_4: 0.9418 - val_loss: 8.4993e-07 - val_accuracy: 0.7537 - val_auc_4: 0.9417\n",
      "Epoch 13/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4700e-06 - accuracy: 0.7543 - auc_4: 0.9424 - val_loss: 8.4210e-07 - val_accuracy: 0.7549 - val_auc_4: 0.9409\n",
      "Epoch 14/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5002e-06 - accuracy: 0.7549 - auc_4: 0.9421 - val_loss: 8.1896e-07 - val_accuracy: 0.7546 - val_auc_4: 0.9414\n",
      "Epoch 15/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4795e-06 - accuracy: 0.7550 - auc_4: 0.9420 - val_loss: 8.0269e-07 - val_accuracy: 0.7547 - val_auc_4: 0.9420\n",
      "Epoch 16/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4960e-06 - accuracy: 0.7549 - auc_4: 0.9421 - val_loss: 7.9990e-07 - val_accuracy: 0.7547 - val_auc_4: 0.9416\n",
      "Epoch 17/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4753e-06 - accuracy: 0.7550 - auc_4: 0.9422 - val_loss: 8.3331e-07 - val_accuracy: 0.7552 - val_auc_4: 0.9414\n",
      "Epoch 18/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4664e-06 - accuracy: 0.7547 - auc_4: 0.9423 - val_loss: 8.4438e-07 - val_accuracy: 0.7553 - val_auc_4: 0.9424\n",
      "Epoch 19/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4494e-06 - accuracy: 0.7551 - auc_4: 0.9419 - val_loss: 8.4102e-07 - val_accuracy: 0.7552 - val_auc_4: 0.9421\n",
      "Epoch 20/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4940e-06 - accuracy: 0.7555 - auc_4: 0.9426 - val_loss: 8.0480e-07 - val_accuracy: 0.7551 - val_auc_4: 0.9425\n",
      "Epoch 21/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4871e-06 - accuracy: 0.7557 - auc_4: 0.9430 - val_loss: 8.0624e-07 - val_accuracy: 0.7552 - val_auc_4: 0.9420\n",
      "Epoch 22/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4617e-06 - accuracy: 0.7557 - auc_4: 0.9428 - val_loss: 8.3818e-07 - val_accuracy: 0.7550 - val_auc_4: 0.9419\n",
      "Epoch 23/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4422e-06 - accuracy: 0.7562 - auc_4: 0.9427 - val_loss: 8.6151e-07 - val_accuracy: 0.7554 - val_auc_4: 0.9425\n",
      "Epoch 24/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4823e-06 - accuracy: 0.7559 - auc_4: 0.9427 - val_loss: 8.2630e-07 - val_accuracy: 0.7558 - val_auc_4: 0.9419\n",
      "Epoch 25/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4563e-06 - accuracy: 0.7563 - auc_4: 0.9425 - val_loss: 8.6210e-07 - val_accuracy: 0.7558 - val_auc_4: 0.9427\n",
      "Epoch 26/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4292e-06 - accuracy: 0.7563 - auc_4: 0.9430 - val_loss: 8.5663e-07 - val_accuracy: 0.7558 - val_auc_4: 0.9424\n",
      "Epoch 27/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4861e-06 - accuracy: 0.7560 - auc_4: 0.9434 - val_loss: 8.2680e-07 - val_accuracy: 0.7561 - val_auc_4: 0.9434\n",
      "Epoch 28/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4954e-06 - accuracy: 0.7564 - auc_4: 0.9430 - val_loss: 8.0643e-07 - val_accuracy: 0.7560 - val_auc_4: 0.9425\n",
      "Epoch 29/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4844e-06 - accuracy: 0.7562 - auc_4: 0.9426 - val_loss: 8.4230e-07 - val_accuracy: 0.7562 - val_auc_4: 0.9422\n",
      "Epoch 30/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4961e-06 - accuracy: 0.7564 - auc_4: 0.9428 - val_loss: 8.3954e-07 - val_accuracy: 0.7564 - val_auc_4: 0.9424\n",
      "Epoch 31/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4884e-06 - accuracy: 0.7569 - auc_4: 0.9433 - val_loss: 7.8575e-07 - val_accuracy: 0.7563 - val_auc_4: 0.9429\n",
      "Epoch 32/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4789e-06 - accuracy: 0.7569 - auc_4: 0.9425 - val_loss: 8.1331e-07 - val_accuracy: 0.7562 - val_auc_4: 0.9425\n",
      "Epoch 33/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4876e-06 - accuracy: 0.7568 - auc_4: 0.9425 - val_loss: 8.6957e-07 - val_accuracy: 0.7563 - val_auc_4: 0.9425\n",
      "Epoch 34/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4733e-06 - accuracy: 0.7571 - auc_4: 0.9428 - val_loss: 8.1460e-07 - val_accuracy: 0.7568 - val_auc_4: 0.9430\n",
      "Epoch 35/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4474e-06 - accuracy: 0.7569 - auc_4: 0.9429 - val_loss: 8.3424e-07 - val_accuracy: 0.7566 - val_auc_4: 0.9431\n",
      "Epoch 36/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4570e-06 - accuracy: 0.7567 - auc_4: 0.9434 - val_loss: 8.1211e-07 - val_accuracy: 0.7570 - val_auc_4: 0.9434\n",
      "Epoch 37/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4384e-06 - accuracy: 0.7571 - auc_4: 0.9434 - val_loss: 8.0826e-07 - val_accuracy: 0.7569 - val_auc_4: 0.9429\n",
      "Epoch 38/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4624e-06 - accuracy: 0.7567 - auc_4: 0.9430 - val_loss: 8.2206e-07 - val_accuracy: 0.7575 - val_auc_4: 0.9429\n",
      "Epoch 39/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4891e-06 - accuracy: 0.7575 - auc_4: 0.9435 - val_loss: 8.5085e-07 - val_accuracy: 0.7579 - val_auc_4: 0.9424\n",
      "Epoch 40/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4548e-06 - accuracy: 0.7574 - auc_4: 0.9433 - val_loss: 8.4221e-07 - val_accuracy: 0.7572 - val_auc_4: 0.9431\n",
      "Epoch 41/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4737e-06 - accuracy: 0.7578 - auc_4: 0.9438 - val_loss: 8.3612e-07 - val_accuracy: 0.7573 - val_auc_4: 0.9428\n",
      "Epoch 42/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4487e-06 - accuracy: 0.7575 - auc_4: 0.9435 - val_loss: 8.2275e-07 - val_accuracy: 0.7576 - val_auc_4: 0.9428\n",
      "Epoch 43/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4403e-06 - accuracy: 0.7580 - auc_4: 0.9434 - val_loss: 8.2000e-07 - val_accuracy: 0.7576 - val_auc_4: 0.9431\n",
      "Epoch 44/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4531e-06 - accuracy: 0.7579 - auc_4: 0.9431 - val_loss: 8.1297e-07 - val_accuracy: 0.7578 - val_auc_4: 0.9432\n",
      "Epoch 45/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4654e-06 - accuracy: 0.7577 - auc_4: 0.9436 - val_loss: 8.2210e-07 - val_accuracy: 0.7582 - val_auc_4: 0.9433\n",
      "Epoch 46/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4351e-06 - accuracy: 0.7584 - auc_4: 0.9439 - val_loss: 8.0100e-07 - val_accuracy: 0.7578 - val_auc_4: 0.9428\n",
      "Epoch 47/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4630e-06 - accuracy: 0.7583 - auc_4: 0.9435 - val_loss: 8.9237e-07 - val_accuracy: 0.7581 - val_auc_4: 0.9435\n",
      "Epoch 48/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4636e-06 - accuracy: 0.7585 - auc_4: 0.9434 - val_loss: 8.4308e-07 - val_accuracy: 0.7588 - val_auc_4: 0.9431\n",
      "Epoch 49/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4630e-06 - accuracy: 0.7585 - auc_4: 0.9444 - val_loss: 8.0619e-07 - val_accuracy: 0.7581 - val_auc_4: 0.9429\n",
      "Epoch 50/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4459e-06 - accuracy: 0.7589 - auc_4: 0.9435 - val_loss: 8.2937e-07 - val_accuracy: 0.7587 - val_auc_4: 0.9435\n",
      "Epoch 51/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4677e-06 - accuracy: 0.7588 - auc_4: 0.9435 - val_loss: 8.8436e-07 - val_accuracy: 0.7587 - val_auc_4: 0.9432\n",
      "Epoch 52/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4643e-06 - accuracy: 0.7590 - auc_4: 0.9443 - val_loss: 8.0503e-07 - val_accuracy: 0.7589 - val_auc_4: 0.9440\n",
      "Epoch 53/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4361e-06 - accuracy: 0.7590 - auc_4: 0.9447 - val_loss: 8.2832e-07 - val_accuracy: 0.7587 - val_auc_4: 0.9440\n",
      "Epoch 54/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4524e-06 - accuracy: 0.7590 - auc_4: 0.9435 - val_loss: 8.4434e-07 - val_accuracy: 0.7592 - val_auc_4: 0.9426\n",
      "Epoch 55/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4752e-06 - accuracy: 0.7597 - auc_4: 0.9439 - val_loss: 8.1247e-07 - val_accuracy: 0.7587 - val_auc_4: 0.9432\n",
      "Epoch 56/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4444e-06 - accuracy: 0.7595 - auc_4: 0.9434 - val_loss: 8.3225e-07 - val_accuracy: 0.7587 - val_auc_4: 0.9444\n",
      "Epoch 57/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4835e-06 - accuracy: 0.7598 - auc_4: 0.9440 - val_loss: 8.1763e-07 - val_accuracy: 0.7593 - val_auc_4: 0.9431\n",
      "Epoch 58/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4462e-06 - accuracy: 0.7594 - auc_4: 0.9436 - val_loss: 7.9548e-07 - val_accuracy: 0.7593 - val_auc_4: 0.9435\n",
      "Epoch 59/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4630e-06 - accuracy: 0.7597 - auc_4: 0.9443 - val_loss: 8.6341e-07 - val_accuracy: 0.7591 - val_auc_4: 0.9441\n",
      "Epoch 60/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.5076e-06 - accuracy: 0.7598 - auc_4: 0.9445 - val_loss: 8.0861e-07 - val_accuracy: 0.7594 - val_auc_4: 0.9435\n",
      "Epoch 61/500\n",
      "1078/1078 [==============================] - 3s 3ms/step - loss: 1.4557e-06 - accuracy: 0.7594 - auc_4: 0.9441 - val_loss: 8.5600e-07 - val_accuracy: 0.7591 - val_auc_4: 0.9426\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc53842e4f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.values, y_train.values, batch_size=256, epochs=500, callbacks=[TB, ES, MC, LR],\n",
    "          validation_data=(X_val.values, y_val.values, val_weights.values), shuffle=True,\n",
    "          sample_weight=train_weights.values, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
